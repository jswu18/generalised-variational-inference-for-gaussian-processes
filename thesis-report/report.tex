\documentclass{article}
\usepackage{setspace}
%\usepackage{subfigure}

\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{paper}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{cfr-lm}
\usepackage{sectsty}
\usepackage{pgf -umlcd}
\usepackage{xcolor}
\usepackage{listings}
\definecolor{light-gray}{gray}{0.95}
\lstdefinestyle{yaml}{
     backgroundcolor=\color{light-gray},
     numbers=left,
     showstringspaces=true,
     basicstyle=\color{black}\scriptsize,
     rulecolor=\color{black},
     string=[s]{'}{'},
     stringstyle=\color{black},
     comment=[l]{:},
     commentstyle=\color{blue},
     morecomment=[l]{-}
 }

% \sectionfont{\fontsize{12}{12}\selectfont}
% \subsectionfont{\fontsize{11}{11}\selectfont}
% \subsectionfont{\fontsize{10}{10}\selectfont}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{fact}[theorem]{Fact}

\newtheorem{problem}[theorem]{Problem}
\newtheorem{exercise}[theorem]{Exercise}
\def \set#1{\{#1\} }
\sectionfont{\fontsize{12}{12}\selectfont}
\subsectionfont{\fontsize{11}{11}\selectfont}
\subsubsectionfont{\fontsize{10}{10}\selectfont}
\def\code#1{\texttt{#1}}
\usepackage[nottoc,notlot,notlof]{tocbibind}




\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}

\usepackage{xcolor}
\newcommand{\jk}[1]{{\color{blue} [JK: #1]}}
\newcommand{\jw}[1]{{\color{gray} [JW: #1]}}
\newcommand{\vw}[1]{{\color{green} [VW: #1]}}

\newcommand{\calF}{\boldsymbol{\mathcal{F}}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\KL}{\operatorname{KL}}
\newcommand{\MMD}{\operatorname{MMD}}
\newcommand{\JSD}{\operatorname{JSD}}



\newcommand{\nats}{\mbox{\( \mathbb N \)}}
\newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}
\newcommand{\Cat}{\operatorname{\mathcal{C}}}
\newcommand{\Chol}{\operatorname{Chol}}
\newcommand{\KLD}{\operatorname{\mathbb{KL}}}
\newcommand{\D}{\operatorname{\mathbb{D}}}
\newcommand{\WD}{\operatorname{\mathbb{W}_2}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\GP}{\operatorname{\mathcal{GP}}}
\newcommand{\wc}{\operatorname{{}\cdot{}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% make sure equation numbers start with the section they are from
\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\onehalfspacing
\begin{titlepage}
	\centering
\begin{figure}[h!]
\begin{flushright}
    \includegraphics[width=.333\textwidth]{ucl_logo.png}
\end{flushright}
\end{figure}
    {}
	\vspace{3cm}

	{\Huge\bfseries Generalised Variational Inference \\ for Gaussian Processes\par}
	% \vfill

	\vspace{1cm}
	{\LARGE James (\textit{Jian Shu}) Wu\par}
	\vspace{0.5cm}
	supervised by\par
    \Large{Veit D. Wild \& Jeremias Knoblauch}\\
	\vfill
	{\large September 2023\par}
 \textsc{MSc Computational Statistics and Machine Learning}\\
	\vspace{1cm}
 \textit{\scriptsize{This report is submitted as part requirement for the MSc Computational Statistics and Machine Learning at University College London. It is substantially the result of my own work except where explicitly indicated in the text. \\ The report may be freely copied and distributed provided the source is explicitly acknowledged.}}

	% \vfill

% Bottom of the page
\end{titlepage}
\pagenumbering{gobble}
\newpage
\setcounter{page}{1}
\pagenumbering{roman}

% This report is submitted as part requirement for the Master's in Computational Statistics \& Machine Learning Degree at University College London (UCL). It is substantially the result of my own work except where explicitly indicated in the text.

% \begin{flushright}
%     \textit{James (Jian Shu) Wu}

%     September 2023
% \end{flushright}
% \newpage

\begin{abstract}
Proposed by \cite{knoblauch2022optimization}, generalised variational inference (GVI) is a flexible learning framework motivated by an optimisation-centric interpretation of the Bayesian posterior. 
Extending this framework to infinite dimensions, \cite{wild2022generalized} proposed Gaussian Wasserstein Inference (GWI) in Function Spaces, a GVI framework for Gaussian processes (GPs). 
GWI circumvents many limitations of traditional variational inference for GPs and provides a more flexible and scaleable learning framework.
Our work improves components of GWI and also proposes a new framework that we call projected GVI (pGVI) for GPs. 
Compared to the computational costs of currently existing approaches, pGVI is a significantly cheaper learning framework.
Our proposed methods are experimentally-driven and predominantly motivated to improve the computational and predictive performance of variational GPs. 
Finally, we present our learning frameworks through a comprehensive software implementation available at \href{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes}{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes}.
\end{abstract}

\newpage
\section*{Acknowledgements}
\textit{This work would not have been possible without Jeremias Knoblauch and Veit D. Wild. 
Thank you for your patience and guidance as I navigated this world of theory while wearing the hardhat of an engineer. 
I look forward to continuing our collaboration into the future.
I’d also like to thank my friends and family who have supported me throughout my life and in particular, this past year. 
This degree has been a very challenging but rewarding experience and I couldn’t have done it without all of you.
}

\hspace*{\fill} \textit{- James}

\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\counterwithin{figure}{section}
% \setcounter{section}{-1}

\newpage
\section{Introduction}
Uncertainty quantification is an important research direction within the machine learning community.
As a modelling approach it can encourage desirable prediction behaviours, such as out-of-distribution (OOD) detection. 
In practice, quantifying uncertainty can enable better informed decision-making, particularly important in high-risk settings.

% These include Monte-Carlo dropout, an approach based on weight sampling for neural networks, ensemble techniques, quantifying the spread of `disagreement' across a number of different models, and Bayesian approaches, where previous observations are used to quantify our beliefs for new data through the Bayesian posterior.
% This has motivated variational approaches such as \cite{titsias2009variational}.
% However, these approaches have their own limitations with respect to scaleability and restrictive approximation spaces.

There are a number of modelling approaches for uncertainty quantification. 
Bayesian approaches are a common choice and generally provide more theoretically intuitive interpretations of uncertainty compared to other methods. 
Our work focuses on a well-known Bayesian approach called Gaussian processes (GPs), which we review in Section \ref{section:gaussian-processes}.
GPs quantify uncertainty in the function space domain, but like most Bayesian approaches they suffer from mis-specifications and computational intractability. 
The problem of GP intractability motivated variational approaches such as \cite{titsias2009variational}, which we review in Section \ref{section:vi-gp}.
But these approaches have their own limitations, operating within restrictive approximation spaces.
Section \ref{section:gvi-for-gps} reviews generalised variational inference (GVI), proposed by \cite{knoblauch2022optimization} and Gaussian Wasserstein inference (GWI) in function spaces, proposed by \cite{wild2022generalized}. 
These address the issues of mis-specification, intractability, and restrictiveness that limited previous GP approaches. 

Our contributions begin in Section \ref{section:improving-gwi} where we present various improvements to the GWI learning framework.
This is followed by Section \ref{section:pgvi}, which introduces a new computationally cheaper framework we call projected GVI 
(pGVI) for GPs.
We also present an experimentation framework, summarised in Section \ref{section:experimentation-framework}. 
This includes the development of an extensive software implementation designed for flexibility and scaleability, introducing appropriate abstraction architectures, and following best practices common in the engineering community. 
This section will also present our experimentation setup, followed by examples to show this in action. We finish with Section \ref{section:future-work} proposing future research directions and concluding thoughts in Section \ref{section:conclusions}.

\newpage
\section{Notation}
Before we begin, we will make some general remarks on the notation used throughout the text. 
We will denote $\mathcal{X}$ as any input space, $\mathbb{R}$ as the real space, and $\mathbb{R}^N$ as an $N$-dimensional real space. 
$\boldsymbol{\boldsymbol{\mathcal{F}}}$, $\boldsymbol{\mathcal{G}}$, $\boldsymbol{\mathcal{Q}}$, and $\boldsymbol{\Gamma}$ will also denote spaces, but will be defined throughout the text as needed.

A Gaussian distribution will be denoted $\mathcal{N}(\cdot, \cdot)$, where 
\begin{align}
    Y \sim \mathcal{N}(\mathbf{m}, \mathbf{K})
\end{align}
and $Y \in \mathbb{R}^N$ denotes that $Y$ is a random vector taking values in $\mathbb{R}^N$ following a Gaussian distribution with $\mathbf{m} \in \mathbb{R}^N$ as the mean and $\mathbf{K} \in \mathbb{R}^{N \times N}_{\succcurlyeq 0}$ as the covariance. 
The notation $\mathbf{K} \in \mathbb{R}^{N \times N}_{\succcurlyeq 0}$ indicates that $\mathbf{K}$ is a real-valued matrix of shape $N \times N$ that is positive semi-definite, which is denoted by the subscript ${\succcurlyeq 0}$. 


For any random element $F$, we review the following notational conventions:
\begin{itemize}
    \setlength\itemsep{0em}
    \item sample notation: 
        \begin{align}
            F \sim \mathcal{N}(\cdot, \cdot)
        \end{align}
        as seen previously, which becomes $F \vert \mathbf{Y}$ when the random element is conditioned on the known element $\mathbf{Y}$,
    \item measure notation: 
        \begin{align}
            \mathbb{P}^F = \mathcal{N}(\cdot, \cdot),
        \end{align}
        where $\mathbb{P}^F$ is the probability measure of $F$ or alternatively $\mathbb{P}(F) = \mathcal{N}(\cdot, \cdot)$, which becomes $\mathbb{P}(F \vert \mathbf{Y})$ when conditioned, and
    \item probability density notation: 
        \begin{align}
           p(f) = \mathcal{N}(\cdot, \cdot),
        \end{align}
        when there exists a density $p$ with respect to some other measure, most commonly the Lebesgue measure. 
        When conditioned, we denote $p(f\vert \mathbf{Y})$.
\end{itemize}
Sample, measure, and probability density notation will be used interchangeably throughout this text, whenever appropriate. 
We will also define new notation such as $P \coloneqq \mathbb{P}(F)$ and $Q \coloneqq \mathbb{Q}(F)$, whenever convenient.

Other conventions that we follow includes $\mathbb{E}_{P}[\wc]$, which denotes expectation with respect to $P$ and $\mathbb{D}[Q, P]$, which denotes a divergence between the measures $Q$ and $P$. 
Other divergences will follow similar notation and will be defined whenever used.


\newpage
\section{Gaussian Processes}\label{section:gaussian-processes}
Gaussian processes (GPs) are powerful universal function approximators that can be used for both regression and classification tasks. The following sections will review the GP and its application to regression and classification following \cite{rasmussen2003gaussian}, \cite{matthews2017scalable}, and \cite{wild2022generalized}.

\subsection{The Gaussian Process}\label{section:the-gp}
A GP is a stochastic process such that for any $N$ points $\mathbf{X} \coloneqq \left\{ x_n\right\}_{n=1}^N$ where $x_n \in \mathcal{X}$, the corresponding random response vector $Y \in \mathbb{R}^N$ has the Gaussian distribution
\begin{align}
    \label{gp-vector}
    Y \sim \mathcal{N}\left(\mathbf{m}, \mathbf{K}\right),
\end{align}
where $\mathbf{m} \in \mathbb{R}^N$ and $\mathbf{K} \in  \mathbb{R}^{N \times N}_{\succcurlyeq 0}$.
The mean vector $\mathbf{m}$ is constructed through the selection of a mean function mapping $m: \mathcal{X} \rightarrow \mathbb{R}$ such that
\begin{align}
    \label{gp-mean-vector}
    \mathbf{m} \coloneqq \left[ m(x_1) \cdots m(x_N)\right]^T,
\end{align}
while constructing $\mathbf{K}$ involves choosing a kernel function mapping $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ such that each element of the matrix is the evaluation
\begin{align}
    \label{gp-kernel-matrix}
    \left[\mathbf{K}\right]_{n, n'} \coloneqq k(x_n, x_{n'}),
\end{align}
for $n, n'=1,\dots, N$.
$\mathbf{K}$ is also known as the gram matrix.
Appendix \ref{appendix:positive-definite-kernel} shows that choosing kernel functions defined as inner products of a feature space mapping will ensure that $\mathbf{K}$ is a valid positive semi-definite covariance matrix.


The distribution in (\ref{gp-vector}) is a finite-dimensional instance of the GP random function mapping
\begin{align}
    F \sim \GP(m, k),
    \label{gp}
\end{align}
 where $F \coloneqq \left\{F(x): x \in \mathcal{X}\right\}$ such that a sample path from $F$ has the mapping $f: \mathcal{X} \rightarrow \mathbb{R}$.
In other words, choosing a mean and kernel to construct (\ref{gp}) ensures that \textit{any} finite set of inputs will have a consistent joint Gaussian distribution adhering to (\ref{gp-vector}).

GPs are a powerful modelling approach. With minimal restrictions for choosing the mean function and kernel function, there are endless possibilities for constructing expressive GP model spaces.
This control and visibility into the model's behaviour is a strong advantage of GPs compared to other approaches.

Morevoer, \cite{novak2019neural} explains that the GP is an important construction for understanding the theoretical properties of many neural network architectures at initialisation.
Showing that the infinite width limit of many such architectures can be expressed as a GP has provided a theoretical framework for analysing neural networks typically viewed as black box approaches.
This provides further motivation for the potential and importance of GPs.

\subsection{Gaussian Process Regression}
Consider the regression task where we have $N$ observation pairs $(\mathbf{X}, \mathbf{Y}) \coloneqq \left\{(x_n, y_n)\right\}_{n=1}^{N}$ with inputs $x_n \in \mathcal{X}$ and responses $y_n \in \mathbb{R}$. GP regression models the data generating process as
\begin{align}
    y \sim F(x) + \epsilon,
    \label{regression-data-uncertainties}
\end{align}
where the GP random function mapping $F$ accounts for the epistemic (model) uncertainty and the random scalar $\epsilon$ accounts for the aleatoric (measurement) uncertainty. In this formulation, we assume that the aleatoric uncertainty is homoscedastic of the form
\begin{align}
    \epsilon \sim \mathcal{N} \left(0, \sigma^2\right).
    \label{aleotric-uncertainty}
\end{align}

In GP regression, the Bayesian posterior for a test point $x \in \mathcal{X}$ when conditioned on training data $\mathbf{X}$ and $\mathbf{Y}$, acts as a `prediction' for the epistemic uncertainty of the test data responses. With all terms being Gaussian, this posterior has a closed-form conditional Gaussian expression.
Having also chosen the aleoteric data uncertainty to be modelled as Gaussian in (\ref{aleotric-uncertainty}), GP regression models the test data response $y \in \mathbb{R}$ as
\begin{align}
    y \vert \mathbf{X}, \mathbf{Y}, \sigma^2
    \sim \mathcal{N}\left(\bar{m}(x), \bar{k}(x, x)\right),
    \label{gp-posterior-normal}
\end{align}
with
\begin{align}
    \label{gp-posterior-mean}
    \bar{m}(x) = m(x) + \mathbf{K}_{x, \mathbf{X}} \left(\mathbf{K}_{\mathbf{X}, \mathbf{X}} + \sigma^2 \mathbf{I}\right)^{-1} \left( \mathbf{Y} - \mathbf{m}_{\mathbf{X}}\right)
\end{align}
and
\begin{align}
    \label{gp-posterior-covariance}
    \bar{k}(x, x) = k(x, x) - \mathbf{K}_{x, \mathbf{X}} \left(\mathbf{K}_{\mathbf{X}, \mathbf{X}} + \sigma^2 \mathbf{I}\right)^{-1} \mathbf{K}_{\mathbf{X}, x},
\end{align}
where $\mathbf{m}_{\mathbf{X}}$ is more verbose notation for (\ref{gp-mean-vector}), $\mathbf{K}_{\mathbf{X}, \mathbf{X}}$ is verbose for (\ref{gp-kernel-matrix}), $\mathbf{K}_{x, \mathbf{X}} \in \mathbb{R}^{1 \times N}$ and $\mathbf{K}_{\mathbf{X}, x} \in \mathbb{R}^{N \times 1}$ are gram matrices constructed with $\mathbf{X}$ and $x$ following (\ref{gp-kernel-matrix}), and $\mathbf{I} \in \mathbb{R}^{N \times N}$ is the identity matrix.

\subsection{Gaussian Process Classification}
Consider the classification task where we have $N$ observation pairs $(\mathbf{X}, \mathbf{Y}) \coloneqq \left\{(x_n, y_n)\right\}_{n=1}^{N}$ with inputs $x_n \in \mathcal{X}$ and label responses $y_n \in \{1, \dots, J\}$. In other words, we wish to map each input $x_n$ to one of $J$ labels. Following the approach from \cite{matthews2017scalable}, GPs can be used for classification through the model
\begin{align}
    Y \sim \mathcal{C}\Big(s\left(F_1(x), \dots, F_J(x)\right)\Big),
    \label{gp-classifier}
\end{align}
where we construct GPs $F_j \sim \GP\left(m_j, k_j\right)$ for each label $j=1, \dots J$, such that $F_1, \dots, F_J$ are stochastically independent GPs and $s: \mathbb{R}^J \rightarrow \Delta(J)$ is a mapping to a $J$ dimensional probability simplex parameterising a categorical distribution $\mathcal{C}$. In other words, the probability of the $j^{th}$ label is
\begin{align}
    \mathbb{P}(Y=j) = s_j(F_1(x), \dots, F_J(x)),
\end{align}
the $j^{th}$ element of the probability simplex $s(\cdot)$.

\paragraph{The Robust Max Function}\cite{matthews2017scalable} provides a few different choices for $s$ in (\ref{gp-classifier}). We follow \cite{wild2022generalized}, using the robust max function to define the $j^{th}$ element of the probability simplex with
\begin{align}
s_{j}\left(f_1, \dots, f_J\right) = \begin{cases}
      1-\delta, &  \text{if } j = \argmax_{j=1\dots J}\left(f_j\right), \\
      \frac{\delta}{J-1}, & \text{otherwise}, \\
   \end{cases}
   \label{robust-max-function}
\end{align}
where $\delta \in [0, 1]$. 
Typically, $\delta$ is chosen as a very small value (i.e. $1e^{-2}$). 
Constructing the $\Delta(J)$ vector with (\ref{robust-max-function}), we have the probability value of $1-\delta$ for the label of maximum value and $\frac{\delta}{J-1}$ otherwise. 
\cite{wild2022generalized} explains that this formulation provides robustness to outliers, as it only considers the ranking of the GP models for each label.

A benefit of the robust max function is that the expected log-likelihood under the categorical distribution in (\ref{gp-classifier}) becomes analytically tractable. \cite{wild2022generalized} shows that with $N$ input and response pairs, the expected log-likelihood is
\begin{align}
    \mathbb{E} \left[\log p\left(y \vert x\right)\right] \approx \sum_{n=1}^N \log(1-\epsilon) S(x_n, y_n) + \log\left(\frac{\epsilon}{J-1}\right) \left(1-S(x_n, y_n)\right),
    \label{robust-max-function-expected-log-likelihood}
\end{align}
with
\begin{align}
    S(x, j) \coloneqq \frac{1}{\sqrt{\pi}}\sum_{i=1}^{I} w_i \left(\prod_{j'=1, j'\neq j}^J \phi\left(\frac{\xi_i\sqrt{(2 k_{j'}(x, x)}+m_j(x) - m_{j'}(x)}{\sqrt{k_{j'}(x, x)}}\right)\right)
\end{align}
and $\left\{w_i, \xi_i\right\}_{i=1}^I$ being the weights and roots of the Hermite polynomial of order $I \in \mathbb{N}$. $\phi(\cdot)$ is the standard normal cumulative distribution function.

\newpage
\section{Variational Inference for Gaussian Processes}\label{section:vi-gp}
Although they are analytically tractable, a major drawback of GPs has been their inability to scale computationally with respect to $N$, the number of training points.
Both classification and regression predictive posteriors rely on evaluating the inversion of an $\mathbb{R}^{N \times N}$ matrix in (\ref{gp-posterior-mean}) and (\ref{gp-posterior-covariance}).
This operation has computational complexity $\mathcal{O}(N^3)$ and space complexity $\mathcal{O}(N^2)$, both of which quickly become problematic when scaling to larger-sized training sets.
This problem has been a serious limitation of GPs and has restricted their use to problem domains of smaller sized data sets.
In this section, we will review variational inference (VI) for GPs to obtain computationally cheaper approximations of the true predictive posterior.
We will also discuss the challenges of learning within this framework.

\subsection{Variational Inference in Finite Dimensions}\label{section:vi-in-finite-dimensions}
Although GPs are objects constructed in an infinite dimensional setting, this section will first review VI in a finite dimensional setting.

A Bayesian approach to modelling begins with assuming a data generating process for an observation $y \in \mathbb{R}$ as conditionally dependent on $M$ unobserved latent random variables $Z \in \mathcal{X}^M$ through a likelihood $p(Y=y\vert Z)$ and a prior on the latent variables $p(Z)$.
These are used for the Bayesian posterior 
\begin{align}
    p(Z \vert Y=y) \propto p(Y=y\vert Z)p(Z),
    \label{bayesian-posterior}
\end{align}
which is often computationally and/or analytically intractable.
This has motivated the need for variational methods like VI to approximate (\ref{bayesian-posterior}).
VI is based on the observation that the Kullback-Leibler (KL) divergence between an arbitrary probability measure $Q \coloneqq \mathbb{Q}(Z)$ and the true Bayesian posterior in (\ref{bayesian-posterior}), which we denote $\mathbb{P}(Z\vert Y)$, can be rewritten as
\begin{align}
    \KLD\left[Q, \mathbb{P}(Z \vert Y)\right] = L(Q) - \log p(y),
    \label{finite-dimensional-vi-kld}
\end{align}
where
\begin{align}
    L(Q) \coloneqq -\mathbb{E}_{Q}\left[\log p(y \vert Z)\right] + \KLD\left[Q, P\right],
    \label{finite-dimensional-vi-loss}
\end{align}
having $P \coloneqq \mathbb{P}(Z)$ the prior, $p(y) = \int p(y\vert z)p(z) dz$ the marginal log-likelihood, $\mathbb{E}_{Q}\left[\wc \right]$ denoting the expectation with respect to $Q$, and $\KLD\left[\wc, \wc\right]$ denoting the KL divergence.
It follows that
\begin{align}
    \argmin_{Q \in \boldsymbol{Q}} \KLD\left[Q, \mathbb{P}(Z\vert Y)\right] = \argmin_{Q \in \boldsymbol{Q}} L(Q),
    \label{optimal-approximation-vi}
\end{align}
where $\boldsymbol{Q}$ is a set of candidate probability measures.
In other words, approximating the posterior by minimising (\ref{finite-dimensional-vi-kld}) is equivalent to minimising (\ref{finite-dimensional-vi-loss}). Typically, $\boldsymbol{Q}$ is constructed with respect to a parameter set $\boldsymbol{\Gamma}$ such that
\begin{align}
    \boldsymbol{Q} \coloneqq \left\{Q_{\gamma}: \gamma \in \boldsymbol{\Gamma} \right\},
\end{align}
where $\boldsymbol{\Gamma}$ is a euclidean parameter space such that solving
\begin{align}
    \gamma^* \in \argmin_{\gamma \in \boldsymbol{\Gamma}} L(Q_{\gamma})
\end{align}
obtains $Q_{\gamma^*}$ the VI approximation of the Bayesian posterior minimising (\ref{finite-dimensional-vi-kld}). The VI procedure depends on three important assumptions to ensure a reasonable approximation:
\begin{enumerate}
    \item the parameterised set of measures $\boldsymbol{Q}$ is large enough to contain a reasonable approximation of the true Bayesian posterior,
    \item the parameterisation of $Q$ ensures that $L(Q)$ is tractable or easy to approximate, and
    \item there exists an optimisation procedure that can find a reasonable minimiser $\gamma^*$.
\end{enumerate}
These three assumptions are in tension with each other.
For example, a larger approximation space $\boldsymbol{Q}$ can cause $L(Q)$ to be intractable or create a more difficult optimisation setup.
However in practice, VI can be quite successful if employed by well-informed practitioners.

\subsection[Sparse Variational Gaussian Processes]{Sparse\footnote{Also known as stochastic or scaleable variational Gaussian processes (svGPs)} Variational Gaussian Processes}\label{section:svgp}
To overcome the computationally intractable GP we review \cite{titsias2009variational}, which proposed the variational approximation for the Bayesian predictive posterior in (\ref{gp-posterior-normal}) as
\begin{align}
    \mathbb{Q}(F) \coloneqq \GP(m_Q, r),
\end{align}
where the mean function is
\begin{align}
    \label{svgp-mean}
    m_Q(x) = m_P(x) + \mathbf{K}_{x, \mathbf{Z}}\left(\mathbf{K}_{\mathbf{Z}, \mathbf{Z}}\right)^{-1} \boldsymbol{\mu}
\end{align}
and the kernel function is
\begin{align}
r(x, x) = k(x, x) - \mathbf{K}_{x, \mathbf{Z}}\left(\mathbf{K}_{\mathbf{Z}, \mathbf{Z}}\right)^{-1} \mathbf{K}_{\mathbf{Z}, x} + \mathbf{K}_{x, \mathbf{Z}}\left(\mathbf{K}_{\mathbf{Z}, \mathbf{Z}}\right)^{-1}\mathbf{\Sigma}\left(\mathbf{K}_{\mathbf{Z}, \mathbf{Z}}\right)^{-1} \mathbf{K}_{\mathbf{Z}, x}.
\label{svgp-covariance}
\end{align}
This defines a parameter space
\begin{align}
    \mathbf{\Gamma} = \left\{\boldsymbol{\mu} \in \mathbb{R}^{M}, \mathbf{\Sigma} \in \mathbb{R}^{M\times M}_{\succcurlyeq 0}, \mathbf{Z} \in \mathcal{X}^M \right\},
    \label{svgp-parameter-space}
\end{align}
where $m_P$ and $k$ are the mean and kernel functions of the target GP that we want to approximate. $\mathbf{Z}$ is $M$ inducing points, typically chosen as some subset of $\mathbf{X}$.
Following (\ref{finite-dimensional-vi-loss}), the variational loss for a candidate $Q \coloneqq \mathbb{Q}(F)$ becomes
\begin{align}
L(Q) = -\sum_{n-1}^N \mathbb{E}_Q \left[\log p(y_n \vert F(x_n)\right] + \KLD\left[\mathbb{Q}^F, \mathbb{P}^F\right],
\label{svgp-vi-loss}
\end{align}
where the expectation in (\ref{svgp-vi-loss}) is tractable, since $F(x_n)$ is Gaussian under $Q$.
However it is unclear if the KL divergence between two GPs in the second term is even well-defined and from a practical viewpoint, computable.
\cite{matthews2016sparse} points out that the choice of $m_Q$ and $r$ by \cite{titsias2009variational} ensures that
\begin{align}
    \KLD\left[\GP\left(m_Q, r\right), \GP\left(m_P, k\right)\right] = \KLD\left[\mathcal{N}\left(\boldsymbol{\mu}, \mathbf{\Sigma}\right), \mathcal{N}\left(\mathbf{m}_{\mathbf{Z}}, \mathbf{K}_{\mathbf{Z}, \mathbf{Z}}\right)\right],
\end{align}
reducing the KL divergence between two stochastic processes to
\begin{align}
        \KLD\left[\mathbb{Q}^F, \mathbb{P}^F\right]
    = \frac{1}{2}\left( \tr\left(\left(\mathbf{K}_{\mathbf{Z}, \mathbf{Z}}\right)^{-1} \boldsymbol{\Sigma}\right) - M +
    \left(\mathbf{m}_{\mathbf{Z}} - \boldsymbol{\mu}\right)^T \mathbf{K}_{\mathbf{Z}, \mathbf{Z}}^{-1} \left(\mathbf{m}_{\mathbf{Z}} - \boldsymbol{\mu}\right)+ \log\left(\frac{\det\mathbf{K}_{\mathbf{Z}, \mathbf{Z}}}{\det\boldsymbol{\Sigma}}\right) \right),
    \label{kld-closed-form}
\end{align}
the KL divergence between two finite dimensional Gaussian distributions on $\mathbb{R}^M$.
\cite{titsias2009variational} shows that for a given set of inducing points $\mathbf{Z}$, the optimal choices $\boldsymbol{\mu}^*$ and $\mathbf{\Sigma}^*$ to minimise (\ref{svgp-vi-loss}) have the closed forms
\begin{align}
    \label{svgp-optimal-mean}
    \boldsymbol{\mu}^* = \sigma^{-2}\mathbf{K}_{\mathbf{Z}, \mathbf{Z}} \mathbf{\Psi}^{-1}\mathbf{K}_{\mathbf{Z}, \mathbf{X}}  \left(\mathbf{Y} - \mathbf{m}_\mathbf{X}\right)
\end{align}
and
\begin{align}
    \label{svgp-optimal-covariance}
    \mathbf{\Sigma}^* = \mathbf{K}_{\mathbf{Z}, \mathbf{Z}}  \mathbf{\Psi}^{-1}\mathbf{K}_{\mathbf{Z}, \mathbf{Z}},
\end{align}
where
\begin{align}
    \mathbf{\Psi} \coloneqq \mathbf{K}_{\mathbf{Z}, \mathbf{Z}}  + \sigma^{-2}\mathbf{K}_{\mathbf{Z}, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{Z}},
    \label{svgp-optimal-sigma-m}
\end{align}
conveniently eliminating the need for gradient-based optimisations.
This parameterises the approximate GP with $\gamma = \left(\boldsymbol{\mu}^*, \mathbf{\Sigma}^*,  \mathbf{Z}\right)$.

This formulation ensures the matrix inversion of $\mathbb{R}^{M \times M}$ matrices having $\mathcal{O}\left(M^3\right)$ computational complexity and the operation $\mathbf{K}_{\mathbf{Z}, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{Z}}$ in (\ref{svgp-optimal-sigma-m}) having computational complexity $\mathcal{O}\left(NM^2\right)$.
Thus, the overall computational complexity of this approach is $\mathcal{O}\left(NM^2\right)$ with space complexity $\mathcal{O}\left(NM\right)$.
This significantly improves the scalability of GP approaches from the traditional GP in Section \ref{section:gaussian-processes}. $M$ is typically chosen as $\mathcal{O}(N^{1/2})$ such that the svGP has $\mathcal{O}(N^{2})$ and $\mathcal{O}(N^{3/2})$ time and space complexity respectively.

This svGP formulation provides a solution to the scaling issues of the GP but illustrates how the three assumptions of VI discussed in Section \ref{section:vi-in-finite-dimensions} can break down.
In particular, the variational set $\mathbf{Q}$ defined by the parameter set $\boldsymbol{\Gamma}$ may not be expressive enough to contain a reasonable approximation of the true Bayesian posterior.
For example, the mean function of the true posterior with a zero mean GP can be expressed as the linear combination
\begin{align}
    \hat{m}(x) = \sum_{n=1}^{N} \alpha_n k(x, x_n) \in \text{span} \left(\left\{k(\cdot, x_1), \dots, k(\cdot, x_N)\right\}\right),
\end{align}
with $\left[\alpha_1 \cdots \alpha_N\right]^T = \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1}\mathbf{Y}$.
On the other hand, the corresponding variational mean $m_Q$ in (\ref{svgp-mean}) is only a linear combination within the inducing point space such that
\begin{align}
    m_Q(x) = \sum_{m=1}^{M} \beta_m k(x, z_m) \in \text{span}\left(\left\{k(\cdot, z_1), \dots, k(\cdot, z_M))\right\}\right),
\end{align}
with $\left[\beta_1 \cdots \beta_M\right]^T = \mathbf{K}_{\mathbf{Z}, \mathbf{Z}}^{-1}\boldsymbol{\mu}$.
Choosing $M$ as $\mathcal{O}(N^{1/2})$, it is not unlikely that the inducing point space will be too small to contain a reasonable approximation of $\hat{m}$.
A similar argument can be made for the posterior kernel.
\cite{wild2022generalized} explains that the KL divergence between two GPs is not generally tractable or even finite.
Thus within the Bayesian context of VI, we are forced to restrict the variational set $\boldsymbol{Q}$, to obtain a tractable loss $L$.

\subsection{Inducing Point Selection}\label{section:inducing-point-selection}
Within the context of VI, we wish to find an optimal set of inducing points such that
\begin{align}
    \text{span} \left(\left\{k(\cdot, x_1), \dots, k(\cdot, x_N)\right\}\right) \approx \text{span}\left(\left\{k(\cdot, z_1), \dots, k(\cdot, z_M))\right\}\right),
\end{align}
where the inducing points can approximate the majority of the input space. Several methods have been explored to achieve this \jw{citation needed}.
Algorithm \ref{alg:inducing-point-selection} reviews \cite{burt2020convergence}, which proposes an iterative selection procedure that greedily chooses the next inducing point based on the highest marginal variance in the prior when conditioned on the currently selected set of inducing points.

\begin{algorithm}
\caption{Greedy Variance Inducing Point Selection}\label{alg:inducing-point-selection}
\begin{algorithmic}
\State $m \leftarrow 1$
 \State $i \leftarrow \argmax \left(\diag\mathbf{K}_{\mathbf{X}, \mathbf{X}}\right) $
 \State $\mathbf{z} \leftarrow \{x_i\}$
\While{$m < M$}
\State$i \leftarrow \argmax \left(\diag \left(\mathbf{K}_{\mathbf{X}, \mathbf{X}} - \mathbf{K}_{\mathbf{X}, \mathbf{z}} \left(\mathbf{K}_{\mathbf{z}, \mathbf{z}}\right)^{-1}\mathbf{K}_{\mathbf{z}, \mathbf{X}}\right)\right)$
 \State  $\mathbf{z} \leftarrow \mathbf{z} \cup \{x_i\}$
 \State  $m \leftarrow m+1$
\EndWhile
\State \Return $\mathbf{z}$
\end{algorithmic}
\end{algorithm}

In the loop of Algorithm \ref{alg:inducing-point-selection}, each element of the diagonal evaluation has computational complexity $\mathcal{O}(M^2)$ from matrix multiplication, so computing for all $N$ candidate points along the diagonal is $\mathcal{O}(NM^2)$.
The matrix inversion is $\mathcal{O}(M^3)$, but remains a constant when computing each element of the diagonal.
Thus the complexity of selecting each inducing point is $\mathcal{O}(NM^2)$, assuming $M << N$.
Looping to select $M$ inducing points, Algorithm \ref{alg:inducing-point-selection} has $\mathcal{O}(NM^3)$ computational complexity.
Choosing $M$ as $\mathcal{O}(N^{1/2})$, we have $\mathcal{O}(N^{5/2})$ computational complexity and $\mathcal{O}(N)$ space complexity.

The inducing point selection method in Algorithm \ref{alg:inducing-points-prior-gp} requires a pre-selected kernel.
However in practice, kernel selection generally involves negative log likelihood optimisation on training data.
In the case of learning the prior GP, this would involve learning the kernel hyper-parameters on the selected inducing points.
This presents a chicken and egg problem.
\cite{burt2020convergence} proposes an EM-like approach, iteratively learning the kernel hyper-parameters and selecting the inducing points with Algorithm \ref{alg:inducing-point-selection}. \cite{burt2020convergence} iterates this process until convergence of the evidence lower bound (ELBO), however our implementation takes a more naive approach iterating until the inducing points do not change. We summarise this in Algorithm \ref{alg:inducing-points-prior-gp}.

\begin{algorithm}[h!]
\caption{Prior Kernel Learning and Inducing Points Selection}\label{alg:inducing-points-prior-gp}
\begin{algorithmic}
\Require $\theta_0$ \Comment{initial hyper-parameters of $k$}
 \State $i \leftarrow 0$
  \State $\mathbf{Z}_0 \leftarrow$ \Call{GreedyVarianceSelection}{$\mathbf{X}$, $\theta_0$} \Comment{via Algorithm \ref{alg:inducing-point-selection} with $k$}
  \State $\mathbf{U}_i \leftarrow \mathbf{Y}\left[\mathbf{X}.\text{index}\left(\mathbf{Z}_0\right)\right]$ \Comment{Corresponding inducing point responses}
\While{$\mathbf{Z}_i \neq \mathbf{Z}_{i-1}$}
\State $\theta_{i+1} \leftarrow$ \Call{OptimiseNLL}{$\theta_0$, $\mathbf{Z}_i$, $\mathbf{U}_i$} \Comment{via type-II maximimum likelihood on inducing points}
\State $\mathbf{Z}_{i+1} \leftarrow$ \Call{GreedyVarianceSelection}{$\mathbf{X}$, $\theta_{i+1}$}
\State $\mathbf{U}_{i+1} \leftarrow \mathbf{Y}\left[\mathbf{X}.\text{index}\left(\mathbf{Z}_i\right)\right]$
\State $i \leftarrow i+1$
\EndWhile
\State \Return ($\theta_i$, $\mathbf{Z}_i$)
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:inducing-points-prior-gp} incorporates response data $\mathbf{Y}$ into the inducing point selection process, which did not exist in Algorithm \ref{alg:inducing-point-selection}. 
Learning the kernel with the response data can give more appropriate covariances when comparing candidate points $\mathbf{X}$ during the inducing points selection of Algorithm \ref{alg:inducing-point-selection}. 
Figure \ref{inducing-points-and-kernel} visualises the inducing points selected for the first four iterations of Algorithm \ref{alg:inducing-points-prior-gp} overlayed with the Bayesian posterior of a zero mean GP parameterised with the learned kernel, conditioned on those selected inducing points. 
This visualises the inducing point selection procedure, attempting to minimise large uncertainty bounds over the training points to ensure maximum variance coverage of the selected points.
By learning the kernel in between iterations, we see that Algorithm \ref{alg:inducing-points-prior-gp} can also appropriately adjust the variances on training data, which was held fixed during Algorithm \ref{alg:inducing-point-selection}.

\begin{figure}[h!]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/inducing_points_kernel/iteration-0/prediction.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/inducing_points_kernel/iteration-2/prediction.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/inducing_points_kernel/iteration-1/prediction.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/inducing_points_kernel/iteration-3/prediction.png}
\end{minipage}%
\caption{Inducing Point Selection for Four Iterations of Algorithm \ref{alg:inducing-points-prior-gp}}
\label{inducing-points-and-kernel}
\end{figure}

\newpage
\section{Generalised Variational Inference for Gaussian Processes}\label{section:gvi-for-gps}
The variational objective in (\ref{svgp-vi-loss}) is the root cause of the restrictive variational set for the svGP. This is because GP learning in the VI framework necessitates a KL divergence, which is often ill-defined.
This section will review generalised variational inference (GVI) in finite dimensions from \cite{knoblauch2022optimization}, a learning framework that does not rely on the KL divergence.
This will be followed by a review of \cite{wild2022generalized} which extends GVI to an infinite dimensional setting to construct a GP learning framework that also avoids the KL divergence, providing richer approximation spaces.

\subsection{Generalising Variational Inference in Finite Dimensions}
Extending the Bayesian posterior from (\ref{bayesian-posterior}) to $N$ observations we have
\begin{align}
    \label{bayesian-posterior-gvi}
    q_B(Z) \coloneqq p(Z\vert Y=y) \propto p(Z) \prod_{n=1}^N p(Y=y_n \vert Z),
\end{align}
where $p(Z)$ is the prior for the latent variables and $p(Y=y \vert Z)$ is the likelihood of an observation $y \in \mathbb{R}$.
The Bayesian posterior $q_B(Z)$ is traditionally viewed as an updated belief for the latent variables by incorporating the likelihood evaluation of new observations.
This interpretation is commonly cited in the context of statistical modelling, where practitioners are focused on correct model specification to characterise an underlying data generation process. The validity of the belief update relies on three assumptions:
\begin{enumerate}
    \item a well-specified prior,
    \item a well-specified likelihood, and
    \item an analytically and/or computationally tractable posterior, or the existence of a tractable and reasonable VI approximation,
\end{enumerate}
which are often violated in settings of larger-scaled models like Bayesian Neural Networks (BNNs).
For example in BNNs, Gaussian priors are generally chosen for computational convenience and are most likely mis-specified in the Bayesian context.
The likelihood evaluation of BNNs are also most definitely mis-specified, as it's unlikely that evaluating such an over-parameterised model would provide any meaningful insights into the data likelihood.
Finally, the posterior of BNNs are often approximated either through samplers or variational approximations.
To achieve sampling convergence, larger-scaled models may require much more computational resources and time than is practically available.
A reasonable variational approximation is also not always guaranteed, as discussed in Section \ref{section:vi-in-finite-dimensions}.

\cite{knoblauch2022optimization} introduces a new interpretation of the Bayesian posterior, showing that it is also the solution of the optimisation problem
\begin{align}
    q_B(Z) = \argmin_{Q \in \boldsymbol{Q}} \left\{\mathbb{E}_{Q}\left[\sum_{n=1}^N \ell \left(Z, y_n\right)\right] + \mathbb{D}\left[Q, P\right]\right\}
    \label{gvi-posterior}
\end{align}
when choosing $\boldsymbol{Q}$ as the space of all probability measures on $Z$, the negative log-likelihood $\ell(Z, y) =-\log p\left(y \vert Z\right)$,  and KL divergence $\mathbb{D}\left[Q, P\right] = \KLD\left[Q, P\right]$ where $P$ is the prior.
More generally, $\ell$ is called the loss function and $\mathbb{D}$ is the divergence. With this interpretation, the Bayesian posterior is the solution of a regularised empirical risk minimisation problem.

Framed through optimisation, the Bayesian posterior will always be a valid solution of (\ref{gvi-posterior}), regardless of the three  assumptions required to ensure a valid belief update.
This is more in-tune with practitioners of larger-scaled models who focus on predictive performance rather than model specification.
\cite{knoblauch2022optimization} also shows that by generalising the Bayesian posterior within the learning framework of (\ref{gvi-posterior}), any choice of prior $P$, valid divergence $\mathbb{D}$, loss $\ell$, and approximation set $\boldsymbol{Q}$ will result in a generalised posterior that maintains interpretations as a belief update.
This flexible inference approach will motivate the replacement of the KL divergence that was problematic in VI for GPs reviewed in Section \ref{section:vi-gp}.

\subsection{Generalising Variational Inference on Function Spaces}
This section reviews \cite{wild2022generalized}, which extends GVI to infinite dimensional settings for quantifying uncertainties on function spaces and proposes  a new framework for variational learning of GPs.

\cite{wild2022generalized} shows that GVI in function spaces involves solving
\begin{align}
    Q^* = \argmin_{Q \in \boldsymbol{Q}} \left\{\mathbb{E}_{Q}\left[\sum_{n=1}^{N}\ell(F, y_n)\right] + \mathbb{D}\left[Q, \mathbb{P}(F)\right]\right\},
    \label{gvi-posterior-in-fs}
\end{align}
where $Q \in \boldsymbol{Q}$ is a variational family of probability measures on a function space $\boldsymbol{\mathcal{F}}$ and $\mathbb{P}(F)$ is a prior on $\boldsymbol{\mathcal{F}}$.
\cite{wild2022generalized} explains that the Kolomogorov Extension Theorem ensures that for a given GP, there exists a corresponding Gaussian measure (GM) on the trivial function space $\boldsymbol{\mathcal{F}} = \left\{f: \mathcal{X} \rightarrow \mathbb{R}\right\}$.
However, this space is highly prone to support mis-match.
Ensuring a tractable KL on $\boldsymbol{\mathcal{F}}$ requires heavy restrictions on the variational family $\boldsymbol{Q}$, as shown in Section \ref{section:svgp}.
Instead, \cite{wild2022generalized} identifies the existence of GPs with corresponding GMs on the Hilbert space of square integrable functions $\mathcal{L}^2\left(\mathcal{X}, \rho, \mathbb{R}\right)$ when the mean satisfies
\begin{align}
    m \in \mathcal{L}^2\left(\mathcal{X}, \rho, \mathbb{R}\right)
    \label{l2-mean}
\end{align}
and the kernel satisfies
\begin{align}
    \int k(x, x) d\rho(x) < \infty,
    \label{l2-kernel}
\end{align}
also known as the trace-class kernel condition.
These conditions guarantee that sample functions from the GP will have square-integrable paths such that there exists $\mathbb{P} \coloneqq \mathcal{N}(m, C)$, a GM on $\mathcal{L}^2$ with the same mean $m$ and a covariance operator
\begin{align}
    C(f) \coloneqq \int_{\mathcal{X}} k(\cdot, x) f(x) d\rho(x),
\end{align}
for any function $f \in \mathcal{L}^2$.
With the GVI objective in (\ref{gvi-posterior-in-fs}), \cite{wild2022generalized} proposes Gaussian Wasserstein Inference (GWI), replacing the KL divergence with the 2-Wasserstein distance between GMs on Hilbert spaces given as
\begin{align}
    \mathbb{W}\left[Q, P\right]^2 = \left\|m_P - m_Q\right\|_2^2 + \tr\left(C_P\right) + \tr\left(C_Q\right) - 2 \tr \left[\left(\left(C_P\right)^{1/2} C_Q \left(C_P\right)^{1/2} \right)^{1/2}\right],
    \label{wasserstein-distance}
\end{align}
which is always well-defined and does not have the support mis-match problem of the KL divergence.
The subscripts denote the respective GM of the object, $\tr(\cdot)$ is the trace of an operator, and $(C)^{1/2}$ denotes the square root of the positive self-adjoint operator $C$.
Most mean functions satisfy (\ref{l2-mean}) and most kernels satisfy (\ref{l2-kernel}), ensuring the existence of the corresponding Gaussian measures P and Q on $\mathcal{L}^2$.
This provides significantly more freedom for the variational set $\boldsymbol{Q}$ than the svGP approach from \cite{titsias2009variational}.

With GWI, \cite{wild2022generalized} proposes GWI-net, a variational GP parameterised with a neural network mean to replace the svGP mean in (\ref{svgp-mean}).
The experimental results of this approach in \cite{wild2022generalized} showed promising potential in both regression and classification tasks.

\newpage

\section{Improving Gaussian Wasserstein Inference}\label{section:improving-gwi}
In this section we expand on GWI, the function space GVI framework from \cite{wild2022generalized}.
We introduce new prior kernels and new variational kernels to GWI, further exploring the flexibility of the learning framework.
We also introduce a new modular GWI learning procedure.
GPs learned in this framework will be called GWI-GPs.
Finally, we describe some numerical approximations that we developed for the Wasserstein distance to significantly improve inference speed with minor trade-offs.

\subsection{Improved Kernel Selection}
The GWI framework is parameterised in terms of the infinite-dimensional parameters:
\begin{itemize}
    \setlength\itemsep{0em}
    \item $m_P \in \mathcal{L}^2$, a prior mean function,
    \item $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, a prior (trace-class) kernel,
    \item $m_Q \in \mathcal{L}^2$, a variational mean function, and
    \item $r: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$, a variational (trace-class) kernel,
\end{itemize}
to construct the objective in (\ref{gvi-posterior-in-fs}). \cite{wild2022generalized} proposes GWI-net, parameterising $m_P$ as the zero mean function, $k$ as the ARD kernel, $m_Q$ as a neural network, and $r$ as the svGP kernel
\begin{align}
    r(x, x') = k(x, x') - \mathbf{K}_{x, \mathbf{Z}} \mathbf{K}_{\mathbf{Z}, \mathbf{Z}} \mathbf{K}_{\mathbf{Z}, x'} + \mathbf{K}_{x, \mathbf{Z}} \mathbf{\Sigma} \mathbf{K}_{\mathbf{Z}, x'},
    \label{gwi-svgp-kernel}
\end{align}
where $\mathbf{\Sigma} \in \mathbb{R}^{M \times M}_{\succcurlyeq 0}$ parameterises $r$. The following sections will introduce new parameterisations for the prior and variational kernel.

\subsubsection{Prior Kernels}\label{prior-kernels}
The GWI-net construction from \cite{wild2022generalized} uses an ARD kernel for the prior
\begin{align}
    k(x, x') = \sigma^2_f \exp\left(-\frac{1}{2} \sum_{d=1}^D \frac{1}{\alpha_d^2}(x_d-x_d')^2\right),
\end{align}
where $\sigma_f > 0$ is the kernel scaling factor and for each dimension $d=1, \dots, D$, $\alpha_d >0$ is the corresponding length-scale.
In structured data settings like images, having a length-scale for each dimension can be less effective when it is important to learn correlations between features.
We propose using GWI with kernels that are formulated to learn better in more structure data settings. These include the neural network Gaussian process kernels (NNGP kernels) from \cite{novak2019neural} and a form of neural network kernels that we will introduce as custom feature mapping kernels.

\paragraph{NNGP Kernels} \cite{novak2019neural} shows that NNGP kernels are the infinite width limit of neural network architectures at initialisation.
We propose choosing NNGP kernels for the prior GP that will be the corresponding infinite width limit of the neural network mean function $m_Q$.
We also explore settings when $k$ is a NNGP kernel being the infinite width limit of an architecture unrelated to $m_Q$.
The learnable hyper-parameters of NNGP kernels are the variances of the weights and biases for each infinite-width limit layer of the neural network architecture.

\begin{figure}[h!]
\centering
\begin{minipage}{.25\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/mnist_inducing_point/random_mnist_inducing_point_selection.png}
\end{minipage}%
\begin{minipage}{.25\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/mnist_inducing_point/greedy_mnist_ard_inducing_point_selection.png}
\end{minipage}%
\begin{minipage}{.25\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/mnist_inducing_point/greedy_mnist_fcn_nngp_inducing_point_selection.png}
\end{minipage}%
\begin{minipage}{.25\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/mnist_inducing_point/greedy_mnist_cnn_nngp_inducing_point_selection.png}
\end{minipage}%
\caption{Inducing Point Selection with Different Prior Kernels}\label{fig:mnist-inducing-points}
\end{figure}

We show some preliminary results in Figure \ref{fig:mnist-inducing-points}, where we compare inducing point selection with different kernels on MNIST data. 
Using Algorithm \ref{alg:inducing-point-selection}, we selected 10 images from 5000 for each digit.
We include random inducing point selection as a control for visual comparison.
We can see that with the NNGP kernels, Algorithm \ref{alg:inducing-point-selection} is significantly better at selecting different instances of the same digit, providing better coverage of the data variation.
On the other hand, images selected with the ARD kernel are comprable to random selection. 
It should be noted that this is a preliminary result, solely for the purposes of showing the potential of NNGP kernels. 
These experiments were performed without any training on the kernel hyper-parameters.

\paragraph{Custom Feature Mapping Kernels} We also explore neural network kernels of the form
\begin{align}
    k(x, x') = k_0(h(x), h(x')),
    \label{custom-feature-map-kernel}
\end{align}
where $k_0$ is a base kernel that can be any kernel function and $h(\cdot)$ is any feature mapping. 
We use GWI to learn the hyper-parameters of $k_0$ and the  feature mapping $h$. 
In our experiments, we choose non-stationary kernels for $k_0$ to ensure non-zero gradients for the gram matrix diagonals $k_0(h(x), h(x))$. 
Depending on the data domain, we propose using a suitable neural network architecture for $h$, such as a convolutional neural network feature extractor for image data.



\subsubsection{Variational Kernels}\label{variational-kernels}
\cite{wild2022generalized} proposes learning the positive semi-definite matrix $\mathbf{\Sigma}$ in  (\ref{gwi-svgp-kernel}) by learning the parameters of $\mathbf{L} \in \mathbb{R}^{M \times M}$, a lower triangle matrix with positive diagonals. 
This becomes a Cholesky decomposition, ensuring $\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^T \in \mathbb{R}^{M \times M}_{\succcurlyeq 0}$. 
We call this the Cholesky parameterisation. 
This section introduces new parameterisations for the svGP kernel and new forms of variational kernels.


\paragraph{svGP Parameterisations}
We introduce a new parameterisation of $\mathbf{\Sigma}$ called the diagonal parameterisation. The diagonal parameterisation learns
\begin{align}
    \diag(\mathbf{v}): \log(\mathbf{v}) \in \mathbb{R}^M,
\end{align}
where $\mathbf{\Sigma} = \diag(\mathbf{v}) \in \mathbb{R}^{M \times M}_{\succcurlyeq 0}$, a diagonal matrix of positive elements. 
Although this new parameterisation is more restrictive than the Cholesky decomposition parameterisation, it is computationally cheaper during gradient-based learning and has shown to have comprable performance during our experiments.

We also propose a base kernel parameterisation, replacing $\mathbf{K}_{x, \mathbf{Z}} \mathbf{\Sigma} \mathbf{K}_{\mathbf{Z}, x}$, the last term of (\ref{gwi-svgp-kernel}) with
\begin{align}
    r(x, x') = k(x, x') - \mathbf{K}_{x, \mathbf{Z}} \mathbf{K}_{\mathbf{Z}, \mathbf{Z}} \mathbf{K}_{\mathbf{Z}, x'} + r_0(x, x'),
\end{align}
where $r_0$ is the base kernel, which can be any kernel function. We learn the hyper-parameters of $r_0$ with GWI.

\paragraph{Sparse Posterior Kernels}
Inspired by the GP posterior covariance in (\ref{gp-posterior-covariance}) and the svGP kernel from \cite{titsias2009variational}, we present a new form of variational kernel we call sparse posterior kernels with the formulation
\begin{align}
    r(x, x') = r_0(x, x') - r_0\left(x, \mathbf{Z}\right) r_0\left(\mathbf{Z}, \mathbf{Z}\right)^{-1} r_0\left(\mathbf{Z}, x'\right),
\end{align}
where $r_0$ is any base kernel learned during GWI such that $r_0\left(x, \mathbf{Z}\right) \in \mathbb{R}^{1 \times M}$, $r_0\left(\mathbf{Z}, \mathbf{Z}\right) \in \mathbb{R}^{M \times M}$, and $r_0\left(\mathbf{Z}, x'\right) \in \mathbb{R}^{M \times 1}$ are gram matrices of $r_0$ following the construction in (\ref{gp-kernel-matrix}).
In our experiments, we choose and initialise the parameters of $r_0$ to be the same as that of the prior kernel $k$.
These kernels have $\mathcal{O}(M^3)$ computational complexity.

\paragraph{Fixed Sparse Posterior Kernels}
During prediction, the inversion $r_0\left(\mathbf{Z}, \mathbf{Z}\right)^{-1}$ is a constant. However during inference, gradients must pass through it to learn the parameters of the base kernel. Thus, we also present a fixed version of the sparse posterior kernel
\begin{align}
    r(x, x') = r_0(x, x') - r_0\left(x, \mathbf{Z}\right) k\left(\mathbf{Z}, \mathbf{Z}\right)^{-1} r_0\left(\mathbf{Z}, x'\right),
\end{align}
where the matrix inversion is a constant and defined with respect to the prior kernel. This option provides faster training, no longer needing gradients to pass through a matrix inverse. Treating the inversion as a pre-computed constant, this kernel has $\mathcal{O}(M^2)$ computational complexity.

\subsection{An Improved Learning Procedure}
We improve on the GWI learning procedure from \cite{wild2022generalized} to propose Algorithm \ref{alg:gwi-gp}.
The rest of this section provides more details for certain steps of the procedure.
\begin{algorithm}[h!]
\caption{GWI-GP Learning}\label{alg:gwi-gp}
\begin{algorithmic}
    \Require{$k$, $m_Q$, $r$} \Comment{Use zero mean for $m_P$}
    \State ($\theta$, $\mathbf{Z}$) $\leftarrow$ \Call{KernelAndInducingPoints}{$k$} \Comment{via Algorithm \ref{alg:inducing-points-prior-gp}, $\theta$ are hyper-parameters of $k$}
    \State $P \leftarrow$ ($k$, $\theta$) \Comment{Construct Prior/Regulariser GP}
    \State $Q \leftarrow$ ($m_Q$, $r$, $\mathbf{Z}$) \Comment{Construct GWI-GP}
    \State ($\gamma_{m_Q}$, $\gamma_r$) $\leftarrow$ \Call{OptimiseGWI}{$P$, $Q$} \Comment{via GWI, $\gamma_{m_Q}$, $\gamma_r$ are hyper-parameters of $m_Q$, $r$}
    \State $\gamma_r \leftarrow$ \Call{OptimiseTemper}{$\gamma_r$} \Comment{via type-II maximimum likelihood on a validation set}
    \State \Return ($\gamma_{m_Q}$, $\gamma_r$)
\end{algorithmic}
\end{algorithm}


\paragraph{The Regulariser} The prior is an essential component of a Bayesian posterior belief update.
However in the context of GVI, the prior GP can be interpreted as a regulariser for empirical risk minimisation.
To prevent confusion in this discussion, we will call this the regulariser GP. In our experiments, we explore traditional regularisers constructed with the standard GP prior, having $\mathcal{O}(1)$ computational complexity. We also propose the GP regulariser
\begin{align}
    y \vert \mathbf{Z}, \mathbf{U}, \sigma^2
    \sim \mathcal{N}\left(\bar{m}(x), \bar{k}(x, x)\right),
    \label{gp-posterior-regulariser}
\end{align}
a `lightweight' Bayesian posterior formulated with the inducing points $\mathbf{Z}\in \mathcal{X}^M$ and corresponding responses $\mathbf{U} \in \mathbb{R}^M$.
This regulariser would violate the traditional interpretation of a prior as it would explicitly use training data twice during inference.
We loosen this interpretation with the optimisation-centric nature of GVI, viewing this as as potentially better informed regulariser than a prior.
Our experiments show that this less traditional regulariser can sometimes construct a better learning objective for GWI-GPs.
Moreover, the computational tractability of GWI is maintained with inversions of $\mathbb{R}^{M\times M}$ matrices only, when evaluating (\ref{gp-posterior-regulariser}).
\jw{maybe some images visualising a prior GP regulariser, posterior GP regulariser, and the corresponding GWI-GPs learned from them}

\paragraph{Tempering}\cite{wild2022generalized} and \cite{adlam2020cold} have explained that tempering the predictive posterior of GPs can improve predictive performance and has theoretical interpretations for correcting against a mis-specified prior. Following \cite{wild2022generalized}, we temper the  GVI-GPs by learning a factor $\alpha_T < 0$ for the tempered kernel formulated as
\begin{align}
    r_T(\mathbf{X}, \mathbf{X}) = \alpha_T \left(r(\mathbf{X}, \mathbf{X})+\sigma^2 \mathbf{I}\right),
\end{align}
where $\alpha_T$ is learned through negative log-likelihood optimisation on $\GP(m_Q, r_T)$ for a hold-out validation set with the rest of the GP parameters held fixed. 
This will form the final tempered GWI-GP predictive posterior. 
\cite{wild2022generalized} and \cite{} have argued that tempering 
\begin{align}
    y \sim \mathcal{N}(m_Q(x), \alpha_T \left( r(x, x) + \sigma^2\right).
\end{align}
Tempering is also applied to classification by constructing $\left[\alpha_T^1 \dots \alpha_T^J\right]^T \in [0, \infty)^J$, a separate tempering factor for each GP, and optimising the classification log-likelihood in (\ref{robust-max-function-expected-log-likelihood}).

\subsection{Wasserstein Distance Approximations}
\cite{wild2022generalized} provides the batched approximation of the Wasserstein distance from (\ref{wasserstein-distance}) for $N$ data points as
\begin{align}
    \mathbb{W}\left[Q, P\right]^2  &\approx \frac{1}{N_B}\sum_{n_b=1}^{N_B} \left(m_P(x_{n_b}) - m_Q(x_{n_b})\right)^2 \nonumber \\
    & \qquad + \frac{1}{N_B} \sum_{n_b=1}^{N_B} k(x_{n_b}, x_{n_b}) + \frac{1}{N_B} \sum_{n_b=1}^{N_B} r(x_{n_b}, x_{n_b}) \nonumber \\
    & \qquad - \frac{2}{\sqrt{N_B N_S}} \sum_{n_s=1}^{N_S} \sqrt{\lambda_{n_s}\left\big(r\left(\mathbf{X}_{N_S}, \mathbf{X}_{N_B}\right)k\left(\mathbf{X}_{N_B}, \mathbf{X}_{N_S}\right)\right\big)},
    \label{wasserstein-distance-approximation}
\end{align}
where $N_B < N$ and $N_S < N$ are two independent batches from the full data and $\lambda_{n}(\cdot)$ evaluates the $n^{th}$ eigenvalue. We introduce a few numerical approximations of the Wasserstein distance to improve computational performance.

\paragraph{Symmetric Matrices}In our experiments, we use the same data sample for $\mathbf{X}_{N_B}$ and $\mathbf{X}_{N_S}$, to ensure symmetric square matrices for $r\left(\mathbf{X}_{N_S}, \mathbf{X}_{N_B}\right)$ and $k\left(\mathbf{X}_{N_B}, \mathbf{X}_{N_S}\right)$ in the eigen-decomposition term of (\ref{wasserstein-distance-approximation}). 
This allows us to take advantage of eigenvalue properties of symmetric matrices, further explained in Appendix \ref{appendix:eigenvalue-symmetric-matrix}. 
We use symmetric matrices to leverage the eigendecomposition implementation in version 0.4 of Python JAX which is only available on GPUs for symmetric matrices. 
Our experiments showed that using the same batch for $\mathbf{X}_{N_S}$ and $\mathbf{X}_{N_B}$ don't seem to make any significant difference on the resulting GWI-GP.

\paragraph{Dropping the Eigendecomposition}The eigendecomposition in the last term of (\ref{wasserstein-distance-approximation}) is computationally expensive with $\mathcal{O}(N^3)$ complexity, which in our case is the sample size $N_S$.
In our experiments we found that we can often drop this last term during GWI with minor effect on the learned GWI-GP.
This trade-off of having a slightly imprecise Wasserstein distance is significantly counteracted with much faster training speeds.

\newpage
\section{Projected Generalised Variational Inference for Gaussian Processes}\label{section:pgvi}
This section proposes a new learning framework called projected GVI (pGVI) for GPs to learn variational GPs we call pGVI-GPs.
pGVI uses the same GWI procedure in Algorithm \ref{alg:gwi-gp}, except it replaces the Wasserstein regularisation with a new form of regularisation we call projected regularisation.
Projected regularisations are significantly cheaper computationally, making pGVI an extremely attractive option in practice.
We will present the general projected regularisation formulation, followed by different constructions of the projected GVI objective.

\subsection{Projected Regularisation}
The GVI framework from \cite{knoblauch2022optimization} assumes
\begin{align}
    \mathbb{D}\left[Q, P\right] = 0 \Leftrightarrow P = Q,
\end{align}
in other words, that the regulariser $\mathbb{D}$ is definite.
We propose dropping this assumption and choosing regularisers of the form
\begin{align}
    \mathbb{D}\left[Q, P\right] = \sum_{{n}=1}^{N} \mathbb{D}_0 \Big[\mathbb{Q}\left(F(x_n)\right), \mathbb{P}\left(F(x_n)\right)\Big],
    \label{projected-regulariser}
\end{align}
where $\mathbb{D}_0$ is a (base) regulariser between two probability measures on $\mathbb{R}$ and $x_n \in \mathcal{X}$.
Specifically, since both the variational and prior are GPs, we have that $\mathbb{Q}\left(F(x_n)\right) = \mathcal{N}\left(m_Q(x_n), r(x_n, x_n)\right)$ and $\mathbb{P}\left(F(x_n)\right) = \mathcal{N}\left(m_P(x_n), k(x_n, x_n)\right)$.
Therefore any base regulariser $\mathbb{D}_0$ that can be computed between two Gaussian random variables will lead to a tractable regulariser computation and therefore a tractable variational loss. We call regularisation schemes following (\ref{projected-regulariser}), projected regularisation.

We acknowledge that by only regularising against the marginal function values, we will be unconstrained with respect to correlations.
However our experiments suggest that this can act as a  sufficient regulariser to reap most of the benefits of GVI in terms of uncertainty quantification, while having significantly cheaper computational costs than GWI-GP learning or any other previous approach to function space VI such as \jw{[CITATIONS]}.

Projected regularisations are very cheap to compute.
The base regularisers that we propose are between scalar random variables having $\mathcal{O}(1)$ computational complexity for each training input $x_n$.
With $N$ points, we have $\mathcal{O}(N)$ complexity, significantly cheaper than the Wasserstein distance in (\ref{wasserstein-distance-approximation}), which is $\mathcal{O}(N^3)$.  

\subsection{Base Regularisers}
We will now review the base regularisers $\mathbb{D}_0$ used in our experiments. 
These provide different constructions of the pGVI objective.
In the following we will denote $Q_{x} \coloneqq \mathbb{Q}\left(F(x)\right)$, $P_{x} \coloneqq \mathbb{P}\left(F(x)\right)$, $\mu_p \coloneqq m_P(x)$, $\mu_q \coloneqq m_Q(x)$, $\sigma_p^2 \coloneqq k(x, x)$, and $\sigma_q^2 \coloneqq r(x, x)$.
\paragraph{The Bhattacharyya distance} Given as
\begin{align}
    \mathbb{B} \left[Q_{x}, P_{x}\right] &= \frac{1}{4} \frac{\left(\mu_p - \mu_q\right)^2}{\sigma_p^2 + \sigma_q^2} + \frac{1}{2} \log\left(\frac{\sigma_p^2 + \sigma_q^2}{2\sigma_p\sigma_q^}\right).
\end{align}
\paragraph{The Wasserstein distance} Given as
\begin{align}
    \mathbb{W} \left[Q_{x}, P_{x}\right]^2 &= \left(\mu_p - \mu_q\right)^2 + \sigma_p^2 + \sigma_q^2 - 2 \sigma_p\sigma_q
\end{align}
for Gaussians on $\mathbb{R}$.
\paragraph{The Hellinger divergence} Given as
\begin{align}
    \mathbb{H} \left[Q_{x}, P_{x}\right] = 1 - \sqrt{\frac{2\sigma_p\sigma_q}{\sigma_p^2 + \sigma_q^2}} \exp\left(\frac{1}{4} \frac{ \left(\mu_p - \mu_q\right)^2}{\sigma_p^2 + \sigma_q^2}\right).
\end{align}
\paragraph{The KL divergence} Given as
\begin{align}
    \mathbb{KL} \left[Q_{x}, P_{x}\right] &= \log\frac{\sigma_q^2}{\sigma_p^2} + \frac{\sigma_p^2 + \left(\mu_p - \mu_q\right)^2}{2 \sigma_q^2} - \frac{1}{2}
\end{align}
for Gaussian on $\mathbb{R}$.
\paragraph{The $\alpha$-Renyi divergence} Given as
\begin{align}
    \mathbb{R}_\alpha \left[Q_{x}, P_{x}\right] &= \log\frac{\sigma_p}{\sigma_q} + \frac{1}{2(\alpha-1)}\log\left(\frac{\sigma_p^2}{\alpha \sigma_p^2 + (1-\alpha) \sigma_q^2}\right)  + \frac{1}{2}\frac{\alpha \left(\mu_p - \mu_q\right)^2}{\alpha \sigma_p^2 + (1-\alpha) \sigma_q^2},
\end{align}
with $\alpha \sigma_p^2 + (1-\alpha) \sigma_q^2 > 0$.
\paragraph{A naive divergence}Finally, we also experimented with a naive divergence
\begin{align}
    \mathbb{N} \left[Q_{x}, P_{x}\right] = \left(\mu_p - \mu_q\right)^2 + \left(\sigma_q^2-\sigma_p^2\right)^2,
\end{align}
taking the squared difference of the means and covariances.

\newpage
\section{Experimentation Framework}\label{section:experimentation-framework}
This section presents the experimentation framework that we developed for learning GWI-GPs and pGVI-GPs.
We begin by describing our implementation architecture followed by our setup for scaling experimentation.
Finally we will present results for some 1-D curve regression problems to show our framework in action.
Our most up-to-date\footnote{We include the most up-to-date version of the implementation for future reference. For grading purposes, please refer to \href{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes/tree/7b81dd607ef2ace4b66f9440becb2722be0eb9e6}{this commit} as the last version prior to the submission date: \href{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes/tree/7b81dd607ef2ace4b66f9440becb2722be0eb9e6}{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes/tree/7b81dd607ef2ace4b66f9440becb2722be0eb9e6}} Python implementation is openly available on \href{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes}{GitHub}\footnote{\href{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes}{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes}}.

\subsection{Implementation Architecture}\label{implementation-architecture}
GWI and pGVI are highly flexible learning frameworks. As such, abstraction was critical to ensuring a scaleable and maintainable implementation architecture.
Luckily, the GVI framework proposed by \cite{knoblauch2022optimization} naturally translates to a clear implementation architecture that we have developed and visualised with the UML class diagram in Figure \ref{gvi-implementation}.
For visual clarity, we only included relevant attributes and methods.
We see that the GVI objective in (\ref{gvi-posterior-in-fs}) with $\ell$ and $\mathbb{D}$ has been abstracted to accommodate any valid empirical risk and regularisation, respectively.
These abstractions are exactly mirrored in our implementation architecture as abstract base classes, ensuring the same interface is inherited by all child class implementations.
\begin{figure}[h!]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}

\begin{class}[text width=5cm]{GVI}{0,0}
\attribute{empirical risk : EmpiricalRisk}
\attribute{regularisation : Regularisation}
\operation{calculate ( $\gamma$ , $\mathbf{X}$ , $\mathbf{Y}$ ) : float}
\end{class}

\begin{abstractclass}[text width=5cm]{EmpiricalRisk}{-6,-3}
\attribute{$Q$ : ApproximateGP}
\operation{calculate ( $\gamma$ , $\mathbf{X}$ , $\mathbf{Y}$ ) : float}
\end{abstractclass}


\begin{class}[text width=5cm]{NegativeLogLikelihood}{-9,-6}
\inherit{EmpiricalRisk}
\end{class}

\begin{class}[text width=5cm]{CrossEntropy}{-3,-6}
\inherit{EmpiricalRisk}
\end{class}

\begin{abstractclass}[text width=5cm]{Regularisation}{6 , -3}
\attribute{$P$ : ExactGP}
\attribute{$\theta$ : ExactGP Parameters}
\attribute{$Q$ : ApproximateGP}
\attribute{mode : \{`prior', `posterior'\}}
\operation{calculate ( $\gamma$ , $\mathbf{X}$ ) : float}
\end{abstractclass}

\begin{class}[text width=5cm]{Wasserstein}{9,-7.5}
\inherit{Regularisation}
\attribute{include eigendecomp. : bool}
\end{class}

\begin{abstractclass}[text width=5cm]{ProjectedRegularisation}{0,-7.5}
\inherit{Regularisation}
\attribute{$D$ : int}
\end{abstractclass}

\begin{class}[text width=5cm]{Bhattacharyya}{-9,-9.25}
\inherit{ProjectedRegularisation}
\end{class}

\begin{class}[text width=5cm]{Wasserstein}{-9,-11}
\inherit{ProjectedRegularisation}
\end{class}

\begin{class}[text width=5cm]{Hellinger}{-3,-11}
\inherit{ProjectedRegularisation}
\end{class}

\begin{class}[text width=5cm]{KL}{3,-11}
\inherit{ProjectedRegularisation}
\end{class}

\begin{class}[text width=5cm]{Renyi}{9,-11}
\inherit{ProjectedRegularisation}
\attribute{$\alpha$ : float}
\end{class}

\begin{class}[text width=5cm]{SquaredDifference}{9,-9.25}
\inherit{ProjectedRegularisation}
\end{class}

\composition{GVI}{}{}{EmpiricalRisk}
\composition{GVI}{}{}{Regularisation}

\end{tikzpicture}
}
\caption{UML Class Diagram of GVI Implementation Architecture}
\label{gvi-implementation}
\end{center}
\end{figure}

Algorithms \ref{alg:inducing-point-selection}, \ref{alg:gwi-gp}, and \ref{alg:inducing-points-prior-gp} are also in an abstracted form, accomodating any valid mean $m_P$ or $m_Q$, kernel $k$, and variational kernel $r$. We developed the architecture in Figure \ref{gp-implementation} for our implementation of GPs, Figure \ref{mean-implementation} for mean functions, and Figure \ref{kernel-implementation} for kernels. These architectures are clearly reflected in our code base.
\begin{figure}[h!]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}

\begin{abstractclass}[text width=5cm]{GP}{0, 4}
\attribute{$m$ : Mean}
\attribute{$k$ : Kernel}
\end{abstractclass}

\begin{abstractclass}[text width=5cm]{ExactGP}{-9, 1.25}
\inherit{GP}
\operation{predict ( $\theta$ , $\mathbf{X}$ ) : Distribution}
\end{abstractclass}

\begin{abstractclass}[text width=5cm]{GPRegression}{-3, 1.25}
\inherit{GP}
\attribute{dist. : Dist. = Gaussian}
\end{abstractclass}

\begin{abstractclass}[text width=5cm]{GPClassification}{3, 1.25}
\inherit{GP}
\attribute{dist. : Dist = Multinomial}
\end{abstractclass}

\begin{abstractclass}[text width=5cm]{ApproxGP}{9, 1.25}
\inherit{GP}
\operation{predict ( $\gamma$ , $\mathbf{X}$ ) : Distribution}
\end{abstractclass}

\begin{class}[text width=5cm]{ExactGPRegression}{-9, -3}
\inherit{ExactGP}
\inherit{GPRegression}
\end{class}

\begin{class}[text width=5cm]{ApproxGPRegression}{-3, -3}
\inherit{ApproxGP}
\inherit{GPRegression}
\end{class}

\begin{class}[text width=5cm]{ExactGPClassification}{3, -3}
\inherit{ExactGP}
\inherit{GPClassification}
\end{class}

\begin{class}[text width=5cm]{ApproxGPClassification}{9, -3}
\inherit{ApproxGP}
\inherit{GPClassification}
\end{class}


\begin{abstractclass}[text width=5cm]{Mean}{-9, 3.85}
\operation{predict ( $\theta_{m}$ , $\mathbf{X}$ ) : array}
\end{abstractclass}

\begin{abstractclass}[text width=5cm]{Kernel}{9, 3.85}
\operation{gram ( $\theta_{k}$ , $\mathbf{X}$ ) : array}
\end{abstractclass}

\composition{GP}{}{}{Mean}
\composition{GP}{}{}{Kernel}
\end{tikzpicture}
}
\caption{UML Class Diagram of GP Implementation Architecture}
\label{gp-implementation}
\end{center}
\end{figure}


\begin{figure}[h!]
\begin{center}
\resizebox{0.75\textwidth}{!}{
\begin{tikzpicture}
\begin{abstractclass}[text width=5cm]{Mean}{0, 0}
\operation{predict ( $\theta_{m}$ , $\mathbf{X}$ ) : array}
\end{abstractclass}

\begin{class}[text width=5cm]{Constant}{0, -3}
\inherit{Mean}

\end{class}
\begin{class}[text width=5cm]{SVGP}{-6, -3}
\attribute{$m_P$ : Mean}
\attribute{$\theta_{m_P}$ : Mean Parameters}
\attribute{$k$ : Kernel}
\attribute{$\theta_{k}$ : Kernel Parameters}
\attribute{$\mathbf{Z}$ : array}
\inherit{Mean}
\end{class}

\begin{class}[text width=5cm]{NeuralNetwork}{6, -3}
\attribute{architecture : list}

\inherit{Mean}
\end{class}
\end{tikzpicture}
}
\caption{UML Class Diagram of Mean Implementation Architecture}
\label{mean-implementation}
\end{center}
\end{figure}



\begin{figure}[h!]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tikzpicture}
\begin{abstractclass}[text width=5cm]{Kernel}{4, 0}
\operation{gram ( $\theta_{k}$ , $\mathbf{X}$ ) : array}
\end{abstractclass}

\begin{class}[text width=5cm]{Polynomial}{-2, -1.5}
\attribute{degree : float}
\inherit{Kernel}
\end{class}

\begin{class}[text width=5cm]{ARD}{-2, -4}
\inherit{Kernel}
\end{class}

\begin{class}[text width=5cm]{NeuralNetwork}{10, -4}
\attribute{$k_0$ : Kernel}
\attribute{$h$ : NeuralNetwork}
\inherit{Kernel}
\end{class}

\begin{class}[text width=5cm]{NNGP}{10, -1.5}
\attribute{architecture: list}
\inherit{Kernel}
\end{class}



\begin{abstractclass}[text width=5cm]{Approximate}{4, -4}
\inherit{Kernel}
\attribute{$\mathbf{Z}$ : array}
\end{abstractclass}

\begin{abstractclass}[text width=5cm]{SVGP}{1, -7}
\inherit{Approximate}
\attribute{$k$ : Kernel}
\attribute{$\theta_{k}$ : Kernel Parameters}
\end{abstractclass}

\begin{class}[text width=5cm]{SparsePosterior}{7, -7}
\inherit{Approximate}
\attribute{$r_0$ : Kernel}
\end{class}

\begin{class}[text width=5cm]{FixedSparsePosterior}{13, -7}
\inherit{Approximate}
\attribute{$k$ : Kernel}
\attribute{$\theta_{k}$ : Kernel Parameters}
\attribute{$r_0$ : Kernel}
\end{class}

\begin{class}[text width=5cm]{CholeskySVGP}{-5, -10.5}
\inherit{SVGP}
\end{class}

\begin{class}[text width=5cm]{DiagonalSVGP}{1, -10.5}
\inherit{SVGP}
\end{class}

\begin{class}[text width=5cm]{KernelisedSVGP}{7, -10.5}
\attribute{$r_0$ : Kernel}
\inherit{SVGP}
\end{class}

\end{tikzpicture}
}
\caption{UML Class Diagram of Kernel Implementation Architecture}
\label{kernel-implementation}
\end{center}
\end{figure}

Our implementation is in Python and predominantly uses JAX, a high-performance numerical computing package developed by \cite{jax2018github}.
Because there is currently no existing stable, well-maintained, and well-documented implementation of GPs using JAX, all implementations were essentially developed from scratch for the purposes of this project.
We chose JAX for its currently growing user-base, unique customisations, and to ensure compatibility with the implementation of NNGP kernels by \cite{novak2019neural}, which is also developed in JAX.
We chose Python for its current popularity within the machine learning community.
We incorporated a number of commonly used implementation techniques whenever necessary, such as adding jitter and using the Cholesky decomposition for matrix inversions.

As we continue to grow the code base, we incorporated a number of standard software engineering tools to ensure a scaleable, maintainable, and controlled implementation environment.
We introduce strict typing controls for all model parameter classes with Pydantic base models from \cite{samuel_colvin_2023_8277473}.
Pydantic models enforce typing hints during run-time that would otherwise be considered `syntactic sugar' in Python. This also provides clear guidelines when constructing GPs within our implementation framework.
Our implementation also includes a rigorous number of tests that currently has 95\% test coverage over the implementations found in the \href{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes/tree/main/src}{\code{src/}} directory, much higher than needed in most software development projects.
This also includes the use of mockers for isolated testing of different components (i.e. mockers for means and kernels to isolate testing of GP implementations). 
Finally, we use Poetry developed by \cite{Eustace} to strictly control package requirements, ensuring the version compatibility of dependencies.

\subsection{Scaling Experiments}
In addition to our implementation architecture, we developed a scaleable solution for performing large numbers of experiments.
Implemented in the \href{https://github.com/jswu18/generalised-variational-inference-for-gaussian-processes/tree/main/experiments}{\code{experiments/}} directory, we constructed schemas and resolvers for each abstraction in Section \ref{implementation-architecture}.
This allows us to define and construct experiments through \code{.yaml} configuration files.
Each \code{.yaml} outlines the configuration settings required to define an experiment (i.e. the GP, learning objective, learning rate, optimiser, etc.).
Appendix \ref{appendix:configuration-yamls} shows example \code{.yaml} configuration files used for training a regulariser GP and an approximate GP .

Generating configuration files was automated to scale our experiments to try all possible combinations of empirical risks, regularisers, mean functions, and kernels. These configurations were then submitted as jobs to a computing cluster, running our experiments in an organised and scaleable framework.

\subsection{Regression Curve Experiments}
To demonstrate our experimentation framework, we learned GWI-GPs and pGVI-GPs for ten regression curves. 
Each curve has randomly selected segments removed from the training data to visualise an out-of-data (OOD) setting.
With our framework, we constructed 1513 experiments for each curve, each defined through \code{.yaml} configuration files. 
An experiment is a unique combination of constructions for a regulariser GP, an approximate GP, and a GVI objective (among other settings). 
After training all possible combinations, we selected the configuration with the best performance. 
Given the setup of these regression curve problems, we chose the best performing experiment as the GP having the highest negative log-likelihood with respect to the training data. 
In other problem regimes, it may be more suitable to select based on a different performance metric, such as the test set negative log-likelihood. 
We visualise four GPs in Figure \ref{toy-curves-gps}, the best performing GWI-GP or pGVI-GPs for their respective curve problems.
The best performing GPs for each of the ten curves are visualised in Appendix \ref{appendix:toy-curves}.

\begin{figure}[h!]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve0/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve3/data.png}
\end{minipage}%
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve1/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve4/data.png}
\end{minipage}%
\label{toy-curves-gps}
\caption{GPs learned with GWI or pGVI}
\end{figure}

Additionally, Table X presents a subset of the configurations resulting in each of these best performing variational GPs. 
This different settings combinations in Table X gives an idea of the large search spaces available within GWI and pGVI. 
It also demonstrates the flexibility of the learning frameworks to accommodate different variational spaces and learning objectives that may be better suited for a specific problem. 

\newpage
\section{Future Work}\label{section:future-work}
Following the work that we have present, this section discusses some extensions that could be worth exploring in the future.

\paragraph{Fictitious Inducing Points} Algorithm \ref{alg:inducing-points-prior-gp} selects inducing points as a subset of existing training points within the training set.
Limiting the selection space to the training data could be restrictive. 
Learning `fictitious' inducing points that are not explicitly in the training data may help incorporate additional flexibility to the selection process.
The \code{k-means++} algorithm proposed by \cite{arthur2007k} is another common approach to inducing point selection, identifying cluster centroids as potential inducing points.
This moves away from limiting inducing point selection to the training data.
Incorporating this approach into Algorithm \ref{alg:inducing-point-selection} and \ref{alg:inducing-points-prior-gp} may improve inducing point selection in certain data regimes.

\paragraph{UCI Regression Benchmarking} 
Many variational GP approaches such as \cite{wild2022generalized}, \jw{MORE CITATIONS}, benchmark their work against the standard UCI regression datasets. 
The next step for our work will be to benchmark the experimentation framework we proposed in Section \ref{section:experimentation-framework} with these same datasets.
This will provide better understanding of the strengths and weaknesses of GWI-GPs and pGVI-GPs within the context of the existing literature for function space inference.

\paragraph{Image Classification} 
As identified by \cite{wild2022generalized}, a differentiating feature of GWI is the freedom to choose any mean function for the variational GP. 
\cite{wild2022generalized} shows promising experimental results for GWI within the context of image classification by parameterising the variational mean with a convolutional neural network.
However, \cite{wild2022generalized} uses ARD kernels to construct their variational kernel.

A natural next step is to explore the performance of GWI-GPs and pGWI-GPs using the kernels proposed in Section \ref{prior-kernels}, which have promising potential as shown in the inducing point selection of Figure \ref{fig:mnist-inducing-points}.
Moreover, the new variational kernels in Section \ref{variational-kernels} are another direction to explore within this context of classification.

\paragraph{NLP Classification} The GWI framework proposed by \cite{wild2022generalized} introduces flexibility and computational tractability previously unavailable within variational GP learning. 
This motivates exploring the computational limits of our learning framework by exploring larger data settings, such as problems in the Natural Language Processing (NLP) domain.
One such problem could be sentiment analysis. Another is named-entity recognition (NER), entity classification for words in a text (i.e. noun, verb, adjective, etc). These would both be natural modelling extensions of an image classification model.

\cite{pmlr-v119-hron20a} proposes the existence of infinite-width attention networks, a fundamental building block for many current modelling approaches in the NLP domain. 
These can be incorporated during kernel selection for our variational inference framework.
Additionally, our custom feature map kernel proposed in (\ref{custom-feature-map-kernel}) can accommodate any feature mapping for $h$ including pre-trained transformer embedding models, currently a default architecture within the NLP community. 
\newpage
\section{Conclusions}\label{section:conclusions}
Exploring the computational limits of GVI in new problem domains will provide better experimental understanding into the strengths and weaknesses of this learning framework.


\newpage
\bibliography{references}


\newpage
\appendix
\counterwithin{figure}{subsection}

\section{Appendix}

\subsection{Positive Semi-Definite Kernels} \label{appendix:positive-definite-kernel}
For a kernel function $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ defined as an inner product of features in some feature space $\mathcal{H}$:
\begin{align}
    k(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}
\end{align}
where $\phi: \mathcal{X} \rightarrow \mathcal{H}$, a gram matrix $\mathbf{K} \in \mathbb{R}^{N\times N}$ defined element-wise:
\begin{align}
    \left[\mathbf{K}\right]_{n, n'} = k(x_n, x_{n'})
\end{align}
for any $N$ points $\mathbf{X} = \left\{x_n\right\}_{n=1}^N$ with $x_n \in \mathcal{X}$, and any vector $\mathbf{v} \in \mathbb{R}^N$ then:
\begin{align}
    \mathbf{v}^T \mathbf{K} \mathbf{v} &= \sum_{n=1}^N\sum_{n'=1}^N v_n v_{n'}  \left\langle \phi(x_n), \phi(x_{n'}) \right\rangle_{\mathcal{H}} \\
    &= \left\langle\sum_{n=1}^N v_n \phi(x_n), \sum_{n'=1}^N  v_{n'}\phi(x_{n'}) \right\rangle_{\mathcal{H}} \\
    &= \left\| \sum_{n=1}^N v_n \phi(x_n) \right\|^2 \geq 0
\end{align}
showing that $\mathbf{K}$ is a positive semi-definite matrix.

\newpage
\subsection{Symmetric Matrix Eigenvalues}\label{appendix:eigenvalue-symmetric-matrix}
For square symmetric matrices $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{N \times N}$, $\sqrt{\mathbf{A}}\mathbf{B}\sqrt{\mathbf{A}}$ is also a symmetric matrix where $\sqrt{\mathbf{A}}$ is the square root such that $\mathbf{A} = \sqrt{\mathbf{A}}^T \sqrt{\mathbf{A}}$. Moreover,
\begin{align}
    \lambda_{n} \left(\mathbf{A} \mathbf{B}\right) = \lambda_{n} \left(\sqrt{\mathbf{A}}\mathbf{B}\sqrt{\mathbf{A}}\right)
\end{align}
where $\lambda_{n}(\cdot)$ computes the $n^{th}$ eigenvalue.

\newpage
\subsection{Example YAML Configurations}\label{appendix:configuration-yamls}
\begin{figure}[!h]
\begin{lstlisting}[style=yaml]
data_name: 604913dc3af2417fb1d5a21ea26e4afd
empirical_risk_break_condition: -10
empirical_risk_schema: negative_log_likelihood
inducing_points:
  inducing_points_factor: 1.0
  inducing_points_power: 3
  inducing_points_selector_schema: conditional_variance
kernel:
  kernel_kwargs:
    input_shape:
    - 1
    layers:
      layer_1:
        layer_kwargs:
          features: 10
        layer_schema: dense
      layer_2:
        layer_kwargs: null
        layer_schema: tanh
  kernel_parameters: null
  kernel_schema: nngp
number_of_iterations: 5
save_checkpoint_frequency: 1000
trainer_settings:
  batch_drop_last: false
  batch_shuffle: true
  batch_size: 1000
  learning_rate: 0.01
  number_of_epochs: 1000
  optimiser_schema: adam
  seed: 0
\end{lstlisting}
\caption{Regulariser Gaussian Process Example Configuration}
\end{figure}

\newpage
\begin{figure}[!h]
\begin{lstlisting}[style=yaml]
empirical_risk_schema: negative_log_likelihood
kernel:
  kernel_kwargs:
    diagonal_regularisation: 1.0e-10
    is_diagonal_regularisation_absolute_scale: false
  kernel_parameters: null
  kernel_schema: sparse_posterior
mean:
  mean_kwargs:
    nn_function_kwargs:
      input_shape:
      - 1
      layers:
        layer_1:
          layer_kwargs:
            features: 10
          layer_schema: dense
        layer_2:
          layer_kwargs: null
          layer_schema: tanh
        layer_3:
          layer_kwargs:
            features: 1
          layer_schema: dense
      seed: 0
    number_output_dimensions: 1
  mean_parameters: null
  mean_schema: custom
reference_name: 7b386a3faf1f4b79ac6ff6604b6bc932
regularisation:
  regularisation_kwargs:
    mode: posterior
    alpha: 0.5
  regularisation_schema: projected_renyi
save_checkpoint_frequency: 1000
trainer_settings:
  batch_drop_last: false
  batch_shuffle: true
  batch_size: 1000
  learning_rate: 0.01
  number_of_epochs: 1000
  optimiser_schema: adam
  seed: 0
\end{lstlisting}
\caption{Approximate Gaussian Process Example Configuration}
\end{figure}

\newpage
\subsection{Regression Curves}\label{appendix:toy-curves}
\begin{figure}[h!]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve0/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve2/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve4/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve6/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve8/data.png}
\end{minipage}%
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve1/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve3/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve5/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve7/data.png}
  \includegraphics[width=\linewidth]{thesis-report/figures/toy_curves/data/curve9/data.png}
\end{minipage}%
\label{toy-curves}
\caption{All ten Regression Curves}
\end{figure}

\end{document}x