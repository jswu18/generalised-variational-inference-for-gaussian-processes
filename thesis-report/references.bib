@article{wild2022generalized,
  title={Generalized variational inference in function spaces: Gaussian measures meet Bayesian deep learning},
  author={Wild, Veit David and Hu, Robert and Sejdinovic, Dino},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3716--3730},
  year={2022}
}
@article{knoblauch2022optimization,
  title={An optimization-centric view on Bayes’ rule: Reviewing and generalizing variational inference},
  author={Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={132},
  pages={1--109},
  year={2022}
}
@inproceedings{matthews2016sparse,
  title={On sparse variational methods and the Kullback-Leibler divergence between stochastic processes},
  author={Matthews, Alexander G de G and Hensman, James and Turner, Richard and Ghahramani, Zoubin},
  booktitle={Artificial Intelligence and Statistics},
  pages={231--239},
  year={2016},
  organization={PMLR}
}
@phdthesis{matthews2017scalable,
  title={Scalable Gaussian process inference using variational methods},
  author={Matthews, Alexander Graeme de Garis},
  year={2017},
  school={University of Cambridge}
}
@article{novak2019neural,
  title={Neural tangents: Fast and easy infinite neural networks in python},
  author={Novak, Roman and Xiao, Lechao and Hron, Jiri and Lee, Jaehoon and Alemi, Alexander A and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  journal={arXiv preprint arXiv:1912.02803},
  year={2019}
}
@inproceedings{titsias2009variational,
  title={Variational learning of inducing variables in sparse Gaussian processes},
  author={Titsias, Michalis},
  booktitle={Artificial intelligence and statistics},
  pages={567--574},
  year={2009},
  organization={PMLR}
}
@incollection{rasmussen2003gaussian,
  title={Gaussian processes in machine learning},
  author={Rasmussen, Carl Edward},
  booktitle={Summer school on machine learning},
  pages={63--71},
  year={2003},
  publisher={Springer}
}
@article{burt2020convergence,
  title={Convergence of sparse variational inference in Gaussian processes regression},
  author={Burt, David R and Rasmussen, Carl Edward and Van Der Wilk, Mark},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5120--5182},
  year={2020},
  publisher={JMLRORG}
}
@article{wild2023rigorous,
  title={A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods},
  author={Wild, Veit David and Ghalebikesabi, Sahra and Sejdinovic, Dino and Knoblauch, Jeremias},
  journal={arXiv preprint arXiv:2305.15027},
  year={2023}
}
@inproceedings{wynne2022variational,
  title={Variational gaussian processes: A functional analysis view},
  author={Wynne, George and Wild, Veit},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4955--4971},
  year={2022},
  organization={PMLR}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}
@software{samuel_colvin_2023_8277473,
  author       = {Samuel Colvin and
                  David Montague and
                  Adrian Garcia Badaracco and
                  Hasan Ramezani and
                  Eric Jolibois and
                  Marcelo Trylesinski and
                  Terrence Dorsey and
                  pyup.io bot and
                  Serge Matveenko and
                  Arseny Boykov and
                  Sebastián Ramírez and
                  David Hewitt and
                  Nikita Grishko and
                  Koudai Aono and
                  Yurii Karabas and
                  Vitaly Samigullin and
                  Stephen Brown II and
                  Viicos and
                  Amin Alaee and
                  Davis Kirkendall and
                  layday and
                  Yasser Tahiri and
                  Daniel Smith and
                  Marc Mueller and
                  Nuno André and
                  Hmvp and
                  John Carter and
                  Ofek Lev},
  title        = {pydantic/pydantic: v2.3.0},
  month        = aug,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v2.2.2},
  doi          = {10.5281/zenodo.8277473},
  url          = {https://doi.org/10.5281/zenodo.8277473}
}
@misc{Eustace, title={Poetry}, url={https://github.com/python-poetry/poetry}, author={Eustace, Sébastien}} 

@inproceedings{arthur2007k,
  title={K-means++ the advantages of careful seeding},
  author={Arthur, David and Vassilvitskii, Sergei},
  booktitle={Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
  pages={1027--1035},
  year={2007}
}


@InProceedings{pmlr-v119-hron20a,
  title = 	 {Infinite attention: {NNGP} and {NTK} for deep attention networks},
  author =       {Hron, Jiri and Bahri, Yasaman and Sohl-Dickstein, Jascha and Novak, Roman},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4376--4386},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/hron20a/hron20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/hron20a.html},
  abstract = 	 {There is a growing amount of literature on the relationship between wide neural networks (NNs) and Gaussian processes (GPs), identifying an equivalence between the two for a variety of NN architectures. This equivalence enables, for instance, accurate approximation of the behaviour of wide Bayesian NNs without MCMC or variational approximations, or characterisation of the distribution of randomly initialised wide NNs optimised by gradient descent without ever running an optimiser. We provide a rigorous extension of these results to NNs involving attention layers, showing that unlike single-head attention, which induces non-Gaussian behaviour, multi-head attention architectures behave as GPs as the number of heads tends to infinity. We further discuss the effects of positional encodings and layer normalisation, and propose modifications of the attention mechanism which lead to improved results for both finite and infinitely wide NNs. We evaluate attention kernels empirically, leading to a moderate improvement upon the previous state-of-the-art on CIFAR-10 for GPs without trainable kernels and advanced data preprocessing. Finally, we introduce new features to the Neural Tangents library (Novak et al.,2020) allowing applications of NNGP/NTK models, with and without attention, to variable-length sequences, with an example on the IMDb reviews dataset.}
}

@article{abdar2021review,
  title={A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
  author={Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U Rajendra and others},
  journal={Information Fusion},
  volume={76},
  pages={243--297},
  year={2021},
  publisher={Elsevier}
}

@article{adlam2020cold,
  title={Cold posteriors and aleatoric uncertainty},
  author={Adlam, Ben and Snoek, Jasper and Smith, Samuel L},
  journal={arXiv preprint arXiv:2008.00029},
  year={2020}
}