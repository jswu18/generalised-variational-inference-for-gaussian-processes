\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{paper}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\usepackage{xcolor}
\newcommand{\jk}[1]{{\color{blue} [JK: #1]}}
\newcommand{\jw}[1]{{\color{gray} [JW: #1]}}
\newcommand{\Cat}{\operatorname{Cat}}
\newcommand{\Chol}{\operatorname{Chol}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{GVI in FS for Image Data}{Notes}
\firstpageno{1}

\begin{document}

\title{Notes on GVI in FS for Image Data}

% \author{\name Jian Shu (James) Wu \email jian.wu.22@ucl.ac.uk \\
%        \addr Department of Computer Science\\
%        University College London\\
%       Seattle, WA 98195-4322, USA
%       \AND
%       \name Author Two \email two@cs.berkeley.edu \\
%       \addr Division of Computer Science\\
%       University of California\\
%       Berkeley, CA 94720-1776, USA
% }

% \editor{}

\maketitle

%\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%\blindtext
%\end{abstract}

%\begin{keywords}
%  keyword one, keyword two, keyword three
%\end{keywords}

%\section{Generalized Variational Inference}\label{sec:generalized-variational-inference}
%Notes taken from \cite{knoblauch2022optimization}.

\section{GWI in FS Set Up}\label{sec:gwi-in-fs-set-up}

Notes taken from \cite{wild2022generalized}.

\subsection{Gaussian Measures of GWI in FS}\label{sec:gaussian-measures-of-gwi-in-fs}

The Reference Gaussian Measure:
\[P \coloneqq \mathbb{P}^{F} \sim \mathcal{N} (m_{\mathbb{P}}(\cdot), \mathbf{C}_{k(\cdot, \cdot)})\]

where $m_{\mathbb{P}}(\cdot)$ is the reference mean function (generally zero constant mean function) and $k(\cdot, \cdot)$ is the reference kernel function.

Similarly, the Approximation Gaussian Measure:
\[Q \coloneqq \mathbb{Q}^{F} \sim \mathcal{N} (m_{\mathbb{Q}}(\cdot), \mathbf{C}_{r(\cdot, \cdot)})\]

where $m_{\mathbb{Q}}(\cdot)$ is the approximation mean function, which will be defined with dependence on the reference mean function, and $r(\cdot, \cdot)$ is the approximation kernel function, which will be defined with dependence on the reference kernel function.

\subsection{Motivations}\label{sec:motivations}


Evaluations of Gaussian Measures have computational complexity of order $\mathcal{O}(N^3)$ where $N$ is the number of training points.
Thus, despite the nice uncertainty quantification properties that Gaussian Measures provide, they are unable to scale well in large and more complicated data regimes (i.e training on a large number of images).


\subsection{Modelling Approach}\label{sec:modelling-approach}


We will assume that our $N$ data pairs $(\mathbf{x}_n, y_n)$ are generated from:

\[y_n = F(\mathbf{x}_n) + \epsilon_n\]

where $F \sim \mathcal{N}(0, \mathbf{C})$ is a random function, $\epsilon \sim \mathcal{N}(0, \sigma^2)$ for $n=1, \cdots, N$, and $\mathbf{C} = \mathbf{C}_{k}$ depends on the reference kernel function $k$.
$\sigma >0$ is the observation noise or aleatoric uncertainty (irreducible data uncertainty).

The approach of GVI in FS is to define a reference Gaussian Measure $P$ that will have the usual scaling issues.
$P$ will be trained on a subset of the data of size $\mathcal{O}\left(N^{1/2}\right)$ such that the evaluation of these points will have computational complexity $\mathcal{O}\left(N^{3/2}\right)$.
Then an approximation Gaussian Measure $Q$ is defined with respect to these $\mathcal{O}\left(N^{1/2}\right)$ inducing points such that the evaluation of the full $N$ training points on $Q$ will have computational complexity $\mathcal{O}\left(N^{3/2}\right)$.

GVI in FS involves the process of training $Q$ such that it's behaviour will resemble that of $P$, our reference measure that is computationally intractable.
This will be done in a two step process:
\begin{enumerate}
    \item Train the parameters of the reference measure $P$ using the inducing points. ($\mathcal{O}(N^{3/2})$)
    \item To mimic the behaviour of $P$, train the parameters of $Q$ by minimising of the Wasserstein metric between $P$ and $Q$. ($\mathcal{O}(N^{3/2})$)
\end{enumerate}


\subsection{Reference Kernels}

One choice of a reference kernel $k$ is the ARD kernel:
\[k(\mathbf{x}, \mathbf{x}') = \sigma_f^2 \exp\left( - \frac{1}{2} \sum_{d=1}^D \frac{(x_d - x'_d)^2}{\alpha_d^2}\right)\]

for $\mathbf{x}, \mathbf{x}' \in \mathbb{R}^D$. $\sigma_f^2$ is the kernel scaling factor and $\mathbf{\alpha} = [\alpha_1 \cdots \alpha_D]^T$ where each $\alpha_d > 0$ is the length-scale for dimension $d$.
Any kernel can be chosen as the reference kernel, another example is the NNGP kernel.

\subsubsection{Reference Kernel Hyperparameter Tuning}
For GWI, we choose $M$ inducing points where $M \in \{0.5\sqrt{N}, \sqrt{N},\cdots\}$.
$M$ is some multiple of $\sqrt{N}$, which we will see is for control the computational cost of evaluating the approximation kernel is controlled.
This provides us with inducing points $z_1, \cdots z_M$.

The reference kernel hyper-parameters are then chosen using these inducing points.
One example of hyper-parameter tuning is by maximising the log-likelihood on the inducing points:
\[\log p(\mathbf{y}_{\mathbf{Z}}) = -\frac{1}{2}\log\left( \det (k(\mathbf{X}_{\mathbf{Z}}, \mathbf{X}_{\mathbf{Z}})) + \sigma^2 \mathbf{I}_M\right) - \frac{1}{2} \mathbf{y}_{\mathbf{Z}}^T\left(k(\mathbf{X}_{\mathbf{Z}}, \mathbf{X}_{\mathbf{Z}}) + \sigma^2 \mathbf{I}_M\right)^{-1} \mathbf{y}_{\mathbf{Z}}\]

where the inducing point pairs of the vectors $\mathbf{X}_{\mathbf{Z}}$ and $\mathbf{y}_{\mathbf{Z}}$ are $(\mathbf{x}_m, y_m)$ for $m=1,\cdots M$.
Recall that $\sigma >0$ is the observation noise or aleatoric uncertainty (irreducible data uncertainty).

\jw{If we choose the NNGP kernel, I think they have their own method of finding the optimal hyper-parameters.}

\subsection{Approximation Mean Functions}\label{subsec:approximation-mean-functions}
There are many different ways we can define $m_{\mathbb{Q}}(\cdot)$, all of which should depend on the reference Gaussian Measure.

\subsubsection{SVGP Mean Function}
To recover the stochastic variational Gaussian Process from \cite{titsias2009variational}, we can define the approximation mean function as:
\[m_{\mathbb{Q}}(\mathbf{x}) \coloneqq m_{\mathbb{P}}(\mathbf{x}) + \sum_{m=1}^{M} \beta_m k_m(\mathbf{x})\]

where $m_{\mathbb{P}}(\mathbf{x})$ is the evaluation of the reference mean function and $k_m(\mathbf{x})$ is the evaluation of the reference kernel function at the inducing point $\mathbf{x}_m$.
With this mean function and the SVGP Approximation Kernel, we recover the SVGP from \cite{titsias2009variational}.

\subsubsection{DNN Mean Function}

Another choice of mean function is by incorporating a neural network such that:
\[m_{\mathbb{Q}}(\mathbf{x}) \coloneqq m_{\mathbb{P}}(\mathbf{x}) + g(\mathbf{x})\]

where $m_{\mathbb{P}}(\mathbf{x})$ is the evaluation of the reference mean function and $g(\mathbf{x})$ a neural network.

\subsection{Approximation Kernels}\label{subsec:approximation-kernels}

There are many different ways we can define $r(\cdot, \cdot)$, all of which should depend on the reference Gaussian Measure.

\subsubsection{SVGP Kernel}

One choice of approximation kernel is by defining a variational kernel:
\[r(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}, \mathbf{x}') - k(\mathbf{x}, \mathbf{X}_{\mathbf{Z}})k(\mathbf{X}_{\mathbf{Z}}, \mathbf{X}_{\mathbf{Z}})^{-1}k(\mathbf{X}_{\mathbf{Z}}, \mathbf{x}') + k(\mathbf{x}, \mathbf{X}_{\mathbf{Z}})\mathbf{\Sigma}k(\mathbf{X}_{\mathbf{Z}}, \mathbf{x}')\]

where $k$ is the reference kernel and $\mathbf{\Sigma} \in \mathbb{R}^{m \times m}$ defined with respect to a Cholesky decomposition:
\[\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^T\]

and
\[\mathbf{L} = \Chol\left( \left[k(\mathbf{X}_{\mathbf{Z}}, \mathbf{X}_{\mathbf{Z}}) + \frac{1}{\sigma^2} k(\mathbf{X}_{\mathbf{Z}}, \mathbf{x})k(\mathbf{x}', \mathbf{X}_{\mathbf{Z}})\right]^{-1}\right)\]

where we can also calculate the matrix inverse above with a Cholesky decomposition.

\subsection{The Wasserstein Metric}\label{subsec:wasserstein-metric}

Notes taken from \cite{wild2022generalized}.

%
\jk{Explain why this is a natural estimator/what the exact Wasserstein distance would look like and how this estimate relates to that. Also, you haven't covered the regression case here so it's not clear how this differs/is similar to the regression case.}
\jw{Using Veit's paper I've followed the same reasoning. I've tried elaborating on the last trace term to fill in the steps that Veit skipped for my own understanding.}
%

For two Gaussian Measures $\mathbb{P} = \mathcal{N}(m_{\mathbb{P}}, C_{\mathbb{P}})$ and $\mathbb{Q} = \mathcal{N}(m_{\mathbb{Q}}, C_{\mathbb{Q}})$ on the Hilbert space $H=L^2(\mathcal{X}, \rho, \mathbb{R})$ Wasserstein distance $W_2^2(P_j, Q_j)$ the Wasserstein metric is given as:
\[W_2^2(\mathbb{P}, \mathbb{Q}) = \|m_{\mathbb{P}} - m_{\mathbb{Q}}\|_2^2 + tr(C_{\mathbb{P}}) + tr(C_{\mathbb{Q}}) - 2 \cdot tr \left[ \left( C_{\mathbb{P}}^{\frac{1}{2}}C_{\mathbb{Q}}C_{\mathbb{P}}^{\frac{1}{2}}\right)^{\frac{1}{2}}\right]\]

which can be approximated with $\hat{W}^2$:
\[
\begin{split}
\hat{W}^2 & \coloneqq \frac{1}{N}\sum_{n=1}^N (m_{\mathbb{P}}(x_n)-m_{\mathbb{Q}}(x_n))^2 + \frac{1}{N}\sum_{n=1}^{N}k(x_n, x_n) \\
 &\qquad +\frac{1}{N}\sum_{n=1}^{N}r(x_n, x_n) - \frac{2}{\sqrt {N N_S}}\sum_{s=1}^{N_S}\sqrt {\lambda_s(r(X_S, X)k(X, X_S))} \\
\end{split}
\]

where:
\begin{itemize}
    \item $X_S \coloneqq (x_{S, 1}, \dots, x_{S, N_S})$ with $x_{S, 1}, \dots, x_{S, N_S} \in \mathbb{R}^D$, a set of $N_S$ points sub-sampled from the input data $X$
    \item $r(X_S, X) \coloneqq \left(r(x_{S_s, x_n})\right)_{s, n} \in \mathbb{R}^{N_s \times N}$
    \item $k(X, X_S) \coloneqq \left(k(x_{x_n, S_s})\right)_{n, s} \in \mathbb{R}^{N \times N_s}$
    \item $\lambda_s(\cdot)$ calculates the s-th eigenvalue
\end{itemize}

and $n=1,\dots, N$, $s=1, \dots, N_S$, $k$ is the kernel for $\mathbb{P}$, $r$ is the kernel for $\mathbb{Q}$. \\

The approximation for each term of $W^2$ is shown below.

For $\|m_{\mathbb{P}} - m_{\mathbb{Q}}\|_2^2$:
\[\|m_{\mathbb{P}} - m_{\mathbb{Q}}\|_2^2 = \int \left(m_{\mathbb{P}}(x) - m_{\mathbb{Q}}(x)\right)^2 d\rho(x)\]

Approximating $\rho(x)$ with the empirical data distribution:
\[\|m_{\mathbb{P}} - m_{\mathbb{Q}}\|_2^2 \approx \frac{1}{N}\sum_{n=1}^N (m_{\mathbb{P}}(x_n)-m_{\mathbb{Q}}(x_n))^2\]

we have our approximation for the first term.

For $tr(C_{\mathbb{P}})$ and  $tr(C_{\mathbb{Q}})$:
\[tr(C_{\mathbb{P}}) = \int k(x, x) d\rho(x)\]

Again, approximating $\rho(x)$ with the empirical data distribution, we have our estimate:
\[tr(C_{\mathbb{P}}) \approx \frac{1}{N}\sum_{n=1}^{N}k(x_n, x_n)\]

Similarly:
\[tr(C_{\mathbb{Q}})  \approx \frac{1}{N}\sum_{n=1}^{N}r(x_n, x_n)\]

For $tr \left[ \left( C_{\mathbb{P}}^{\frac{1}{2}}C_{\mathbb{Q}}C_{\mathbb{P}}^{\frac{1}{2}}\right)^{\frac{1}{2}}\right]$:
\begin{align*}
tr \left[ \left( C_{\mathbb{P}}^{\frac{1}{2}}C_{\mathbb{Q}}C_{\mathbb{P}}^{\frac{1}{2}}\right)^{\frac{1}{2}}\right] &= \sum_{n=1}^{\infty} \sqrt {\lambda_n\left( C_{\mathbb{P}}^{\frac{1}{2}}C_{\mathbb{Q}}C_{\mathbb{P}}^{\frac{1}{2}}\right)}\\
&= \sum_{n=1}^{\infty} \sqrt {\lambda_n\left( C_{\mathbb{Q}}C_{\mathbb{P}}\right)}
\end{align*}

because $C_{\mathbb{Q}}C_{\mathbb{P}}$ has the same eigenvalues as $C_{\mathbb{P}}^{\frac{1}{2}}C_{\mathbb{Q}}C_{\mathbb{P}}^{\frac{1}{2}}$.

Knowing that $C_{\mathbb{P}}(x) = \int k(x, x') d\rho(x')$, the operator $C_{\mathbb{Q}}C_{\mathbb{P}}$  is given as:
\begin{align*}
C_{\mathbb{Q}}C_{\mathbb{P}}f(x) &= \int r(x, x')\left(C_{\mathbb{P}} f \right) (x') d\rho(x')\\
&= \int r(x, x')\left( \int k(x', t)f(t) d\rho(t) \right) d\rho(x')\\
&= \int \int r(x, x')  k(x', t)f(t) d\rho(x') d\rho(t)\\
&= \int (r * k)(x, t)  f(t) d\rho(t)\\
\end{align*}

where $(r * k)(x, t) \coloneqq \int r(x, x')  k(x', t) d\rho(x'), \forall x, t \in \mathcal{X}$.

This means $C_{\mathbb{Q}}C_{\mathbb{P}}$ is also an integral operator with (non-symmetric) kernel $r * k$.
We can again approximate $\rho$ with the data samples:
\[\widehat{(r * k)}(x, t) = \frac{1}{N} \sum_{n=1}^N r(x, x_n)  k(x_n, t)\]

Thus, the spectrum of $C_{\mathbb{Q}}C_{\mathbb{P}}$ (set of its eigenvalues), we can calculate the spectrum by choosing a subsample of the data $X_S$ of size $N_S < N$, we can approximate:
\begin{align*}
\lambda\left( C_{\mathbb{Q}}C_{\mathbb{P}}\right) &\approx \lambda\left(\frac{1}{N_S}\widehat{(r * k)}(X_S, X_S)\right)\\
&= \lambda\left(\frac{1}{N_S}\frac{1}{N} r(X_S, X)  k(X, X_S)\right)\\
\end{align*}

We can then use this to approximate:
\begin{align*}
tr \left[ \left( C_{\mathbb{P}}^{\frac{1}{2}}C_{\mathbb{Q}}C_{\mathbb{P}}^{\frac{1}{2}}\right)^{\frac{1}{2}}\right] &= \sum_{n=1}^{\infty} \sqrt {\lambda_n\left( C_{\mathbb{Q}}C_{\mathbb{P}}\right)}\\
&\approx \sum_{s=1}^{N_S} \sqrt {\lambda_s\left( \frac{1}{N_S}\frac{1}{N} r(X_S, X)  k(X, X_S)\right)}\\
&\qquad = \frac{1}{\sqrt{N N_S}} \sum_{s=1}^{N_S} \sqrt {\lambda_s\left(  r(X_S, X)  k(X, X_S)\right)}\\
\end{align*}

Combining, we arrive at the approximation:
\[
\begin{split}
\hat{W}^2 & \coloneqq \frac{1}{N}\sum_{n=1}^N (m_{\mathbb{P}}(x_n)-m_{\mathbb{Q}}(x_n))^2 + \frac{1}{N}\sum_{n=1}^{N}k(x_n, x_n) \\
 &\qquad +\frac{1}{N}\sum_{n=1}^{N}r(x_n, x_n) - \frac{2}{\sqrt {N N_S}}\sum_{s=1}^{N_S}\sqrt {\lambda_s(r(X_S, X)k(X, X_S))} \\
\end{split}\label{eq:w-2-distance-approx}
\]


\section{Gaussian Processes for Classification}\label{sec:gaussian-processes-for-classification}

Notes taken from chapter 4 of \cite{matthews2017scalable}.

For Gaussian process regression (GPR), a class of models is defined:
\[f \sim \mathcal{GP}(0, K(\theta))\]
where $f: X \rightarrow \mathbb{R}$, mapping to the set of real numbers $\mathbb{R}$ and $K$ is the covariance function $K: X \times X \rightarrow \mathbb{R}$ parameterised by $\theta$.

For binary Gaussian process classification (GPC), a mapping is defined:
\[g: \mathbb{R} \rightarrow [0, 1]\]
transforming a value on the real line to the unit interval to represent a probability.
A bernoulli random variable $\mathcal{B}$ can be defined such that:
\[f_c \sim \mathcal{B}(g(f))\]
where $f_c: X \rightarrow \{0, 1\}$, the desired binary classifier.

For multiclass classification of $J$ different classes, models are defined:
\[f^{(j)} \sim \mathcal{GP}(0, K(\theta^{(j)}))\]
where $j=1, \dots, J$, defining $J$ i.i.d. Gaussian processes.
Concatenating $\mathbf{f} = [f_1 \cdots f_J]^T$, the classification operation can be defined:
\[\mathbf{f}_c \sim Cat (\mathcal{S}(\mathbf{f}))\]
where $\mathbf{f}_c: X \rightarrow \{0, \dots, J\}$, the desired multiclass classifier. $\mathcal{S}: \mathbb{R}^J \rightarrow \Delta(J)$, a mapping from a $J$ dimensional real vector to a $J$ dimensional probability simplex. $Cat$ is the categorical distribution (generalisation of Bernoulli distribution for Categorical Data).

There are different possible choices for $\mathcal{S}$.
The multiclass generalisation of the logit likelihood:
\[\mathcal{S}_{softmax}(\mathbf{f})_i = \frac{\exp(f^{(i)})}{\sum_{j=1}^{J} \exp(f^{(j)})}\]

The robust max function:
\[\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_i = \begin{cases}
      1-\epsilon, &  \text{if } i = \arg \max(\mathbf{f}) \\
      \epsilon, & \text{otherwise} \\
   \end{cases}\]
taking class label of the maximum value with probability of $1-\epsilon$ and probability $\epsilon$ of picking one of the other classes uniformly at random, where $\epsilon$ is chosen.
This formulation provides robustness to outliers, as it only considers the ranking of the GPR models for each class.

A benefit of the robust max function is that the variational expectation is analytically tractable with respect to the normal CDF ($q(\mathbf{f}) = \mathcal{N}(\mu, C), \mathbf{f} \in \mathbb{R}^J$) and one dimensional quadrature ($\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_i \in \mathbb{R}$):
\[\int_{\mathbb{R}^J} q(\mathbf{f}) \log(\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_y) d\textbf{f} = \log(1-\epsilon) S + \log\left(\frac{\epsilon}{J-1}\right)(1-S)\]
where $S$ is the probability that the function value corresponding to observed class $y$ is larger than the other function values at that point:
\[S = \mathbb{E}_{\mathbf{f}^{(y)} \sim \mathcal{N}(\mathbf{f}^{(y)} | \mu^{(y)}, C^{(y)})} \left[\prod_{i \neq y} \phi \left(\frac{\textbf{f}^{(y)}-\mu^{(i)}}{\sqrt{C^{(i)}}} \right)\right]\]
where $\phi$ is the standard normal CDF. This one dimensional integral can be evaluated using Gauss-Hermite quadrature.


\section{GWI for Multiclass Classification}\label{sec:gwi-for-multiclass-classification}
Notes taken from A.6 of \cite{wild2022generalized}.

\subsection{Objective Function}\label{subsec:gwi-for-multiclass-classification-objective-function}

The likelihood:

\[p(y|f_1, \dots, f_J) = \prod_{n=1}^N p(y_n | f_1, \dots, f_J)\]

where $p(y_n | f_1, \dots, f_J) \coloneqq \mathcal{S}_{robust}^{(\epsilon)}(f_1(x_n), \dots , f_J{x_N})$ and $y_n \in \{1, \dots, J\}$.
$\mathcal{S}_{robust}^{(\epsilon)}$ is the robust max function as described in \cite{matthews2017scalable}.
%
\jk{Don't define the same object twice with different names. Easy fix if you can't decide on which notation you want to use and perhaps want to change later: introduce a macro for the max function.}
%
%
\jw{Changed to $\mathcal{S}_{robust}$ for now}
%
\cite{wild2022generalized} used $\epsilon = 1 \%$.

The model consists of $J$ independent Gaussian Random Elements such that:
\[f_j \sim P_j = \mathcal{N}(m_{\mathbb{P}, j}, C_{\mathbb{P}, j})\]
with the corresponding variational measures:
\[Q_j = \mathcal{N}(m_{\mathbb{Q}, j}, C_{\mathbb{Q}, j})\]
The objective to minimise:
\[\mathcal{L} = -\mathbb{E}_{\mathbb{Q}}\left[ \log p(y_n | F_1, \dots, F_J)\right] + \sum_{j=1}^{J} W_2^2(P_j, Q_j)\]

\subsubsection{Expected Log-likelihood}\label{subsec:expected-log-likelihood}

The variational (posterior) approximation of the probability of $\{(F_1(x), \dots, F_J(x)) \in A\}$ will be denoted:
\[\mathbb{Q}\left( (F_1(x), \dots, F_J(x)) \in A\right)\]
where $A \subset \mathbb{R}^J$.
We get the expected log-likelihood:
%
\jk{Would be good to explain where this approximation comes from}
\jw{I've copied over the reasoning from Veit's paper but not entirely sure on how each step follows yet.}
%

\begin{align*}
\mathbb{E}_{\mathbb{Q}}\left[ \log p(y | F_1, \dots, F_J)\right] &= \sum_{n=1}^{N}\mathbb{E}_{\mathbb{Q}}\left[ \log p(y_n | F_1, \dots, F_J)\right]\\
&= \sum_{n=1}^{N}\log(1-\epsilon)\mathbb{Q}\left( \argmax_{j=1, \cdots, J} \{F_j(x_n)\} = y_n\right)\\
& \qquad + \log\left(\frac{\epsilon}{J-1}\right)\mathbb{Q}\left( \argmax_{j=1, \cdots, J} \{F_j(x_n)\} \neq y_n\right)\\
&\approx \sum_{n=1}^{N} \log(1-\epsilon)S(x_n, y_n) + \log \left(\frac{\epsilon}{J-1}\right)(1-S(x_n, y_n))
\end{align*}

\[\mathbb{E}_{\mathbb{Q}}\left[ \log p(y_n | F_1, \dots, F_J)\right] \approx \sum_{n=1}^{N} \log(1-\epsilon)S(x_n, y_n) + \log \left(\frac{\epsilon}{J-1}\right)(1-S(x_n, y_n))\]
where:
\[S(x, j) \coloneqq \frac{1}{\sqrt{\pi}} \sum_{i=1}^I w_i \prod_{l\neq j} \phi \left( \frac{\sqrt{2 r_j (x, x)}\xi_i + m_{Q, j}(x) - m_{Q, l}(x)}{\sqrt{r_l (x, x)}}\right)\]
for any $x \in \mathcal{X}$, $j=1, \dots, J$ where $(w_i, \xi_i)_{i=1}^I$ are the weights and roots of the Hermite polynomial of order $I \in \mathbb{N}$., calculated with \verb|scipy.special.roots_hermite|.
$\phi$ is the standard normal cumulative distribution function.

\subsection{Prediction}\label{subsec:gwi-for-multiclass-classification-prediction}

For an unseen point $x^* \in \mathcal{X}$, the probability that it belongs to class $j \in \{1, \dots, J\}$:

\[\mathbb{Q}(Y^* = j) = (1-\epsilon) S(x^*, j) + \frac{\epsilon}{J-1}(1-S(x^*, j))\]

where the predicted label class is the maximiser  of this probability:
%
\jk{Style thing: define a macro for $Cat$ so that i looks like  $\operatorname{Cat}$ in math environments if you're going to use it more often. Otherwise, just write $\operatorname{Cat}$ in-text. Also unclear what $\operatorname{Cat}$ means here; undefined.}
\jw{Added a $\Cat$ operator, thanks for the tip.}
%
\[\Cat(\mathbb{Q}(Y^*)) = \argmax_{j \in \{1, \dots J\}} \mathbb{Q}(Y^* = j)\]

where $\Cat$ is the categorical operator, choosing the class from $1, \cdots J$ with the highest probability given by $\mathbb{Q}$.

\section{Uncertainty Quantification Review}\label{sec:uncertainty-quantification-review}
Notes taken from a review paper by \cite{abdar2021review}.

There are two main types of uncertainty: aleatoric and epistemic.
Epistemic uncertainty is the model uncertainty (i.e. choosing to fit the data with a quadratic function when the data is sinusoidal) and can be formulated as a probability distribution over the model parameters.
Aleatoric uncertainty is the irreducible uncertainty of the data (data uncertainty) and considered an inherent property of the data distribution.
Aleatoric uncertainty can be further divided into homoscedastic and heteroscedastic uncertainties.

\subsection{Monte Carlo Dropout}\label{subsec:monte-carlo-dropout}
Estimate epistemic uncertainty by applying MC dropout with Bernoulli distribution at the output of the neurons of a NN.
Different options for dropout-based methods include Bernoulli/Gaussian dropout of either the nodes of a NN or the weights of a NN.

\subsection{Markov Chain Monte Carlo}\label{subsec:markov-chain-monte-carlo}
This uses MCMC to estimate intractable posterior distributions.
There are issues with the required iterations for sufficient burn-in of the sampler being unknown, an issue with MCMC that extends beyond uncertainty quantification.

\subsection{Variational Inference}
An approximation method learning the posterior distribution over BNN weights.

\subsection{Ensemble Techniques}
NNs generally have competitive accuracy but poor predictive uncertainty quantification, usually generating overconfident predictions.
Calibration and domain shift are two evaluation measures used to evaluate the quality of predictive uncertainty.
Calibration measures the discrepancy between long-run frequencies and subjective forecasts.
Domain shift quantifies the generalisation of predictive uncertainty to a domain shift in the data (i.e. trained on cats and dogs but then asked to make a prediction on a bird).
Quantifies if the model is aware of what it does/doesn't know.

An ensemble of models enhances predictive performance, but it's not immediately obvious why it would generate good uncertainty estimation.
Bayesian model averaging (BMA) holds belief that the true model lies within the hypothesis class of the prior $\mathcal{H}$.
Ensembles combine models to discover more powerful models, so they can be expected to be better when true model does not lie in $\mathcal{H}$.

An evaluation approach for measuring uncertainty estimators in vision problems can be found in \cite{gustafsson2020evaluating}.

Measures of spread or "disagreement" of ensembles such as mutual information can be used to assess uncertainty in predictions due to knowledge uncertainty:

\[\mathcal{MI}\left[y, \theta | \textbf{x}^{*}, \mathcal{D}\right] = H[\mathbb{E}_{p(\theta | \mathcal{D})} [P(y|\textbf{x}^{*}, \theta)]] - \mathbb{E}_{p(\theta | \mathcal{D})}[H[y|\textbf{x}^{*}, \theta]]\]

where:
\begin{itemize}
\item $\mathcal{MI}\left[y, \theta | \textbf{x}^{*}, \mathcal{D}\right]$ is the knowledge uncertainty
\item $H[\mathbb{E}_{p(\theta | \mathcal{D})} [P(y|\textbf{x}^{*}, \theta)]]$ is the total uncertainty
\item $\mathbb{E}_{p(\theta | \mathcal{D})}[H[y|\textbf{x}^{*}, \theta]]$ is the expected data uncertainty (i.e. regions of severe class overlap)
\end{itemize}

Two situations: all models have similar uncertainty distribution are very uncertain for each label (data uncertainty) or the models in the ensemble have very different predictions (model uncertainty).

\subsection{Other Uncertainty Quantification Methods}
Neural Architecture Distribution Search (NADS) finds an appropriate distribution of different architectures that perform significantly well on a specified task.

explored the training dynamics of over-parameterised NNs under natural gradient descent.
They showed that the discrepancy between NNs trained on non-linearised and linearised natural gradient descent is smaller than that of standard gradient descent.
Also, that empirically there was no need to formulate a limit argument about the width of the neural network layers, as the discrepancy was small for over-parameterised NNs.

BNNs have been used as a solution for NN predictions but specifying priors is still an open problem.
Independent normal prior in weight space leads to weak constraints on function posterior, allowing it to generalise in unanticipated ways on OOD data.
Noise contrastive priors (NCPs) used to estimate consistent uncertainty by \cite{hafner2020noise}.

Mixup is a DNN training technique where extra samples are produced during training by convexly integrating random pairs of images and their labels.
\cite{thulasidasan2019mixup} showed that this provided much better model calibration and was less likely to yield overconfident predictions using random noise and OoD data.

Adversarial training can eradicate the vulnerability in a single model by forcing it to learn more robust features, but this approach is rigid and suffers from substantial loss on clean data accuracy.
Ensemble techniques can be induced to have diverse sub-models robust to a transfer adversarial example.

Gaussian processes do not scale well, but a common technique is to have a variational GP using inducing samples.
Deep Gaussian processes represent a multilayer hierarchy of Gaussian processes.

Most weight perturbation-based algorithms suffer from high variance of gradient estimation due to sharing the same perturbations among all samples in a mini-batch.
Flipout by \cite{wen2018flipout} is an approach that samples pseudo-independent weight perturbations for each input to decorrelate the gradients within a minibatch.

DNNs have been successful with complex high-dimensional image data but are not robust to adversarial examples as shown in \cite{szegedy2013intriguing}.
\cite{bradshaw2017adversarial} proposed a hybrid model of GP and DNNs (GPDNNs) to deal with the uncertainty caused by adversarial examples.
Convolutional structures have also been introduced into GPs such as in \cite{van2017convolutional}.


\section{Neural Tangents}\label{sec:neural-tangents}
Notes taken from \cite{novak2019neural}.

The infinite-width limit of a large class of Bayesian neural networks become Gaussian Processes with specific, architecture-dependent, compositional kernel, forming a Neural Network Gaussian Process (NNGP) model.
Kernels can be defined with recurrence relationships for a wide range of non-linearities (activation functions), convolutional layers, residual connections, and pooling.
Neural Tangent Kernels (NTK) relates to the gradient descent training of the infinite-width limit of a Bayesian Neural Network.
Infinite-width kernels that cannot be constructed analytically can be approximated by Monte Carlo sampling.

Neural Tangents provide framework for automatic construction of infinite-width kernels that would otherwise need to be derived for each new architecture by hand.


% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

%\acks{All acknowledgements go at the end of the paper before appendices and references.
%Moreover, you are required to declare funding (financial activities supporting the
%submitted work) and competing interests (related financial activities outside the submitted work).
%More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

%\appendix
%\section*{Appendix A}

\bibliography{notes}

\end{document}