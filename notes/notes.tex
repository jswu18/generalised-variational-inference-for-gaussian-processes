\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{paper}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{GVI in FS for Image Data}{Notes}
\firstpageno{1}

\begin{document}

\title{Notes on GVI in FS for Image Data}

% \author{\name Jian Shu (James) Wu \email jian.wu.22@ucl.ac.uk \\
%        \addr Department of Computer Science\\
%        University College London\\
%       Seattle, WA 98195-4322, USA
%       \AND
%       \name Author Two \email two@cs.berkeley.edu \\
%       \addr Division of Computer Science\\
%       University of California\\
%       Berkeley, CA 94720-1776, USA
% }

% \editor{}

\maketitle

%\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%\blindtext
%\end{abstract}

%\begin{keywords}
%  keyword one, keyword two, keyword three
%\end{keywords}

\section{Gaussian Processes for Classification}\label{sec:gaussian-processes-for-classification}

Notes taken from chapter 4 from \cite{matthews2017scalable}.

For Gaussian process regression (GPR), a class of models is defined:
\[f \sim \mathcal{GP}(0, K(\theta))\]
where $f: X \rightarrow \mathbb{R}$, mapping to the set of real numbers $\mathbb{R}$ and $K$ is the covariance function $K: X \times X \rightarrow \mathbb{R}$ parameterised by $\theta$.

For binary Gaussian process classification (GPC), a mapping is defined:
\[g: \mathbb{R} \rightarrow [0, 1]\]
transforming a value on the real line to the unit interval to represent a probability.
A bernoulli random variable $\mathcal{B}$ can be defined such that:
\[f_c \sim \mathcal{B}(g(f))\]
where $f_c: X \rightarrow \{0, 1\}$, the desired binary classifier.

For multiclass classification of $J$ different classes, models are defined:
\[f^{(j)} \sim \mathcal{GP}(0, K(\theta^{(j)}))\]
where $j=1, \dots, J$, defining $J$ i.i.d. Gaussian processes.
Concatenating $\mathbf{f} = [f_1 \cdots f_J]^T$, the classification operation can be defined:
\[\mathbf{f}_c \sim Cat (\mathcal{S}(\mathbf{f}))\]
where $\mathbf{f}_c: X \rightarrow \{0, \dots, J\}$, the desired multiclass classifier. $\mathcal{S}: \mathbb{R}^J \rightarrow \Delta(J)$, a mapping from a $J$ dimensional real vector to a $J$ dimensional probability simplex. $Cat$ is the categorical distribution (generalisation of Bernoulli distribution for Categorical Data).

There are different possible choices for $\mathcal{S}$.
The multiclass generalisation of the logit likelihood:
\[\mathcal{S}_{softmax}(\mathbf{f})_i = \frac{\exp(f^{(i)})}{\sum_{j=1}^{J} \exp(f^{(j)})}\]

The robust max function:
\[\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_i = \begin{cases}
      1-\epsilon, &  \text{if } i = \arg \max(\mathbf{f}) \\
      \epsilon, & \text{otherwise} \\
   \end{cases}\]
taking class label of the maximum value with probability of $1-\epsilon$ and probability $\epsilon$ of picking one of the other classes uniformly at random, where $\epsilon$ is chosen.
This formulation provides robustness to outliers, as it only considers the ranking of the GPR models for each class.

A benefit of the robust max function is that the variational expectation is analytically tractable with respect to the normal CDF ($q(\mathbf{f}) = \mathcal{N}(\mu, C), \mathbf{f} \in \mathbb{R}^J$) and one dimensional quadrature ($\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_i \in \mathbb{R}$):
\[\int_{\mathbb{R}^J} q(\mathbf{f}) \log(\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_y) d\textbf{f} = \log(1-\epsilon) S + \log\left(\frac{\epsilon}{J-1}\right)(1-S)\]
where $S$ is the probability that the function value corresponding to observed class $y$ is larger than the other function values at that point:
\[S = \mathbb{E}_{\mathbf{f}^{(y)} \sim \mathcal{N}(\mathbf{f}^{(y)} | \mu^{(y)}, C^{(y)})} \left[\prod_{i \neq y} \phi \left(\frac{\textbf{f}^{(y)}-\mu^{(i)}}{\sqrt{C^{(i)}}} \right)\right]\]
where $\phi$ is the standard normal CDF. This one dimensional integral can be evaluated using Gauss-Hermite quadrature.


\section{GWI for Multiclass Classification}\label{sec:gwi-for-multiclass-classification}
Notes taken from A.6 from \cite{wild2022generalized}.

\subsection{Objective Function}\label{subsec:gwi-for-multiclass-classification-objective-function}

The likelihood:

\[p(y|f_1, \dots, f_J) = \prod_{n=1}^N p(y_n | f_1, \dots, f_J)\]

where $p(y_n | f_1, \dots, f_J) \coloneqq h_{y_n}^{\epsilon}(f_1(x_n), \dots , f_J{x_N})$ and $y_n \in \{1, \dots, J\}$.
$h_{y_n}^{\epsilon}$ is the robust max function $\mathcal{S}_{robust}^{(\epsilon)}$ as described in \cite{matthews2017scalable}.
\cite{wild2022generalized} used $\epsilon = 1 \%$.

The model consists of $J$ independent Gaussian Random Elements such that:
\[f_j \sim P_j = \mathcal{N}(m_{\mathbb{P}, j}, C_{\mathbb{P}, j})\]
with the corresponding variational measures:
\[Q_j = \mathcal{N}(m_{\mathbb{Q}, j}, C_{\mathbb{Q}, j})\]
The objective to minimise:
\[\mathcal{L} = -\mathbb{E}_{\mathbb{Q}}\left[ \log p(y_n | F_1, \dots, F_J)\right] + \sum_{j=1}^{J} W_2^2(P_j, Q_j)\]
The variational (posterior) approximation of the probability of $\{(F_1(x), \dots, F_J(x)) \in A\}$ will be denoted:
\[\mathbb{Q}\left( (F_1(x), \dots, F_J(x)) \in A\right)\]
where $A \subset \mathbb{R}^J$.
We get the expected log-likelihood:
\[\mathbb{E}_{\mathbb{Q}}\left[ \log p(y_n | F_1, \dots, F_J)\right] \approx \sum_{n=1}^{N} \log(1-\epsilon)S(x_n, y_n) + \log \left(\frac{\epsilon}{J-1}\right)(1-S(x_n, y_n))\]
where:
\[S(x, j) \coloneqq \frac{1}{\sqrt{\pi}} \sum_{i=1}^I w_i \prod_{l\neq j} \phi \left( \frac{\sqrt{2 r_j (x, x)}\xi_i + m_{Q, j}(x) - m_{Q, l}(x)}{\sqrt{r_l (x, x)}}\right)\]
for any $x \in \mathcal{X}$, $j=1, \dots, J$ where $(w_i, \xi_i)_{i=1}^I$ are the weights and roots of the Hermite polynomial of order $I \in \mathbb{N}$., calculated with \verb|scipy.special.roots_hermite|.
$\phi$ is the standard normal cumulative distribution function.

The Wasserstein distance $W_2^2(P_j, Q_j)$ can be estimated in the same way as for regression:
\[
\begin{split}
\hat{W}^2 \coloneqq & \frac{1}{N}\sum_{n=1}^N (m_{\mathbb{P}}(x_n)-m_{\mathbb{Q}}(x_n))^2 + \frac{1}{N}\sum_{n=1}^{N}k(x_n, x_n) \\
 &+\frac{1}{N}\sum_{n=1}^{N}r(x_n, x_n) - \frac{2}{\sqrt {N N_S}}\sum_{s=1}^{N_S}\sqrt {\lambda_s(r(X_S, X)k(X, X_S))} \\
\end{split}\label{eq:w-2-distance-approx}
\]
where:
\begin{itemize}
    \item $X_S \coloneqq (x_{S, 1}, \dots, x_{S, N_S})$ with $x_{S, 1}, \dots, x_{S, N_S} \in \mathbb{R}^D$, a set of $N_S$ points sub-sampled from the input data $X$
    \item $r(X_S, X) \coloneqq \left(r(x_{S_s, x_n})\right)_{s, n} \in \mathbb{R}^{N_s \times N}$
    \item $k(X, X_S) \coloneqq \left(k(x_{x_n, S_s})\right)_{n, s} \in \mathbb{R}^{N \times N_s}$
    \item $\lambda_s(\cdot)$ calculates the s-th eigenvalue
\end{itemize}
and $n=1,\dots, N$, $s=1, \dots, N_S$, $k$ is the kernel for $\mathbb{P}$, $r$ is the kernel for $\mathbb{Q}$

\subsection{Prediction}\label{subsec:gwi-for-multiclass-classification-prediction}

For an unseen point $x^* \in \mathcal{X}$, the probability that it belongs to class $j \in \{1, \dots, J\}$:

\[\mathbb{Q}(Y^* = j) = (1-\epsilon) S(x^*, j) + \frac{\epsilon}{J-1}(1-S(x^*, j))\]

where the predicted label class is the maximiser  of this probability:

\[Cat(\mathbb{Q}(Y^*)) = \argmax_{j \in \{1, \dots J\}} \mathbb{Q}(Y^* = j)\]


% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

%\acks{All acknowledgements go at the end of the paper before appendices and references.
%Moreover, you are required to declare funding (financial activities supporting the
%submitted work) and competing interests (related financial activities outside the submitted work).
%More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

%\appendix
%\section*{Appendix A}

\bibliography{notes}

\end{document}