\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{paper}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{GVI in FS for Image Data}{Notes}
\firstpageno{1}

\begin{document}

\title{Notes on GVI in FS for Image Data}

% \author{\name Jian Shu (James) Wu \email jian.wu.22@ucl.ac.uk \\
%        \addr Department of Computer Science\\
%        University College London\\
%       Seattle, WA 98195-4322, USA
%       \AND
%       \name Author Two \email two@cs.berkeley.edu \\
%       \addr Division of Computer Science\\
%       University of California\\
%       Berkeley, CA 94720-1776, USA
% }

% \editor{}

\maketitle

%\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%\blindtext
%\end{abstract}

%\begin{keywords}
%  keyword one, keyword two, keyword three
%\end{keywords}

%\section{Generalized Variational Inference}\label{sec:generalized-variational-inference}
%Notes taken from \cite{knoblauch2022optimization}.


\section{Gaussian Processes for Classification}\label{sec:gaussian-processes-for-classification}


Notes taken from chapter 4 of \cite{matthews2017scalable}.

For Gaussian process regression (GPR), a class of models is defined:
\[f \sim \mathcal{GP}(0, K(\theta))\]
where $f: X \rightarrow \mathbb{R}$, mapping to the set of real numbers $\mathbb{R}$ and $K$ is the covariance function $K: X \times X \rightarrow \mathbb{R}$ parameterised by $\theta$.

For binary Gaussian process classification (GPC), a mapping is defined:
\[g: \mathbb{R} \rightarrow [0, 1]\]
transforming a value on the real line to the unit interval to represent a probability.
A bernoulli random variable $\mathcal{B}$ can be defined such that:
\[f_c \sim \mathcal{B}(g(f))\]
where $f_c: X \rightarrow \{0, 1\}$, the desired binary classifier.

For multiclass classification of $J$ different classes, models are defined:
\[f^{(j)} \sim \mathcal{GP}(0, K(\theta^{(j)}))\]
where $j=1, \dots, J$, defining $J$ i.i.d. Gaussian processes.
Concatenating $\mathbf{f} = [f_1 \cdots f_J]^T$, the classification operation can be defined:
\[\mathbf{f}_c \sim Cat (\mathcal{S}(\mathbf{f}))\]
where $\mathbf{f}_c: X \rightarrow \{0, \dots, J\}$, the desired multiclass classifier. $\mathcal{S}: \mathbb{R}^J \rightarrow \Delta(J)$, a mapping from a $J$ dimensional real vector to a $J$ dimensional probability simplex. $Cat$ is the categorical distribution (generalisation of Bernoulli distribution for Categorical Data).

There are different possible choices for $\mathcal{S}$.
The multiclass generalisation of the logit likelihood:
\[\mathcal{S}_{softmax}(\mathbf{f})_i = \frac{\exp(f^{(i)})}{\sum_{j=1}^{J} \exp(f^{(j)})}\]

The robust max function:
\[\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_i = \begin{cases}
      1-\epsilon, &  \text{if } i = \arg \max(\mathbf{f}) \\
      \epsilon, & \text{otherwise} \\
   \end{cases}\]
taking class label of the maximum value with probability of $1-\epsilon$ and probability $\epsilon$ of picking one of the other classes uniformly at random, where $\epsilon$ is chosen.
This formulation provides robustness to outliers, as it only considers the ranking of the GPR models for each class.

A benefit of the robust max function is that the variational expectation is analytically tractable with respect to the normal CDF ($q(\mathbf{f}) = \mathcal{N}(\mu, C), \mathbf{f} \in \mathbb{R}^J$) and one dimensional quadrature ($\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_i \in \mathbb{R}$):
\[\int_{\mathbb{R}^J} q(\mathbf{f}) \log(\mathcal{S}_{robust}^{(\epsilon)}(\mathbf{f})_y) d\textbf{f} = \log(1-\epsilon) S + \log\left(\frac{\epsilon}{J-1}\right)(1-S)\]
where $S$ is the probability that the function value corresponding to observed class $y$ is larger than the other function values at that point:
\[S = \mathbb{E}_{\mathbf{f}^{(y)} \sim \mathcal{N}(\mathbf{f}^{(y)} | \mu^{(y)}, C^{(y)})} \left[\prod_{i \neq y} \phi \left(\frac{\textbf{f}^{(y)}-\mu^{(i)}}{\sqrt{C^{(i)}}} \right)\right]\]
where $\phi$ is the standard normal CDF. This one dimensional integral can be evaluated using Gauss-Hermite quadrature.


\section{GWI for Multiclass Classification}\label{sec:gwi-for-multiclass-classification}
Notes taken from A.6 of \cite{wild2022generalized}.

\subsection{Objective Function}\label{subsec:gwi-for-multiclass-classification-objective-function}

The likelihood:

\[p(y|f_1, \dots, f_J) = \prod_{n=1}^N p(y_n | f_1, \dots, f_J)\]

where $p(y_n | f_1, \dots, f_J) \coloneqq h_{y_n}^{\epsilon}(f_1(x_n), \dots , f_J{x_N})$ and $y_n \in \{1, \dots, J\}$.
$h_{y_n}^{\epsilon}$ is the robust max function $\mathcal{S}_{robust}^{(\epsilon)}$ as described in \cite{matthews2017scalable}.
\cite{wild2022generalized} used $\epsilon = 1 \%$.

The model consists of $J$ independent Gaussian Random Elements such that:
\[f_j \sim P_j = \mathcal{N}(m_{\mathbb{P}, j}, C_{\mathbb{P}, j})\]
with the corresponding variational measures:
\[Q_j = \mathcal{N}(m_{\mathbb{Q}, j}, C_{\mathbb{Q}, j})\]
The objective to minimise:
\[\mathcal{L} = -\mathbb{E}_{\mathbb{Q}}\left[ \log p(y_n | F_1, \dots, F_J)\right] + \sum_{j=1}^{J} W_2^2(P_j, Q_j)\]
The variational (posterior) approximation of the probability of $\{(F_1(x), \dots, F_J(x)) \in A\}$ will be denoted:
\[\mathbb{Q}\left( (F_1(x), \dots, F_J(x)) \in A\right)\]
where $A \subset \mathbb{R}^J$.
We get the expected log-likelihood:
\[\mathbb{E}_{\mathbb{Q}}\left[ \log p(y_n | F_1, \dots, F_J)\right] \approx \sum_{n=1}^{N} \log(1-\epsilon)S(x_n, y_n) + \log \left(\frac{\epsilon}{J-1}\right)(1-S(x_n, y_n))\]
where:
\[S(x, j) \coloneqq \frac{1}{\sqrt{\pi}} \sum_{i=1}^I w_i \prod_{l\neq j} \phi \left( \frac{\sqrt{2 r_j (x, x)}\xi_i + m_{Q, j}(x) - m_{Q, l}(x)}{\sqrt{r_l (x, x)}}\right)\]
for any $x \in \mathcal{X}$, $j=1, \dots, J$ where $(w_i, \xi_i)_{i=1}^I$ are the weights and roots of the Hermite polynomial of order $I \in \mathbb{N}$., calculated with \verb|scipy.special.roots_hermite|.
$\phi$ is the standard normal cumulative distribution function.

The Wasserstein distance $W_2^2(P_j, Q_j)$ can be estimated in the same way as for regression:
\[
\begin{split}
\hat{W}^2 \coloneqq & \frac{1}{N}\sum_{n=1}^N (m_{\mathbb{P}}(x_n)-m_{\mathbb{Q}}(x_n))^2 + \frac{1}{N}\sum_{n=1}^{N}k(x_n, x_n) \\
 &+\frac{1}{N}\sum_{n=1}^{N}r(x_n, x_n) - \frac{2}{\sqrt {N N_S}}\sum_{s=1}^{N_S}\sqrt {\lambda_s(r(X_S, X)k(X, X_S))} \\
\end{split}\label{eq:w-2-distance-approx}
\]
where:
\begin{itemize}
    \item $X_S \coloneqq (x_{S, 1}, \dots, x_{S, N_S})$ with $x_{S, 1}, \dots, x_{S, N_S} \in \mathbb{R}^D$, a set of $N_S$ points sub-sampled from the input data $X$
    \item $r(X_S, X) \coloneqq \left(r(x_{S_s, x_n})\right)_{s, n} \in \mathbb{R}^{N_s \times N}$
    \item $k(X, X_S) \coloneqq \left(k(x_{x_n, S_s})\right)_{n, s} \in \mathbb{R}^{N \times N_s}$
    \item $\lambda_s(\cdot)$ calculates the s-th eigenvalue
\end{itemize}
and $n=1,\dots, N$, $s=1, \dots, N_S$, $k$ is the kernel for $\mathbb{P}$, $r$ is the kernel for $\mathbb{Q}$

\subsection{Prediction}\label{subsec:gwi-for-multiclass-classification-prediction}

For an unseen point $x^* \in \mathcal{X}$, the probability that it belongs to class $j \in \{1, \dots, J\}$:

\[\mathbb{Q}(Y^* = j) = (1-\epsilon) S(x^*, j) + \frac{\epsilon}{J-1}(1-S(x^*, j))\]

where the predicted label class is the maximiser  of this probability:

\[Cat(\mathbb{Q}(Y^*)) = \argmax_{j \in \{1, \dots J\}} \mathbb{Q}(Y^* = j)\]



\section{Uncertainty Quantification Review}\label{sec:uncertainty-quantification-review}
Notes taken from a review paper by \cite{abdar2021review}.

There are two main types of uncertainty: aleatoric and epistemic.
Epistemic uncertainty is the model uncertainty (i.e. choosing to fit the data with a quadratic function when the data is sinusoidal) and can be formulated as a probability distribution over the model parameters.
Aleatoric uncertainty is the irreducible uncertainty of the data (data uncertainty) and considered an inherent property of the data distribution.
Aleatoric uncertainty can be further divided into homoscedastic and heteroscedastic uncertainties.

\subsection{Monte Carlo Dropout}\label{subsec:monte-carlo-dropout}
Estimate epistemic uncertainty by applying MC dropout with Bernoulli distribution at the output of the neurons of a NN.
Different options for dropout-based methods include Bernoulli/Gaussian dropout of either the nodes of a NN or the weights of a NN.

\subsection{Markov Chain Monte Carlo}\label{subsec:markov-chain-monte-carlo}
This uses MCMC to estimate intractable posterior distributions.
There are issues with the required iterations for sufficient burn-in of the sampler being unknown, an issue with MCMC that extends beyond uncertainty quantification.

\subsection{Variational Inference}
An approximation method learning the posterior distribution over BNN weights.

\subsection{Ensemble Techniques}
NNs generally have competitive accuracy but poor predictive uncertainty quantification, usually generating overconfident predictions.
Calibration and domain shift are two evaluation measures used to evaluate the quality of predictive uncertainty.
Calibration measures the discrepancy between long-run frequencies and subjective forecasts.
Domain shift quantifies the generalisation of predictive uncertainty to a domain shift in the data (i.e. trained on cats and dogs but then asked to make a prediction on a bird).
Quantifies if the model is aware of what it does/doesn't know.

An ensemble of models enhances predictive performance, but it's not immediately obvious why it would generate good uncertainty estimation.
Bayesian model averaging (BMA) holds belief that the true model lies within the hypothesis class of the prior $\mathcal{H}$.
Ensembles combine models to discover more powerful models, so they can be expected to be better when true model does not lie in $\mathcal{H}$.

An evaluation approach for measuring uncertainty estimators in vision problems can be found in \cite{gustafsson2020evaluating}.

Measures of spread or "disagreement" of ensembles such as mutual information can be used to assess uncertainty in predictions due to knowledge uncertainty:

\[\mathcal{MI}\left[y, \theta | \textbf{x}^{*}, \mathcal{D}\right] = H[\mathbb{E}_{p(\theta | \mathcal{D})} [P(y|\textbf{x}^{*}, \theta)]] - \mathbb{E}_{p(\theta | \mathcal{D})}[H[y|\textbf{x}^{*}, \theta]]\]

where:
\begin{itemize}
\item $\mathcal{MI}\left[y, \theta | \textbf{x}^{*}, \mathcal{D}\right]$ is the knowledge uncertainty
\item $H[\mathbb{E}_{p(\theta | \mathcal{D})} [P(y|\textbf{x}^{*}, \theta)]]$ is the total uncertainty
\item $\mathbb{E}_{p(\theta | \mathcal{D})}[H[y|\textbf{x}^{*}, \theta]]$ is the expected data uncertainty (i.e. regions of severe class overlap)
\end{itemize}

Two situations: all models have similar uncertainty distribution are very uncertain for each label (data uncertainty) or the models in the ensemble have very different predictions (model uncertainty).

\subsection{Other Uncertainty Quantification Methods}
Neural Architecture Distribution Search (NADS) finds an appropriate distribution of different architectures that perform significantly well on a specified task.

explored the training dynamics of over-parameterised NNs under natural gradient descent.
They showed that the discrepancy between NNs trained on non-linearised and linearised natural gradient descent is smaller than that of standard gradient descent.
Also, that empirically there was no need to formulate a limit argument about the width of the neural network layers, as the discrepancy was small for over-parameterised NNs.

BNNs have been used as a solution for NN predictions but specifying priors is still an open problem.
Independent normal prior in weight space leads to weak constraints on function posterior, allowing it to generalise in unanticipated ways on OOD data.
Noise contrastive priors (NCPs) used to estimate consistent uncertainty by \cite{hafner2020noise}.

Mixup is a DNN training technique where extra samples are produced during training by convexly integrating random pairs of images and their labels.
\cite{thulasidasan2019mixup} showed that this provided much better model calibration and was less likely to yield overconfident predictions using random noise and OoD data.

Adversarial training can eradicate the vulnerability in a single model by forcing it to learn more robust features, but this approach is rigid and suffers from substantial loss on clean data accuracy.
Ensemble techniques can be induced to have diverse sub-models robust to a transfer adversarial example.

Gaussian processes do not scale well, but a common technique is to have a variational GP using inducing samples.
Deep Gaussian processes represent a multilayer hierarchy of Gaussian processes.

Most weight perturbation-based algorithms suffer from high variance of gradient estimation due to sharing the same perturbations among all samples in a mini-batch.
Flipout by \cite{wen2018flipout} is an approach that samples pseudo-independent weight perturbations for each input to decorrelate the gradients within a minibatch.

DNNs have been successful with complex high-dimensional image data but are not robust to adversarial examples as shown in \cite{szegedy2013intriguing}.
\cite{bradshaw2017adversarial} proposed a hybrid model of GP and DNNs (GPDNNs) to deal with the uncertainty caused by adversarial examples.
Convolutional structures have also been introduced into GPs such as in \cite{van2017convolutional}.


\section{Neural Tangents}\label{sec:neural-tangents}
Notes taken from \cite{novak2019neural}.

The infinite-width limit of a large class of Bayesian neural networks become Gaussian Processes with specific, architecture-dependent, compositional kernel, forming a Neural Network Gaussian Process (NNGP) model.
Kernels can be defined with recurrence relationships for a wide range of non-linearities (activation functions), convolutional layers, residual connections, and pooling.
Neural Tangent Kernels (NTK) relates to the gradient descent training of the infinite-width limit of a Bayesian Neural Network.
Infinite-width kernels that cannot be constructed analytically can be approximated by Monte Carlo sampling.

Neural Tangents provide framework for automatic construction of infinite-width kernels that would otherwise need to be derived for each new architecture by hand.


% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

%\acks{All acknowledgements go at the end of the paper before appendices and references.
%Moreover, you are required to declare funding (financial activities supporting the
%submitted work) and competing interests (related financial activities outside the submitted work).
%More information about this disclosure can be found on the JMLR website.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

%\appendix
%\section*{Appendix A}

\bibliography{notes}

\end{document}