\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

\usepackage{paper}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\usepackage{xcolor}
\newcommand{\jk}[1]{{\color{blue} [JK: #1]}}
\newcommand{\jw}[1]{{\color{gray} [JW: #1]}}
\newcommand{\Cat}{\operatorname{Cat}}
\newcommand{\Chol}{\operatorname{Chol}}
\newcommand{\KLD}{\operatorname{KLD}}

\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumitem}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Short headings should be running head and authors last names

\ShortHeadings{Lit Review: Generalising Variational Inference}{Notes}
\firstpageno{1}

\begin{document}

\title{Literature Review: Generalised Variational Inference}
\maketitle
\section{The Bayesian Posterior}\label{sec:bayesian-posterior}
Traditional statistical modelling focuses on understanding and correctly specifying an underlying data generation process. In a Bayesian inference context, this involves updating the beliefs of a model's parameterisation. Consider a model of a data generating process parameterised by $\theta$. Bayesian inference can be viewed as an update rule on $\pi(\theta)$, our prior belief of $\theta$. Given new observations $x_{1:n}$ and a likelihood function $p_n(x_{1:n}|\theta)$, the belief for $\theta$ is updated as:
\[q_B^*(\theta) = \frac{p_n(x_{1:n}|\theta) \pi(\theta)}{\int_{\Theta} p_n(x_{1:n}|\theta) d \pi(\theta)}\]
where $q_B^*(\theta)$ known as the Bayesian posterior. This update rule makes three main assumptions concerning the prior term, the likelihood term, and the normaliser term. In larger-scaled model settings like Bayesian Neural Networks, these assumptions quickly breakdown making it no longer reasonable to view $q_B^*(\theta)$ as a belief update. \\
\newline 
In contrast to traditional statistical modelling, larger-scaled models are typically focused on predictive performance rather than model specification and parameterisation. Interpreting the mechanism behind calculating the Bayesian posterior in the context of optimisation provides a more reasonable depiction of $q_B^*(\theta)$ for larger-scaled models. The Bayesian posterior $q_B^*(\theta)$ can be shown to be the solution of a special case of a general variational inference (GVI) problem:
\[q^*(\theta) = \argmin_{q \in \Pi} \left\{ \mathbb{E}_{q(\theta)}\left[\sum_{i=1}^n \ell(\theta, x_i)\right] + D(q\|\pi)\right\}\]
where $q_B^*(\theta)$ is recovered by choosing $\ell(\theta, \cdot) = -\log p(\cdot | \theta)$, $D(\cdot \| \pi) = \KLD(\cdot \| \pi)$, and $\Pi = \mathcal{P}(\Theta)$. No longer defining $q_B^*(\theta)$ through a belief update, we are no longer burdened to fulfill the assumptions required for the Bayesian inference interpretation of $q_B^*(\theta)$. Moreover, by $\textit{generalising}$ the Bayesian update mechanism to an optimisation problem, we can further understand more general solutions of the form $q^*(\theta)$, placing the special case of $q_B^*(\theta)$ in a wider context.

\subsection{Assumption 1: The Prior}
Bayesian inference assumes a prior $\pi(\theta)$ that is well-specified and informative of the `true' model parameters, $\theta^*$. The prior is interpreted as embodying \textit{all} previous knowledge about the data generating process such as previously observed data. Alternatively, the prior can also be interpreted as representing \textit{pseudo} observations about how we believe the data behaves.
\subsubsection{Prior Mis-specification}
Larger-scaled models are often over-parameterised black box models, such as the weights of a Bayesian neural network. These parameters are essentially uninterpretable and priors are chosen out of convenience (i.e. Gaussians) with little thought given to their true parameterisation. In these settings, it is no longer practical to view $\pi(\theta)$ as a prior belief in the parameters of the data generating model as it is most definitely mis-specified.
\subsubsection{The Prior is a Regulariser}
The Bayesian posterior can be equivalently expressed as the solution to an optimisation problem:
\[q_B^*(\theta) = \argmin_{q \in \mathcal{P}(\Theta)} \left\{\mathbb{E}_{q(\theta)}\left[-\sum_{i=1}^n \log\left(p(x_i|\theta)\right)\right] + \KLD(q\|\pi)\right\}\]
where $\KLD$ is the Kullback-Leiber divergence. In this context, we can see that the prior $\pi$ only exists in the divergence term. $\pi$ defines the regulariser of an empirical risk minimisation optimisation problem which is solved by the Bayesian posterior $q_B^*(\theta)$. The choice of prior controls model complexity and prevents overfitting to the empirical risk. Unlike in the Bayesian interpretation, in this optimisation setup $\pi$ is no longer required to be a well-specified prior. Thus in larger-scaled models, where prior mis-specification is almost guaranteed, it is more appropriate to view the prior as a regulariser rather than a prior belief of true model parameters.

\subsection{Assumption 2: The Likelihood}

Bayesian inference assumes that there exists $\theta^* \in \Theta$, such that $x_i \sim p_n(x_i | \theta^*)$ for some unknown but \textit{
fixed} $\theta^*$. In other words $p_n(x_i | \theta^*)$, the likelihood function that is chosen is \textit{exactly} the true data generating process and is parameterised by $\theta$. In this case, the problem is simply a matter of finding $\theta^*$. 
\subsubsection{Model Mis-specification}
Although model mis-specification occurs in traditional Bayesian inference, techniques such as hypothesis testing, residual analysis, and domain expertise can help guide the construction of a reasonably well-specified setting. However, the intentions behind using larger-scaled models is completely different. It is not to \textit{understand} the data generating process and its parameterisation, but rather to have superior \textit{predictive} power. With over-parameterisation, these black box models are most definitely mis-specified but often provide high prediction accuracy. These parameters are typically chosen through an optimisation process (i.e. gradient descent) and no longer in the spirit of an update on the prior of a model's parameters. Thus, it is no longer reasonable to hold the traditional view that we are trying to understand the data generating process. For larger-scaled models, it is almost \textit{never} the case that $x_i \sim p_n(x_i|\theta)$ for \texit{any} $\theta \in \Theta$.
\subsubsection{The Likelihood is a Loss}
From our reformulation of the Bayesian posterior as an optimisation solution, the likelihood term exists only in the term $\mathbb{E}_{q(\theta)}\left[-\sum_{i=1}^n \log\left(p(x_i|\theta)\right)\right]$. Note that the empirical risk is defined as:
\[\mathcal{E}(\theta) = \mathbb{E}_{q(\theta)}\left[\sum_{i=1}^n \ell\left(x_i, \theta\right)\right]\]
where $\ell$ is some loss function. We can see that defining the negative log-likelihood as the loss, we recover an empirical risk of the model over empirical data. This interprets the likelihood function as a special loss definition for an optimisation problem,  removing the need for a well-specified likelihood. $q_B^*(\theta)$ is the minimiser of a regularised empirical risk with a log-likelihood loss, and is defined with respect to its predictive performance rather than its updated belief of model parameters.

\subsection{Assumption 3: The Normaliser}

Bayesian inference assumes that the normaliser $\int_{\Theta} p_n(x_{1:n}|\theta) d \pi(\theta)$ is a tractable integral or is computationally tractable. Computational tractability assumes access to  adequate computational resources and time to reasonably approximate the integral. This means that in traditional Bayesian inference, the computational complexities of evaluating $q_B^*(\theta)$ can be ignored. 
\subsubsection{Normaliser Intractability}
The use of conjugate priors is the only case when there exists closed form expressions for $\int_{\Theta} p_n(x_{1:n}|\theta) d \pi(\theta)$ and as such, tractable evaluation of $q_B^*(\theta)$. For over-parameterised black-box models, $q_B^*(\theta)$ will need to be approximated either through sampling approximations of the normaliser or variational approximations of $q_B^*(\theta)$. Samplers such as Metropolis Hastings or Markov Chain Monte Carlo only have convergence guarantees in the infinite limit. Acheiving this limit would require access to infinite computational resources and time, clearly impractical. \\
\newline
Approximating $q_B^*(\theta)$ involves solving for $q_A^*(\theta) \in \mathcal{Q}_{A}$, where $\mathcal{Q}_{A}$ is often viewed as distributions of a simpler form. For example mean field approximations define a family of distributions $\mathcal{Q}_{MF} = \left\{\prod_i q_i(\theta_i)\right\}$, a product of independent distributions. Variational inference is motivated to finding a $q_A^*(\theta) \in \mathcal{Q}_{A}$ that \textit{approximates} $q_B^*(\theta)$, through the minimisation of some divergence between the two, $D(q_A^*(\theta)\| q_B^*(\theta))$. However the space of distributions $\mathcal{Q}_{A}$ is usually severely restrictive in its expressiveness and $q_A^*(\theta)$ is almost never a fair depiction of the structure of $q_B^*(\theta)$. $\mathcal{Q}_{A}$ is chosen purely for computational convenience. With larger-scaled models, it is often no longer reasonable to assume that the normaliser of the Bayesian posterior will be tractable or that $q_B^*(\theta)$ can be reasonably approximated in a tractable manner.
\subsubsection{Model Approximations as Optimisation Constraints}
Rather than viewing $q_A^*(\theta)$ as an approximation of $q_B^*(\theta)$, it is more practical to view $q_A^*(\theta)$ as the solution to an optimisation problem, where we are \textit{constrained} to $\mathcal{Q}_{A}$. In other words, we are not attempting to \textit{approximate} $q_B^*(\theta)$ but rather we are finding the \textit{optimal} solution $q_A^*(\theta)$ in the space $\mathcal{Q}_{A}$. With mis-specified priors and likelihood functions, $q_B^*(\theta)$  is no longer a true `Bayesian` posterior anyways, and so there's little meaning behind approximating it. Especially with the reframing in optimisation, we are more concerned with finding the best performing model in our feasible set $\mathcal{Q}_{A}$ rather than the model that most accurately depicts the data generation process.

% \section{Generalised Variational Inference}

% The GVI posterior is:
% \[q^*(\theta) = \argmin_{q \in \Pi} \left\{ \mathbb{E}_{q(\theta)}\left[\sum_{i=1}^n \ell(\theta, x_i)\right] + D(q\|\pi)\right\}\]
% posterior construction of this form are called construction via the \texit{Rule of Three (RoT)}. The standard Bayesian posterior can be recovered by defining:
% \begin{itemize}
%     \item $\ell(\theta, \cdot) = -\log p(\cdot | \theta)$
%     \item $D(\cdot \| \pi) = \KLD(\cdot \| \pi)$
%     \item $\Pi = \mathcal{P}(\Theta)$
% \end{itemize}
% RoT construction allows for modularity in choosing each component. Adjustments to the loss can avoid model misspecification despite using the same parameterisation, $\theta$. Robustness to misspecification can be ensured through changing $D$.


\end{document}