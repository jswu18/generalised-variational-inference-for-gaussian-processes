\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

\usepackage{paper}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\usepackage{xcolor}
\newcommand{\jk}[1]{{\color{blue} [JK: #1]}}
\newcommand{\jw}[1]{{\color{gray} [JW: #1]}}
\newcommand{\Cat}{\operatorname{Cat}}
\newcommand{\Chol}{\operatorname{Chol}}
\newcommand{\KLD}{\operatorname{KLD}}

\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumitem}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage[backend=biber,style=nature]{biblatex}

\addbibresource{references.bib}
\defbibheading{talikarng}{\References}
% Short headings should be running head and authors last names

\ShortHeadings{Literature Review}{Neural Network Gaussian Processes}
\firstpageno{1}

\begin{document}

\title{Literature Review: Neural Network Gaussian Processes}
\maketitle
\section{Preliminaries}
\subsection{Central Limit Theorem (multi-variate)}
The Central Limit Theorem states that for $\left\{ \mathbf{X}_1, \dots \mathbf{X}_N \right\}$ i.i.d. random vectors with $\mathbf{X}_n \in \mathbb{R}^D$, if $\mathbf{X}_N$ has a mean vector $\mathbb{E}\left[\mathbf{X}_n\right] = \boldsymbol{\mu}$ and a covariance matrix $\boldsymbol{\Sigma}$ for all $n=1, \dots, N$ then as $N \rightarrow \infty$:
\begin{align}
    \sqrt{N}\left(\bar{\mathbf{X}}_N-\boldsymbol{\mu}\right) \xrightarrow{d} \mathcal{N}\left(\mathbf{0}, \boldsymbol{\Sigma}\right)
\end{align}
where $\bar{\mathbf{X}}_N = \frac{1}{N}\sum_{n=1}^N \mathbf{X}_n$. In other words, the average across $N$ random variable vectors will approach a multivariate Gaussian distribution as $N$ approaches infinity.
\subsection{The Kolmogorov Extension Theorem \cite{agdeg1804gaussian}}
The Kolmogorov Extension Theorem ensures that for a consistent set of finite-dimensional marginals, there exists an underlying infinite-dimensional object. In other words, there exists a distribution over functions. 
\\\jw{What does `consistent' mean here? How does fit in with $GP \leftrightarrow GM$? The above says for all GPs there exists a distribution over functions, so they're all GMs?} 

\subsection{Linear Envelope Property for Non-Linearities \cite{agdeg1804gaussian}}
A non-linearity $\phi : \mathbb{R} \rightarrow \mathbb{R}$ obeys the linear envelope property if there exists $c, m \geq 0$ such that:
\begin{align}
    \label{linear-envelope}
    \left\vert \phi(u)\right\vert \leq c + m \left\vert u \right\vert
\end{align}
holds for all $u \in \mathbb{R}$. Most commonly used non-linearities in neural network architectures (i.e. ReLU, ELU, SeLU) follow the linear envelope property. For a random variable $u \sim U$ that is not heavy-tailed, linear bounds ensure that the distribution of $\phi(U)$ will not be heavy tailed. In other words, the non-linearity will not \textit{induce} a heavy tailed distribution.
\subsection{Fully Connected Neural Networks}
For an input $\mathbf{x} \in \mathbb{R}^{D^{(in)}}$, a fully connected network can be defined with an initial step:
\begin{align}
    \label{nn-initial-step}
    \mathbf{f}^{(1)}(\mathbf{x}) = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}
\end{align}
where $\mathbf{W}^{(1)} \in \mathbb{R}^{N^{(1)} \times D^{(in)}}$,  $\mathbf{b}^{(1)} \in \mathbb{R}^{}$
For a network with $L$ hidden layers each with width $N^{(\ell)}$ for $\ell = 1, \dots, L$:
\begin{align}
    \label{nn-recursion-non-linearity}
    g_n^{(\ell)}(\mathbf{x}) &= \phi \left( f_n^{(\ell)}(\mathbf{x})\right) \text{, } \forall n=1,\dots,N^{(\ell)}\\
    \label{nn-recursion-affine-transformation}
    \mathbf{f}^{(\ell+1)}(\mathbf{x}) &= \mathbf{W}^{(\ell+1)}\mathbf{g}^{\ell}(\mathbf{x}) + \mathbf{b}^{\ell+1}
\end{align}
\section{$NN \xrightarrow{D} GP$ (Recursive Limit)}
It can be shown that by recursively taking the infinite-width limit of each layer in a neural network, the final output will follow a multi-variate Gaussian distribution. In particular, if the activations of a previous layer are normally distributed with moments:
\begin{align}
    \mathbb{E}\left[f_i^{(\mu-1)}(x)\right] &= 0 \\
    \mathbb{E}\left[f_i^{(\mu-1)}(x) f_i^{(\mu-1)}(x')\right] &= \delta_{i, j} k(x, x')
\end{align}
\\then under the recursive definition and as $H\rightarrow \infty$, the activations of the next layer converge in distribution to a normal distribution with moments
\begin{align}
    \mathbb{E}\left[f_i^{(\mu)}(x)\right] &= 0\\
    \mathbb{E}\left[f_i^{(\mu)}(x) f_i^{(\mu)}(x')\right] &= \delta_{i, j} \left[\hat{C}^{(\mu)}_w \mathbb{E}_{\left(\epsilon_1, \epsilon_2\right) \sim \mathcal{N}(\mathbf{0}, \mathbf{K})}\left[\phi(\epsilon_1)\phi(\epsilon_2)\right]+C_B^{(\mu)}\right]
\end{align}
where $\mathbf{K} \in \mathbb{R}^{2 \times 2}_{\succ 0}$ contains the input covariances. This recursive relationship is not sufficient to show that the joint distribution converges to a Gaussian when simultaneously taking all layers to their infinite-width limit. \cite{agdeg1804gaussian} suggests that the rate at which a Normal distribution is attained in the nodes of each hidden layer affects the distributions in subsequent layers. 
\\\jw{But is this just a rate problem or does this mean in certain conditions, if the rates are too slow, the subsequent layers will never approach Gaussian? Or is it that the rate that the width of a layer approaches infinity just needs to be faster than the infinite-width rate of subsequent layers?}

\subsection{Single-layer Neural Network Proof \cite{lee2018deep}}
We will show that by assuming i.i.d Gaussian distributions on the weights and biases of a neural network, we can show that the output of a single layer network is the sum of $N$ i.i.d random variables, where $N$ is the network width. Thus by the central limit theorem, the network output will be Gaussian distributed as the network width $N \rightarrow \infty$.
\subsubsection{Single-layer Neural Networks}
For an input vector $\mathbf{x}^{(in)} \in \mathbb{R}^{D^{(in)}}$, the output of a neural network with a single hidden layer of width $N$ can be expressed as $\mathbf{z}^{(out)} = f^{NN}\left(\mathbf{x}^{(in)}\right)$ where $\mathbf{z}^{(out)} \in \mathbb{R}^{D^{(out)}}$. We can breakdown $f^{NN}$:
\begin{align}
    \label{single-layer-hidden}
    \mathbf{z}^{(0)} = \mathbf{W}^{(0)} \mathbf{x}^{(in)} + \mathbf{b}^{(0)}\\
    \label{single-layer-hidden-non-linearity}
    x_n^{(1)} = \phi(z_n^{(0)})\text{, } \forall n = 1, \dots, N\\
    \label{single-layer-output}
    \mathbf{z}^{(out)} = \mathbf{W}^{(out)} \mathbf{x}^{(1)} + \mathbf{b}^{(out)}
\end{align}
where $\mathbf{W}^{(out)} \in \mathbb{R}^{D^{(out)} \times N}$, $\mathbf{b}^{(out)} \in \mathbb{R}^{D^{(out)}}$,  $\mathbf{W}^{(0)} \in \mathbb{R}^{N \times D^{(in)}}$, $\mathbf{b}^{(0)} \in \mathbb{R}^{N}$, and $\phi: \mathbb{R} \rightarrow \mathbb{R}$ is some non-linear activation function applied in a point-wise fashion to $\mathbf{z^{(0)}}$. In this formulation, $\mathbf{z}^{(0)} \in \mathbb{R}^N$ is the pre-activation and $\mathbf{x}^{(1)}  \in \mathbb{R}^N$ is the post-activation of the hidden layer.
\subsubsection{The Infinite-Width Limit}
By assuming all elements of $\mathbf{b}^{(0)}$ and $\mathbf{W}^{(0)}$ are drawn i.i.d where $b^{(0)}_n \sim \mathcal{N}(0, \sigma_{b^{0}}^2)$ and $W^{(0)}_{n, d} \sim \mathcal{N}(0, \frac{\sigma_{w^{0}}^2}{D^{(in)}})$ we can see from (\ref{single-layer-hidden}) that:
\begin{align}
    z_{n}^{(0)} \perp z_{n'}^{(0)} \text{, } \forall n \neq n' \text{ and } n, n' = 1, \dots, N
\end{align}
Because $\phi(\cdot)$ is applied point-wise in (\ref{single-layer-hidden-non-linearity}), it is also the case that:
\begin{align}
    x_{n}^{(1)} \perp x_{n'}^{(1)} \text{, } \forall n \neq n' \text{ and } n, n' = 1, \dots, N
\end{align}
If we also assume that all elements of $\mathbf{b}^{(out)}$ and $\mathbf{W}^{(0)}$ are drawn i.i.d where $b^{(out)}_n \sim \mathcal{N}(0, \sigma_b^2)$ and $W^{(out)}_{d, n} \sim \mathcal{N}(0, \frac{\sigma_{w}^2}{N})$ we see that each element $z_d^{(out)}$ in (\ref{single-layer-output}) for $d=1, \dots, D^{(out)}$, is a weighted average of $N$ i.i.d terms, $x_1^{(1)}, \dots, x_N^{(1)}$. The central limit theorem states that in the infinite limit as $N \rightarrow \infty$, $z_d^{(out)}$ will approach a Gaussian distribution. Moreover, for $K$ inputs $\{\textbf{x}^1, \dots, \textbf{x}^K\}$, the corresponding output set $\{z_d^{(1)}, \dots, z_d^{(K)}\}$ will follow a joint multivariate Gaussian distribution, meaning that the random function $f^{NN}(\cdot)$ is exactly a Gaussian process (now a \textit{random} function because weights are random variables).
\subsubsection{The Infinite-Width Kernel}
\subsection{Multi-layer Neural Network Proof \cite{lee2018deep}}
Building on the single layer case, we will show that the elements of the $\ell^{th}$ layer of a multi-layer neural network is the sum of $N_{\ell-1}$ i.i.d random variables from the  $(\ell-1)^{th}$ layer and so in the infinite-width limit as $N_{\ell-1} \rightarrow \infty$, the elements of the $\ell^{th}$ layer will be Gaussian distributed. Thus, by successively taking the infinite-width limit for each $\ell=1,...,L$ hidden layer, it can be shown that the output of a multi-layer neural network is also Gaussian distributed.
\subsubsection{Multi-layer Neural Networks}
For an input vector $\mathbf{x}^{(in)} \in \mathbb{R}^{D^{(in)}}$, the output of a neural network with $L$ hidden layers, each with width $N^{(\ell)}$ for $\ell = 0, \dots L$ can be expressed iteratively:
\begin{align}
    \label{multi-layer-hidden}
    \mathbf{z}^{(\ell)} = \mathbf{W}^{(\ell)} \mathbf{x}^{(\ell)} + \mathbf{b}^{(\ell)}\\
    \label{multi-layer-hidden-non-linearity}
    x_n^{(\ell+1)} = \phi(z_n^{(\ell)})\text{, } \forall n = 1, \dots, N^{(\ell)}
\end{align}
where the output of each hidden layer is $\mathbf{z}^{(\ell)} \in \mathbb{R}^{N^{(\ell)}}$, the output of the neural network is $\mathbf{z}^{(out)} = \mathbf{z}^{(\ell=L)} \in \mathbb{R}^{D^{(out)}}$ with $D^{(out)} = N^{(L)}$, and the input of the neural network is $\mathbf{x}^{(in)} = \mathbf{x}^{(\ell = 0)} \in \mathbb{R}^{D^{(in)}}$ with $D^{(in)} = N^{(0)}$.
\subsubsection{The Infinite-Width Limit}
\subsubsection{The Infinite-Width Kernel}

\begingroup
\let\clearpage\relax
\AtNextBibliography{\small}
\section*{References}
\printbibliography[heading=talikarng, title = {References}]
\endgroup
\end{document}