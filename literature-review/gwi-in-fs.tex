\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

\usepackage{paper}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\usepackage{xcolor}
\newcommand{\jk}[1]{{\color{blue} [JK: #1]}}
\newcommand{\jw}[1]{{\color{gray} [JW: #1]}}
\newcommand{\Cat}{\operatorname{Cat}}
\newcommand{\Chol}{\operatorname{Chol}}
\newcommand{\KLD}{\operatorname{KLD}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\diag}{\operatorname{diag}}

\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumitem}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage[backend=biber,style=nature]{biblatex}

\addbibresource{references.bib}
\defbibheading{talikarng}{\References}
% Short headings should be running head and authors last names

\ShortHeadings{Literature Review}{GWI in FS}
\firstpageno{1}

\begin{document}

\title{Literature Review: Gaussian Wasserstein Inference in Functon Spaces}
\maketitle
\section{Gaussian Processes \cite{wild2022generalized}, \cite{wild2023connections}}
A Gaussian process $F \sim GP(m, k)$, is a random function mapping $F: \mathcal{X} \rightarrow \mathbb{R}$, defined with respect to a mean function $m: \mathcal{X} \rightarrow \mathbb{R}$ and a positive definite kernel $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$. In particular, for N points $\mathbf{X} = \left\{ x_n\right\}_{n=1}^N$ such that $x_n \in \mathcal{X}$, the evaluation $F_X \coloneqq \left[F(x_1) \cdots F(x_n)\right]^T \in \mathbb{R}^N$ will follow a Gaussian distribution:
\begin{align}
    F_X \sim P_{\mathbf{X}}(\mathbf{Y}) = \mathcal{N}(\mathbf{m}_X, \mathbf{K}_{X, X})
    \label{gp-prior}
\end{align}
where $\mathbf{m}_X = \left[ m(x_1) \cdots m(x_N)\right]^T \in \mathbb{R}^N$, $\mathbf{K}_{X, X} \in \mathbb{R}^{N \times N}$ having elements $\left[\mathbf{K}_{X, X}\right]_{n, n'} = k(x_n, x_{n'})$ for $n, n'=1,\dots, N$, and $\mathbf{Y}$ is a random vector in $\mathbb{R}^N$.
\subsection{Gaussian Process Regression \cite{wild2023connections}}
Consider the regression task where we have $N$ observation pairs $\left\{(x_n, y_n)\right\}_{n=1}^N$ with $x_n \in \mathcal{X}$ and $y_n \in \mathbb{R}$. Using the Gaussian Process formulation in ($\ref{gp-prior}$), we can `predict' for $N^*$ points $\mathbf{X}_{N*} = \left\{ x_{N*}\right\}_{n^*=1}^{N^*}$ with Bayes' Rule to calculate the posterior distribution of $\mathbf{Y}_{N*}$:
\begin{align}
    F_{X_{N*}} \vert Y_N \sim P_{\mathbf{X}_{N*}}(\mathbf{Y}_{N*} | \mathbf{Y}_N= \left\{ y_n\right\}_{n=1}^N) = \frac{ P_{\mathbf{X}_N}(\mathbf{Y}_N=\left\{ y_n\right\}_{n=1}^N \vert \mathbf{Y}_{N*})  P_{\mathbf{X}_{N*}}(\mathbf{Y}_{N*})}{ P_{\mathbf{X}_N}(\mathbf{Y}_N= \left\{ y_n\right\}_{n=1}^N)}
    \label{gp-posterior}
\end{align}
% where $\mathbf{X}_N = \left\{ x_n\right\}_{n=1}^N$, $\mathbf{Y}_N = \left\{ y_n\right\}_{n=1}^N$, $\mathbf{X}_{N*} = \left\{ x_{N*}\right\}_{n^*=1}^{N^*}$, and $\mathbf{Y}_{N*} = \left\{ y_{N*}\right\}_{n^*=1}^{N^*}$.
which we will denote $P^{F \vert \mathbf{Y}_N}$. With all terms in (\ref{gp-posterior}) being Gaussian, the posterior has the closed form expression:
\begin{align}
    P^{F \vert \mathbf{Y}_N}_{GP} =  \mathcal{N}(\bar{\mathbf{m}}_{X_{N*}}, \bar{\mathbf{K}}_{X_{N*}, X_{N*}})
\end{align}
where:
\begin{align}
    \label{gp-posterior-mean}
    \bar{\mathbf{m}}_{X_{N*}} = \mathbf{m}_{X_{N*}} + \mathbf{K}_{X_{N*}, X_N} \left( \mathbf{K}_{X_N, X_N} + \sigma^2 \mathbf{I}_N\right)^{-1} \left( \mathbf{Y}_N - \mathbf{m}_{X_N}\right)\\
    \label{gp-posterior-covariance}
    \bar{\mathbf{K}}_{X_{N*}, X_{N*}} = \mathbf{K}_{X_{N*}, X_{N*}} - \mathbf{K}_{X_{Na*}, X_N}\left( \mathbf{K}_{X_N, X_N} + \sigma^2 \mathbf{I}_N\right)^{-1}\mathbf{K}_{X_N, X_{N*}}
\end{align}
\subsection{Sparse Variational Gaussian Processes \cite{wild2023connections}}
A well-known issue with Gaussian Processes regression is their inability to scale. The posterior covariance matrix in ($\ref{gp-posterior-covariance}$) requires a matrix inversion operation with $\mathcal{O}(N^3)$ computational complexity. A common approach to solve this issue is with sparse Variational Gaussian Processes. This begins by assuming that $\{x_n, y_n\}_{n=1}^{N}$ provides redundant information. In other words, choosing a subset of \textit{inducing points} $\{x_m, y_m\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}$ where $M << N$ will capture the majority of the relationships in the data. Thus, by defining a sparse variational Gaussian Process with the subset $\{x_m, y_m\}_{m=1}^{M}$:
\begin{align}
    Q_{GP} = GP(m_Q, k_Q)
    \label{svgp}
\end{align}
we will ensure $\mathcal{O}(M^3)$ complexity.
To ensure that the posterior $P_{GP}^{F|\mathbf{Y}_N}$ and our approximation $Q_{GP}$ exhibit the same behaviour requires an optimisation process on the hyperparameters of $Q_{GP}$. In a Bayesian context, this involves minimisation of the Kullback-Leibler (KL) divergence:
\begin{align}
    \nu^* = \argmin_{\nu \in \Gamma} \KLD\left(Q_{GP}^{\nu} \Big\| P_{GP}^{F \vert \mathbf{Y}_N} \right)
    \label{svgp-minimiser}
\end{align}
where $Q_{GP}^{\nu}$ indicates a Gaussian Process $Q_{GP}$ parametersied by $\nu$. Moreover, the KL divergence is defined:
\begin{align}
    \KLD\left(Q_{GP}^{\nu} \Big\| P_{GP}^{F \vert \mathbf{Y}_N} \right) = \int \log \left( \frac{dQ_{GP}^{\nu}}{d P_{GP}^{F \vert \mathbf{Y}_N}} \right) d Q_{GP}^{\nu}(f)
    \label{svgp-kld-loss}
\end{align}
\\\jw{Why is using the KL a Bayesian approach? I think Veit mentions this in one of his papers.}
\\To ensure the existence of the Radon-Nikodym derivative $\frac{dQ^{\nu}}{d P^{F \vert \mathbf{Y}_N}}$, \cite{matthews2016sparse} and \cite{titsias2009variational} show that (\ref{svgp-kld-loss}) is a valid expression when $Q_{GP}^{\nu}$ is defined in a very specific fashion. In particular, when the mean function for a test point $x^*$ is defined:
\begin{align}
    m_Q(x^*) = m_P(x^*) + \mathbf{\mu}^T\mathbf{K}_{\mathbf{X}_M, x^*}
    \label{svgp-mean}
\end{align}
and the covariance between points $x^*$ and $x^{**}$ is defined:
\begin{align}
        k_Q(x^*, x^{**}) = k_P(x^*, x^{**}) - \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, x^{**}} + \mathbf{K}_{x^*, \mathbf{X}_M} \mathbf{\Sigma}\mathbf{K}_{\mathbf{X}_M, x^{**}}
    \label{svgp-covariance}
\end{align}
where $\mathbf{\mu} \in \mathbb{R}^{M}$ and $\mathbf{\Sigma} \in \mathbb{R}^{M\times M}_{\succ 0}$ are parameters of $Q$ to be chosen. Note that $\mathbf{K}_{\cdot, \cdot}$ are gram matrices of $k_P$.
\\\jw{This was from Veit's gwi-fs paper. In other papers, the mean is $m_Q(x) =  m_P(x^*) + \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{\mu}$ and the covariance is $k_Q(x^*, x^{**}) = k_P(x^*, x^{**}) - \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, x^{**}} + \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{\Sigma}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, x^{**}}$. Is there no difference because $\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}$ is constant? Could this be helpful for initialising $\mathbf{\mu}$ and $\mathbf{\Sigma}$?}
\\The formulation above defines a Gaussian Process with a matrix inversion operation of $\mathcal{O}(M^3)$, providing us a computationally tractable approach. Moreover, we see that our variational Gaussian Process is parameterised by $\nu \in \Gamma$ where $\Gamma$ is the set:
\begin{align}
    \Gamma = \left\{\mathbf{\mu} \in \mathbb{R}^{M}, \mathbf{\Sigma} \in \mathbb{R}^{M\times M}_{\succ 0}, \left\{x_m, y_m\right\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}\right\}
    \label{svgp-parameter-set}
\end{align} 
It is shown in \cite{matthews2016sparse} that:
\begin{align}
    \KLD\left(Q_{GP}^{\nu} \Big\| P_{GP}^{F \vert \mathbf{Y}_N} \right) = \sum_{n=1}^N\log p(y_n) - \mathcal{L}(\nu)
    \label{svgp-kld-loss-equivalence}
\end{align}
where $p(y_n)$ is the marginal likelihood or evidence of observing $yx_n$ under the prior $F_P \sim GP(m_P, k_P)$ and observation model $y_n \sim \mathcal{N}(F_P(x_i), \sigma^2)$ with $\sigma^2$ being the aleoteric uncertainty. $\mathcal{L}(\nu)$ is the variational free energy:
\begin{align}
    \mathcal{L}(\nu) = \mathbb{E}_{F_Q \sim Q_{GP}^{\nu}}\left[\sum_{n=1}^{N}\log p\left(y_n \vert \mu=F_Q(x_n), \sigma^2\right)\right] -\KLD\left(Q^{\nu}_{GP}\Big\| P_{GP}\right) 
    \label{elbo}
\end{align}
also referred to as the evidence lower bound (ELBO). 
\\We see from (\ref{svgp-kld-loss-equivalence}) that $\mathcal{L}(\nu)$ is the only dependence on $\nu$, so we can rewrite our minimisation objective in (\ref{svgp-minimiser}):
\begin{align}
    \nu^* = \argmin_{\nu \in \Gamma} \left\{\mathbb{E}_{F_Q \sim Q^{\nu}}\left[\sum_{n=1}^{N}\log p\left(y_n \vert \mu=F_Q(x_n), \sigma^2\right)\right] -\KLD\left(Q^{\nu}_{GP}\Big\| P_{GP}\right)\right\}
    \label{svgp-minimiser-gvi-kld}
\end{align}
\\With this specific formulation, \cite{titsias2009variational} shows that there exists a closed form optimums $\mu^*$ and $\mathbf{\Sigma}^*$ of the form:
\begin{align}
    \label{svgp-optimal-mean}
    \mu^* = \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} \left( \sigma^2 \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} + \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N} \mathbf{Y}_N\\
    \label{svgp-optimal-covariance}
    \mathbf{\Sigma}^* = \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} +  \sigma^{-2}\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}
\end{align}
Moreover, the lower bound objective in ($\ref{elbo}$) will have the form:
\begin{align}
    \label{svgp-optimal-elbo}
    \mathcal{L}(\nu^*) = &-\frac{N}{2} \log\left(2\pi \right)\\
    &-\frac{1}{2} \log \left(\det\left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)\right)\\
    & -\frac{1}{2} \left(\mathbf{Y}_N\right)^T \left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)^{-1} \mathbf{Y}_N \\
    &  - \frac{1}{2\sigma^2}\tr\left(\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_N} - \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)
\end{align}
where we assume an optimal selection of inducing points $\left\{x_m, y_m\right\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}$.
\\It is important to note that the existence of the objectives in (\ref{svgp-minimiser}) and (\ref{svgp-minimiser-gvi-kld}) \textit{requires} $P_{GP}^{F \vert \mathbf{Y}_N}$ to dominate $Q_{GP}^{\nu}$, which is \texit{only} guaranteed under the specific formulation of $Q$ in ($\ref{svgp-mean}$) and ($\ref{svgp-covariance}$). This severely restricts our choices for $m_Q$ and $k_Q$. 
\section{Gaussian Measures in Hilbert Spaces \cite{wild2022generalized}, \cite{Kukush_2019}}
Gaussian measures are typically defined as a Lebesgue measure on a physical probability space $(\Omega, \mathcal{A}, \mathbb{P})$. However, there does not exist an infinite-dimensional equivalent to the Lebesgue measure. This means that in most cases, we cannot assume the existence of an infinite-dimensional analog of a probability measure that exists in a finite-dimensional space. However, there are special cases where it is possible to define a \textit{push-forward} measure from $(\Omega, \mathcal{A}, \mathbb{P})$ to an infinite-dimensional space. In the following, we define the infinite-dimensional analog of a Gaussian measure  $\mathbb{P}^{F}(A)$, a push-forward measure constructed from an underlying finite-dimensional $(\Omega, \mathcal{A}, \mathbb{P})$. In particular, we will define $\mathbb{P}^{F}(A)$ on a Hilbert space $\mathcal{H}$ with inner product $\langle \cdot, \cdot \rangle_\mathcal{H}$.
\subsection{Gaussian Random Elements}
 Consider a random element $F \in \mathcal{H}$. $F$ is a \textit{Gaussian Random Element} (GRE) if $\forall h \in \mathcal{H}$:
\begin{align}
    \langle F, h \rangle_\mathcal{H} \sim \mathcal{N}(\mu(F, h), \sigma^2(F, h))
\end{align}
In other words, for \textit{any} $h \in \mathcal{H}$, the inner product $\langle F, h \rangle_\mathcal{H}$ follows a Gaussian distribution (possibly with zero variance). The mean of a GRE can be written as an inner product:
\begin{align}
\mu(F, h) = \langle m, h\rangle_{\mathcal{H}}
\end{align}
where $m$ is the expectation of $F(\omega)$ with respect to some probability measure $\mathbb{P}$ on a finite dimensional space $\Omega$:
\begin{align}
    m \coloneqq \int F(\omega) d \mathbb{P}(\omega)
\end{align}
The variance can be written as an inner product:
\begin{align}
\sigma^2(F, h) = \langle C(h), h\rangle_{\mathcal{H}}
\end{align}
where $C: \mathcal{H} \rightarrow \mathcal{H}$ is the covariance operator:
\begin{align}
    C(h) \coloneqq \int \langle F(\omega), h\rangle_{\mathcal{H}} F(\omega)d \mathbb{P}(\omega) - \langle m, h\rangle_{\mathcal{H}} m 
\end{align}
Thus, we can denote:
\begin{align}
    \langle F, h\rangle_{\mathcal{H}} \sim \mathcal{N}\left(  \langle m, h\rangle_{\mathcal{H}},  \langle C(h), h\rangle_{\mathcal{H}}\right)
\end{align}
or more simply:
\begin{align}
    F \sim \mathcal{N}(m, C)
\end{align}
signifying that $F$ is a GRE.
\subsection{Gaussian Measures \cite{wild2022generalized}}
We can define the push-forward measure of $\mathbb{P}$ as $\mathbb{P}^{F}(A) \coloneqq \mathbb{P}(F^{-1}(H))$, through the mapping $F: \Omega \rightarrow \mathcal{H}$, where $F \in \mathcal{H}$ for all Borel-measurable $H \subset \mathcal{H}$. Moreover if $F$ is a GRE, then we can define the corresponding push-forward measure $P \coloneqq \mathbb{P}^{F}$, as a Gaussian measure. This formulation allows us to define a Gaussian distribution $P$ over the infinite-dimensional Hilbert space $\mathcal{H}$. \jw{The ability to define $\mathbb{P}^{F}(A)$ seems to hinge on the existence of GREs in $\mathcal{H}$. Is there a reason why we can't define random elements in a similar fashion for other distributions that aren't Gaussian? Is it because of the closed form expressions that exist for $m$ and $C$ above?}
\section{Gaussian Wasserstein Inference in Function Spaces}
\subsection{Gaussian Measures $\rightarrow$ Gaussian Processes \cite{wild2022generalized}}
A Gaussian process $F \sim GP(m, k)$ can have many Gaussian measure formulations. To define a corresponding Gaussian \textit{measure} for a GP requires specification of an appropriate \textit{measureable space}. As shown in \cite{wild2022generalized}, a Gaussian process $P_{GP}$ can be specified as a Gaussian measure $P$ on a Hilbert space of square-integrable functions $L^2(\mathcal{X}, \rho, \mathbb{R})$ if the mean $m \in L^2(\mathcal{X}, \rho, \mathbb{R})$ and the kernel $k$ satisfies:
\begin{align}
    \int_{\mathcal{X}} k(x, x) d\rho(x) < \infty
    \label{trace-kernel-condition}
\end{align}
These conditions guarantee sample functions from $F$ to have square-integrable paths, allowing for a corresponding Gaussian measure $P \coloneqq \mathbb{P}^F \sim \mathcal{N}(m, C)$ defined on the \textit{measureable space} $L^2(\mathcal{X}, \rho, \mathbb{R})$ with the same mean $m$ and a covariance operator $C$:
\begin{align}
    C(f(\cdot)) \coloneqq \int k(\cdot, x')f(x')d \rho(x')
    \label{gm-covariance-operator}
\end{align}
for any function $f \in L^2(\mathcal{X}, \rho, \mathbb{R})$. \\
\newline 
Although Gaussian processes and Gaussian measures are distinct concepts, (\ref{trace-kernel-condition}) is satisfied for most GPs, ensuring the existence of corresponding Gaussian measures. As such, Gaussian measures and Gaussian processes can often be used interchangeably (i.e. $P \Leftrightarrow P_{GP}$). %Working with statistical divergences in the following sections, it will be more appropriate to use Gaussian measures instead of Gaussian processes.
\\\jw{Is it trivial to show that the NNGP kernel will satisfy (\ref{trace-kernel-condition})?}

\subsection{Gaussian Wasserstein Inference}
We can reformulate (\ref{svgp-minimiser-gvi-kld}) as a GVI problem of the form:
\begin{align}
    \label{svgp-gwi-gp}
    \nu^* = \argmin_{\nu \in \Gamma} \left\{ \mathbb{E}_{Q_{GP}^{\nu}}\left[\sum_{n=1}^N \ell(\nu, x_n)\right] + D(Q_{GP}^{\nu}, P_{GP})\right\}
\end{align}
where $\ell$ is the negative log-likelihood and $D$ is the KL Divergence, $\KLD(Q_{GP}^{\nu}, P_{GP})$. Having drawn the connection between GPs and GMs, we can also write:
\begin{align}
    \label{svgp-gwi-gm}
    \nu^* = \argmin_{\nu \in \Gamma} \left\{ \mathbb{E}_{Q^{\nu}}\left[\sum_{n=1}^N \ell(\nu, x_n)\right] + D(Q^{\nu}, P)\right\}
\end{align}
where $Q^{\nu}$ and $P$ are push-forward Gaussian measures with corresponding Gaussian process formulations. Moreover, the GM $Q^{\nu}$ maintains a dependence on $\nu$ through the covariance operator in (\ref{gm-covariance-operator}) defined with respect to the kernel of $Q_{GP}^{\nu}$. By reformulating this as a GVI problem between Gaussian measures, \cite{wild2022generalized} and \cite{knoblauch2022optimization} argue that we are now free to choose any valid discrepancy $D$ to replace the KL-divergence. By doing this, we are no longer restricted to ($\ref{svgp-mean}$) and ($\ref{svgp-covariance}$) for $Q$, which was necessary to ensure a valid $\KLD(Q_{GP},  P_{GP})$. By viewing GPs as GMs, we are able to propose distribution discrepancies to replace the KL-divergence. This is exactly how \cite{wild2022generalized} proposes using the Wasserstein distance between Gaussian Measures in function space instead of the KL-divergence. For two Gaussian Measures $P \sim \mathcal{N}(m_P, C_P)$ and $Q \sim \mathcal{N}(m_Q, C_Q)$, the Wasserstein distance between $P$ and $Q$ on (seperable) Hilbert spaces has a closed form expression:
\begin{align}
    \label{wasserstein-distance}
    W_2^2(P, Q) = \| m_P - m_Q\|_2^2 + \tr(C_P) + \tr(C_Q) - 2 \cdot \tr \left[ \left( \left(C_P\right)^{1/2} C_Q \left(C_P\right)^{1/2}\right)^{1/2}\right]
\end{align}
where $\tr$ is the trace of an operator and $\left(C_P\right)^{1/2}$ is the square root of the positive, self-adjoint operator $C_P$. 
\\\jw{mis-match of support}
\\The Wasserstein distance is an integral probability metric (IPM). Unlike f-divergences which compares two measures as a \textit{ratio} (requiring $P$ to dominate $Q$), IPMs compare two measures as a \textit{difference}. Thus $W_2(P, Q)$ is a much more stable discrepancy to use than $\KLD(P, Q)$ in our infinite dimensional setting. This motivates the use the Gaussian Wasserstein Inference objective from \cite{wild2022generalized}:
\begin{align}
    \label{gwi-objective}
    \mathcal{L} = -\sum_{n=1}^N \mathbb{E}_{Q}\left[\log \mathcal{N}(y_n) \vert F(x_n), \sigma^2\right] + W_2 (P, Q)
\end{align}
Crucially, by replacing the KL-divergence in (\ref{svgp-minimiser-gvi-kld}), we are no longer constrained to the approximate GP formulation of $Q$ in  ($\ref{svgp-mean}$) and ($\ref{svgp-covariance}$). With the Wasserstein distance, we are free to choose any mean and covariance kernel. Because of this, \cite{wild2022generalized} proposes GWI-net, a convolutional neural network mean function and squared-exponential kernel for image classification tasks showing that this approach has promising potential.

\section{Numerical Stability \cite{burt2020convergence}}
In addition to the scaling issues of Gaussian Process, stable matrix inversion is also a practical consideration that must be taken into account. In particular in the sparse Gaussian Process setting,  ($\ref{svgp-covariance}$)  inverts a matrix containing the inducing points  $\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}$. A matrix having its smallest and largest eigenvalues many orders of magnitude apart can lead to an ill-conditioned inverse. This means that for $\mathbf{y} = \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{x}$, small perturbations in $\mathbf{x}$ will result in large changes in $\mathbf{y}$. In the context of sparse GPs, this means that small differences in function values will result in large changes in the probability density. 
\subsection{Jitter for Matrix Inversion}
One approach is to apply a spectral shift to 
$\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}$ to increase its smallest eigenvalue. This can be done by adding a `jitter' term $\epsilon$ such that the matrix inversion becomes $\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}+\epsilon\mathbf{I}_M\right)^{-1}$, raising the lowest eigenvalue $\lambda_1$ to $\lambda_1 + \epsilon$. \cite{burt2020convergence} shows that the evidence lower bound in ($\ref{svgp-optimal-elbo}$) is monotonically decreasing in $\epsilon$ where:
\begin{align}
    \label{svgp-optimal-elbo-jitter}
    \mathcal{L}(\nu^*) \geq \mathcal{L}_{\epsilon}(\nu^*) = & -\frac{N}{2} \log\left(2\pi \right) \\
    & -\frac{1}{2} \log \left(\det\left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} +\epsilon\mathbf{I}_M\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)\right)\\
    & -\frac{1}{2} \left(\mathbf{Y}_N\right)^T \left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}+\epsilon\mathbf{I}_M\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)^{-1} \mathbf{Y}_N \\
    & - \frac{1}{2\sigma^2}\tr\left(\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_N} - \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}+\epsilon\mathbf{I}_M\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)
\end{align}
showing that adding jitter introduces a gap between the small gap on the loss objective, though in practice this impact is typically negligable \cite{burt2020convergence}.
\subsection{Inducing Points Selection}
Careful selection of $\mathbf{X}_M$ can also ensure numerical stability of the matrix inversion. Additionally, finding the optimal inducing points $\left\{x_m, y_m\right\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}$ that optimise the loss objective $\mathcal{L}(\nu)$ in ($\ref{elbo}$) can be computationally expensive. \cite{burt2020convergence} proposes a \textit{greedy variance selection} method which iteratively selects the next inducing point based on the highest marginal variance in the prior conditioned on the current set of inducing points:
\begin{align}
    \label{greedy-varaince-selection}
    \text{index}_{m+1} = \argmax \left(\diag \left(\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_N} - \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_{m}} \left(\mathbf{K}_{\mathbf{X}_{m}, \mathbf{X}_{m}} \right) \mathbf{K}_{\mathbf{X}_{m}, \mathbf{X}_N}\right)\right)
\end{align}
where $m < M$, $\mathbf{X}_{m}$ are the $m$ inducing points already chosen, and $\text{index}_{m+1}$ is the index of the next inducing point.
\\\jw{It doesn't seem like \cite{burt2020convergence} explicitly mentions how this will help with matrix inversion. Need to more carefully read this though.}

\begingroup
\let\clearpage\relax
\AtNextBibliography{\small}
\section*{References}
\printbibliography[heading=talikarng, title = {References}]
\endgroup
\end{document}