\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

\usepackage{paper}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\usepackage{xcolor}
\newcommand{\jk}[1]{{\color{blue} [JK: #1]}}
\newcommand{\jw}[1]{{\color{gray} [JW: #1]}}
\newcommand{\Cat}{\operatorname{Cat}}
\newcommand{\Chol}{\operatorname{Chol}}
\newcommand{\KLD}{\operatorname{KLD}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\diag}{\operatorname{diag}}

\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumitem}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage[backend=biber,style=nature]{biblatex}

\addbibresource{references.bib}
\defbibheading{talikarng}{\References}
% Short headings should be running head and authors last names

\ShortHeadings{Literature Review}{GWI in FS}
\firstpageno{1}

\begin{document}

\title{Literature Review: Gaussian Wasserstein Inference in Functon Spaces}
\maketitle
\section{Gaussian Processes \cite{wild2022generalized}, \cite{wild2023connections}}
A Gaussian process $F \sim GP(m_P, k_P)$, is a random function mapping $F: \mathcal{X} \rightarrow \mathbb{R}$, defined with respect to a mean function $m_P: \mathcal{X} \rightarrow \mathbb{R}$ and a positive definite kernel $k_P: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$:
\begin{align}
    F(\cdot) \sim \mathcal{N}(m_P(\cdot), k_P(\cdot, \cdot))
    \label{gp}
\end{align}
In particular, for N points $\mathbf{X}_N = \left\{ x_n\right\}_{n=1}^N$ such that $x_n \in \mathcal{X}$, a Gaussian Process produces a random vector $\mathbf{Y}_N \in \mathbb{R}^{N}$:
\begin{align}
    \label{gp-vector}
    \mathbf{Y}_N = F(\mathbf{X}_N) \sim  \mathcal{N}(\mathbf{m}_{P(\mathbf{X}_N)}, \mathbf{K}_{P(\mathbf{X}_N, \mathbf{X}_N)}) = P\left(\mathbf{Y}_N \Big\vert \left\{ x_n\right\}_{n=1}^N\right)
\end{align}
%where the evaluation $F_X \coloneqq \left[F(x_1) \cdots F(x_n)\right]^T \in \mathbb{R}^N$ will follow the Gaussian distribution
where $\mathbf{m}_{P(\mathbf{X}_N)} = \left[ m_P(x_1) \cdots m_P(x_N)\right]^T \in \mathbb{R}^N$ and $\mathbf{K}_{P(\mathbf{X}_N, \mathbf{X}_N)} \in \mathbb{R}^{N \times N}$ having elements:
\begin{align}
    \left[\mathbf{K}_{P,(\mathbf{X}_N, \mathbf{X}_N)}\right]_{n, n'} = k_P(x_n, x_{n'})
\end{align}
for $n, n'=1,\dots, N$, and $\left\{ x_n\right\}_{n=1}^N$ will be a shorthand for denoting $\mathbb{X}_N = \left\{ x_n\right\}_{n=1}^N$.
\subsection{Gaussian Process Regression \cite{wild2023connections}}
Consider the regression task where we have $N$ observation pairs $\left\{(x_n, y_n)\right\}_{n=1}^N$ with $x_n \in \mathcal{X}$ and $y_n \in \mathbb{R}$. Using the Gaussian Process formulation in ($\ref{gp-vector}$), we can `predict' for $N^*$ test points $\mathbf{X}_{N*} = \left\{ x_{N*}\right\}_{n^*=1}^{N^*}$ with Bayes' Rule by computing the posterior distribution on $\mathbf{X}_{N*}$ given the observations.
\\To do this, we see that the priori is:
\begin{align}
    \label{gp-prior}
    P\left(\mathbf{Y}_{N*} \big\vert \left\{ x_{n*}\right\}_{n*=1}^{N*}\right)
\end{align}
and the likelihood is the evaluation:
\begin{align}
     \label{gp-likelihood}
    P\left(\left\{ y_n\right\}_{n=1}^N \big\vert \mathbf{Y}_{N*}, \left\{ x_{n*}\right\}_{n*=1}^{N*}, \left\{ x_n\right\}_{n=1}^N \right) =P\left(\left\{ y_n\right\}_{n=1}^N \big\vert  \left\{ x_n\right\}_{n=1}^N\right)
\end{align}
where the equality in (\ref{gp-likelihood}) follows because the probabilities of $\mathbf{Y}_N$ only depend on $\mathbf{X}_N$ by the definition of a Gaussian Process in (\ref{gp-vector}).
\\Bayes' Rule gives the posterior distribution on $\mathbf{Y}_{N*}$:
\begin{multline}
     P\left(\mathbf{Y}_{N*} | \left\{ y_n\right\}_{n=1}^N,  \left\{ x_n\right\}_{n=1}^N,  \left\{ x_{n*}\right\}_{n*=1}^{N*}\right) 
     \\\propto P\left(\left\{ y_n\right\}_{n=1}^N \big\vert \left\{ x_n\right\}_{n=1}^N\right) P\left(\mathbf{Y}_{N*} \big\vert  \left\{ x_{n*}\right\}_{n*=1}^{N*}\right)
    \label{gp-posterior}
\end{multline}
With all terms in (\ref{gp-posterior}) being Gaussian, the posterior has the closed form expression:
\begin{align}
    P\left(\mathbf{Y}_{N*} | \left\{ y_n\right\}_{n=1}^N,  \left\{ x_n\right\}_{n=1}^N,  \left\{ x_{n*}\right\}_{n*=1}^{N*}\right)  =  \mathcal{N}(\bar{\mathbf{m}}_{P(X_{N*})}, \bar{\mathbf{K}}_{P(X_{N*}, X_{N*})})
\end{align}
with:
\begin{align}
    \label{gp-posterior-mean}
    \bar{\mathbf{m}}_{P(X_{N*})} = \mathbf{m}_{P(X_{N*})} + \mathbf{K}_{P(X_{N*}, X_N)} \left( \mathbf{K}_{P(X_N, X_N)} + \sigma^2 \mathbf{I}_N\right)^{-1} \left( \mathbf{Y}_N - \mathbf{m}_{P(X_N)}\right)\\
    \label{gp-posterior-covariance}
    \bar{\mathbf{K}}_{P(X_{N*}, X_{N*})} = \mathbf{K}_{P(X_{N*}, X_{N*})} - \mathbf{K}_{P(X_{N*}, X_N)}\left( \mathbf{K}_{P(X_N, X_N)} + \sigma^2 \mathbf{I}_N\right)^{-1}\mathbf{K}_{P(X_N, X_{N*})}
\end{align}
where $\mathbf{I}_N \in \mathbb{R}^{N \times N}$ is the identity matrix.
\subsection{Sparse Variational Gaussian Processes \cite{wild2023connections}}
A well-known issue with Gaussian Processes regression is their inability to scale with $N$, the number of observation points. The posterior covariance matrix in ($\ref{gp-posterior-covariance}$) requires a matrix inversion operation of $\mathcal{O}(N^3)$ computational complexity. A common approach to solve this issue is with sparse Variational Gaussian Processes (svGPs). This begins by assuming that $\{x_n, y_n\}_{n=1}^{N}$ provides redundant information. In particular, choosing a subset of \textit{inducing points} $\{x_m, y_m\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}$ where $M << N$ adequately will capture the relationships in the data. Thus, this approach defines an svGP $G \sim GP(m_Q^\nu, k_Q^\nu)$. We wish the prior of our svGP to approximate the posterior distribution: \begin{align}
    P\left(\mathbf{Y}_{N*} \Big\vert \left\{ y_n\right\}_{n=1}^N,  \left\{ x_n\right\}_{n=1}^N,  \left\{ x_{n*}\right\}_{n*=1}^{N*}\right) \approx Q^\nu\left(\mathbf{Y}_{N*} \Big\vert \left\{ x_{n*}\right\}_{n*=1}^{N*}\right)
\end{align}
 parameterised by $\nu$, which includes the subset $\{x_m, y_m\}_{m=1}^{M}$ and has $\mathcal{O}(M^3)$ complexity. For the posterior and approximation to exhibit the same behaviour requires an optimisation process on the hyperparameters $\nu$. In a Bayesian context (see section \ref{svgp-kld-bayesian} for more details), this involves minimisation of the Kullback-Leibler (KL) divergence:
\begin{align}
    \nu^* = \argmin_{\nu \in \Gamma} \KLD\left(Q^\nu\left(\mathbf{Y}_{N*} \Big\vert \left\{ x_{n*}\right\}_{n*=1}^{N*}\right) \Big\| P\left(\mathbf{Y}_{N*} \Big\vert \left\{ y_n\right\}_{n=1}^N,  \left\{ x_n\right\}_{n=1}^N,  \left\{ x_{n*}\right\}_{n*=1}^{N*}\right) \right)
    \label{svgp-minimiser}
\end{align}
where $\Gamma$ is the parameterisation set for the svGP and the KL divergence is defined as:
\begin{align}
    \KLD\left(Q^{\nu} \Big\| P^{F \vert \mathbf{Y}_N} \right) = \int \log \left( \frac{dQ^{\nu}}{d P^{F \vert \mathbf{Y}_N}} \right) d Q^{\nu}(f)
    \label{svgp-kld-loss}
\end{align}
where $P^{F \vert \mathbf{Y}_N}$ denotes the posterior distribution of $GP(m_P, k_P)$. 
\jw{Kolomogorov existence theorem?}
\\To ensure the existence of the Radon-Nikodym derivative $\frac{dQ^{\nu}}{d P^{F \vert \mathbf{Y}_N}}$, \cite{matthews2016sparse} and \cite{titsias2009variational} show that (\ref{svgp-kld-loss}) is a valid expression when $Q_{GP}^{\nu}$ is defined in a very specific fashion. In particular, when the mean function for a test point $x^*$ is defined:
\begin{align}
    m_Q(x^*) = m_P(x^*) + \mathbf{\mu}^T\mathbf{K}_{\mathbf{X}_M, x^*}
    \label{svgp-mean}
\end{align}
and the covariance between points $x^*$ and $x^{**}$ is defined:
\begin{align}
        k_Q(x^*, x^{**}) = k_P(x^*, x^{**}) - \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, x^{**}} + \mathbf{K}_{x^*, \mathbf{X}_M} \mathbf{\Sigma}\mathbf{K}_{\mathbf{X}_M, x^{**}}
    \label{svgp-covariance}
\end{align}
where $\mathbf{\mu} \in \mathbb{R}^{M}$ and $\mathbf{\Sigma} \in \mathbb{R}^{M\times M}_{\succ 0}$ are parameters of $Q$ to be chosen. Note that $\mathbf{K}_{(\cdot, \cdot)}$ are gram matrices of $k_P$.
\\\jw{This was from Veit's gwi-fs paper. In other papers, the mean is $m_Q(x) =  m_P(x^*) + \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{\mu}$ and the covariance is $k_Q(x^*, x^{**}) = k_P(x^*, x^{**}) - \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, x^{**}} + \mathbf{K}_{x^*, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{\Sigma}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, x^{**}}$. Is there no difference because $\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}$ is constant? Could this be helpful for initialising $\mathbf{\mu}$ and $\mathbf{\Sigma}$?}
\\The formulation above defines a Gaussian Process with a matrix inversion operation of $\mathcal{O}(M^3)$, providing us a computationally tractable approach. Moreover, the parameterisation set $\Gamma$ is defined:
\begin{align}
    \Gamma = \left\{\mathbf{\mu} \in \mathbb{R}^{M}, \mathbf{\Sigma} \in \mathbb{R}^{M\times M}_{\succ 0}, \left\{x_m, y_m\right\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}\right\}
    \label{svgp-parameter-set}
\end{align} 
It is shown in \cite{matthews2016sparse} that:
\begin{align}
    \KLD\left(Q_{GP}^{\nu} \Big\| P_{GP}^{F \vert \mathbf{Y}_N} \right) = \sum_{n=1}^N\log p(y_n) - \mathcal{L}(\nu)
    \label{svgp-kld-loss-equivalence}
\end{align}
where $p(y_n)$ is the marginal likelihood or evidence of observing $yx_n$ under the prior $F_P \sim GP(m_P, k_P)$ and observation model $y_n \sim \mathcal{N}(F_P(x_i), \sigma^2)$ with $\sigma^2$ being the aleoteric uncertainty. $\mathcal{L}(\nu)$ is the variational free energy:
\begin{align}
    \mathcal{L}(\nu) = \mathbb{E}_{F_Q \sim Q_{GP}^{\nu}}\left[\sum_{n=1}^{N}\log p\left(y_n \vert \mu=F_Q(x_n), \sigma^2\right)\right] -\KLD\left(Q^{\nu}_{GP}\Big\| P_{GP}\right) 
    \label{elbo}
\end{align}
also referred to as the evidence lower bound (ELBO). 
\\We see from (\ref{svgp-kld-loss-equivalence}) that $\mathcal{L}(\nu)$ is the only dependence on $\nu$, so we can rewrite our minimisation objective in (\ref{svgp-minimiser}):
\begin{align}
    \nu^* = \argmin_{\nu \in \Gamma} \left\{\mathbb{E}_{F_Q \sim Q^{\nu}}\left[\sum_{n=1}^{N}\log p\left(y_n \vert \mu=F_Q(x_n), \sigma^2\right)\right] -\KLD\left(Q^{\nu}_{GP}\Big\| P_{GP}\right)\right\}
    \label{svgp-minimiser-gvi-kld}
\end{align}
\\With this specific formulation, \cite{titsias2009variational} shows that there exists a closed form optimums $\mu^*$ and $\mathbf{\Sigma}^*$ of the form:
\begin{align}
    \label{svgp-optimal-mean}
    \mu^* = \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} \left( \sigma^2 \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} + \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N} \mathbf{Y}_N\\
    \label{svgp-optimal-covariance}
    \mathbf{\Sigma}^* = \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} +  \sigma^{-2}\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\right)^{-1}\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}
\end{align}
Moreover, the lower bound objective in ($\ref{elbo}$) will have the form:
\begin{align}
    \label{svgp-optimal-elbo}
    \mathcal{L}(\nu^*) = &-\frac{N}{2} \log\left(2\pi \right)\\
    &-\frac{1}{2} \log \left(\det\left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)\right)\\
    & -\frac{1}{2} \left(\mathbf{Y}_N\right)^T \left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)^{-1} \mathbf{Y}_N \\
    &  - \frac{1}{2\sigma^2}\tr\left(\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_N} - \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)
\end{align}
where we assume an optimal selection of inducing points $\left\{x_m, y_m\right\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}$.
\\It is important to note that the existence of the objectives in (\ref{svgp-minimiser}) and (\ref{svgp-minimiser-gvi-kld}) \textit{requires} $P_{GP}^{F \vert \mathbf{Y}_N}$ to dominate $Q_{GP}^{\nu}$, which is \textit{only} guaranteed under the specific formulation of $Q$ in ($\ref{svgp-mean}$) and ($\ref{svgp-covariance}$). This severely restricts our choices for $m_Q$ and $k_Q$. 
\section{Gaussian Measures in Function Spaces}
\subsection{The Kolmogorov Extension Theorem \cite{wild2022generalized}}
A Gaussian process $F \sim GP(m, k)$ can have many Gaussian measure formulations. To define a corresponding Gaussian \textit{measure} for a GP requires specification of an appropriate \textit{measureable space}. The Kolmogorov Extension Theorem states that ...
\newline
\\ A Gaussian process by definition generates a finite set of points which conform to a Gaussian distribution. As such, the Kolmogorov Extension Theorem guarantees the existence of at least one Gaussian measure corresponding to this GP in \textit{some} measureable function space. However, this Gaussian measure is most trivially over the space of \textit{all} functions $\mathcal{F} = \left\{f: \mathbb{R}^{N} \rightarrow \mathbb{R} \right\}$. Without stricter restrictions on $\mathcal{F}$, it is difficult to define distance metrics between measures on $\mathcal{F}$. This provides further intuition for why we are so restricted in the SVGP formulation in (\ref{svgp-optimal-mean}) and (\ref{svgp-optimal-covariance}) which ensure a valid KL-divergence in the function space.
\newline
\\Gaussian measures are typically defined as a Lebesgue measure on a physical probability space $(\Omega, \mathcal{A}, \mathbb{P})$. However, there does not exist an infinite-dimensional equivalent to the Lebesgue measure, especially in a space like $\mathcal{F}$. This means that in most cases, we cannot assume the existence of an infinite-dimensional analog of a probability measure that exists in a finite-dimensional space. However, there are special cases where it is possible to define a closed-form expression for a \textit{push-forward} measure from $(\Omega, \mathcal{A}, \mathbb{P})$ to an infinite-dimensional space. In the following, we define the infinite-dimensional analog of a Gaussian measure  $\mathbb{P}^{F}(A)$, a push-forward measure constructed from an underlying finite-dimensional $(\Omega, \mathcal{A}, \mathbb{P})$. In particular, we will define $\mathbb{P}^{F}(A)$ on a Hilbert space $\mathcal{H}$ with inner product $\langle \cdot, \cdot \rangle_\mathcal{H}$.
\subsection{Gaussian Random Elements \cite{wild2022generalized}, \cite{Kukush_2019}}
 Consider a random element $F \in \mathcal{H}$. $F$ is a \textit{Gaussian Random Element} (GRE) if $\forall h \in \mathcal{H}$:
\begin{align}
    \langle F, h \rangle_\mathcal{H} \sim \mathcal{N}(\mu(F, h), \sigma^2(F, h))
\end{align}
In other words, for \textit{any} $h \in \mathcal{H}$, the inner product $\langle F, h \rangle_\mathcal{H}$ follows a Gaussian distribution (possibly with zero variance). The mean of a GRE can be written as an inner product:
\begin{align}
\mu(F, h) = \langle m, h\rangle_{\mathcal{H}}
\end{align}
where $m$ is the expectation of $F(\omega)$ with respect to some probability measure $\mathbb{P}$ on a finite dimensional space $\Omega$:
\begin{align}
    \label{gm-mean}
    m \coloneqq \int F(\omega) d \mathbb{P}(\omega)
\end{align}
The variance can be written as an inner product:
\begin{align}
\sigma^2(F, h) = \langle C(h), h\rangle_{\mathcal{H}}
\end{align}
where $C: \mathcal{H} \rightarrow \mathcal{H}$ is the covariance operator:
\begin{align}
    \label{gm-covariance}
    C(h) \coloneqq \int \langle F(\omega), h\rangle_{\mathcal{H}} F(\omega)d \mathbb{P}(\omega) - \langle m, h\rangle_{\mathcal{H}} m 
\end{align}
Thus, we can denote:
\begin{align}
    \langle F, h\rangle_{\mathcal{H}} \sim \mathcal{N}\left(  \langle m, h\rangle_{\mathcal{H}},  \langle C(h), h\rangle_{\mathcal{H}}\right)
\end{align}
or more simply:
\begin{align}
    F \sim \mathcal{N}(m, C)
\end{align}
signifying that $F$ is a GRE.
\subsection{Gaussian Measures in Hilbert Spaces \cite{wild2022generalized}}
We can define the push-forward measure of $\mathbb{P}$ as $\mathbb{P}^{F}(A) \coloneqq \mathbb{P}(F^{-1}(H))$, through the mapping $F: \Omega \rightarrow \mathcal{H}$, where $F \in \mathcal{H}$ for all Borel-measurable $H \subset \mathcal{H}$. Moreover if $F$ is a GRE, then we can define the corresponding push-forward measure $P \coloneqq \mathbb{P}^{F}$, as a Gaussian measure. This formulation allows us to define a Gaussian distribution $P$ over the infinite-dimensional Hilbert space $\mathcal{H}$. The ability to define the push-forward with the closed-form expressions of (\ref{gm-mean}) and (\ref{gm-covariance}) is a special case, unique to Gaussians.
\newline
\\It is shown in \cite{wild2022generalized} that a Gaussian process $P_{GP}$ can be specified as a Gaussian measure $P$ on a Hilbert space of square-integrable functions $L^2(\mathcal{X}, \rho, \mathbb{R})$ if the mean function $m$ satisfies:
\begin{align}
    \label{smooth-mean-function-condition}
    m \in L^2(\mathcal{X}, \rho, \mathbb{R})
\end{align}
and the kernel $k$ satisfies:
\begin{align}
    \int_{\mathcal{X}} k(x, x) d\rho(x) < \infty
    \label{trace-kernel-condition}
\end{align}
These conditions guarantee sample functions from $F$ to have square-integrable paths, allowing for a corresponding Gaussian measure $P \coloneqq \mathbb{P}^F \sim \mathcal{N}(m, C)$ defined on the \textit{measureable space} $L^2(\mathcal{X}, \rho, \mathbb{R})$ with the same mean $m$ and a covariance operator $C$:
\begin{align}
    C(f(\cdot)) \coloneqq \int k(\cdot, x')f(x')d \rho(x')
    \label{gm-covariance-operator}
\end{align}
for any function $f \in L^2(\mathcal{X}, \rho, \mathbb{R})$. \\
\newline 
Although Gaussian processes and Gaussian measures are distinct concepts, (\ref{trace-kernel-condition}) is satisfied for most GPs. For our purposes, it is critical to ensure the existence of a corresponding Gaussian measure on a Hilbert space $\mathcal{H}$. Working in more structured space $\mathcal{H}$ allows us to use distance metrics between Gaussian measures that wouldn't have been possible in $\mathcal{F}$.
\\\jw{Is it trivial to show that the NNGP kernel will satisfy (\ref{trace-kernel-condition})?}

\section{Gaussian Wasserstein Inference in Function Spaces}
We can reformulate (\ref{svgp-minimiser-gvi-kld}) as a GVI problem of the form:
\begin{align}
    \label{svgp-gwi-gp}
    \nu^* = \argmin_{\nu \in \Gamma} \left\{ \mathbb{E}_{Q_{GP}^{\nu}}\left[\sum_{n=1}^N \ell(\nu, x_n)\right] + D(Q_{GP}^{\nu}, P_{GP})\right\}
\end{align}
where $\ell$ is the negative log-likelihood and $D$ is the KL Divergence, $\KLD(Q_{GP}^{\nu}, P_{GP})$. Having shown that a Gaussian process can exist as a Gaussian measure on a Hilbert space, we can also write:
\begin{align}
    \label{svgp-gwi-gm}
    \nu^* = \argmin_{\nu \in \Gamma} \left\{ \mathbb{E}_{Q^{\nu}}\left[\sum_{n=1}^N \ell(\nu, x_n)\right] + D(Q^{\nu}, P)\right\}
\end{align}
where $Q^{\nu}$ and $P$ are push-forward Gaussian measures with corresponding Gaussian process formulations. Moreover, the GM $Q^{\nu}$ maintains a dependence on $\nu$ through the covariance operator in (\ref{gm-covariance-operator}) defined with respect to the kernel of $Q_{GP}^{\nu}$. By reformulating this as a GVI problem between Gaussian measures on a Hilbert space, \cite{wild2022generalized} and \cite{knoblauch2022optimization} argue that we are now free to choose any valid discrepancy $D$ in $\mathcal{H}$ to replace the KL-divergence. By doing this, we are no longer restricted to ($\ref{svgp-mean}$) and ($\ref{svgp-covariance}$) for $Q$, which was necessary to ensure a valid $\KLD(Q_{GP},  P_{GP})$. This is exactly how \cite{wild2022generalized} proposes using the Wasserstein distance between Gaussian Measures in Hilbert spaces instead of the KL-divergence. For two Gaussian Measures $P \sim \mathcal{N}(m_P, C_P)$ and $Q \sim \mathcal{N}(m_Q, C_Q)$, the Wasserstein distance between $P$ and $Q$ on (seperable) Hilbert spaces has a closed form expression:
\begin{align}
    \label{wasserstein-distance}
    W_2^2(P, Q) = \| m_P - m_Q\|_2^2 + \tr(C_P) + \tr(C_Q) - 2 \cdot \tr \left[ \left( \left(C_P\right)^{1/2} C_Q \left(C_P\right)^{1/2}\right)^{1/2}\right]
\end{align}
where $\tr$ is the trace of an operator and $\left(C_P\right)^{1/2}$ is the square root of the positive, self-adjoint operator $C_P$. 
\\\jw{mis-match of support}
% \\The Wasserstein distance is an integral probability metric (IPM). Unlike f-divergences which compares two measures as a \textit{ratio} (requiring $P$ to dominate $Q$), IPMs compare two measures as a \textit{difference}. Thus $W_2(P, Q)$ is a much more stable discrepancy to use than $\KLD(P, Q)$ in our infinite dimensional setting. 
\\This motivates the Gaussian Wasserstein Inference objective from \cite{wild2022generalized}:
\begin{align}
    \label{gwi-objective}
    \mathcal{L} = -\sum_{n=1}^N \mathbb{E}_{Q}\left[\log \mathcal{N}(y_n) \vert F(x_n), \sigma^2\right] + W_2^2 (P, Q)
\end{align}
Crucially, by replacing the KL-divergence in (\ref{svgp-minimiser-gvi-kld}), we are no longer constrained to the approximate GP formulation of $Q_{GP}$ in  ($\ref{svgp-mean}$) and ($\ref{svgp-covariance}$). Instead, we must define Gaussian processes that have corresponding Gaussian measures in a Hilbert space, which requires (\ref{smooth-mean-function-condition}) and (\ref{trace-kernel-condition}) to be satisfied. But most mean and covariance functions satisfy these conditions, providing significantly more freedom in defining $Q_{GP}$ than the standard SVGP approach from \cite{titsias2009variational}. With the Gaussian Wasserstein objective, \cite{wild2022generalized} proposes the training of GWI-net, a neural network mean function to replace the SVGP mean function in (\ref{svgp-mean}), showing that this approach has promising potential.
\jw{Do the two measures have to live in the same Hilbert space? Or is it adequate that they live in Hilbert spaces. Surely the Hilbert space induced by the SVGP kernel will not be the same Hilbert space as that induced by the reference kernel. So between Gaussian measures that lie in Hilbert Spaces, not a \textit{single} Hilbert space?§}

\section{Numerical Stability \cite{burt2020convergence}}
In addition to the scaling issues of Gaussian Process, stable matrix inversion is also a practical consideration that must be taken into account. In particular in the sparse Gaussian Process setting,  ($\ref{svgp-covariance}$)  inverts a matrix containing the inducing points  $\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}$. A matrix having its smallest and largest eigenvalues many orders of magnitude apart can lead to an ill-conditioned inverse. This means that for $\mathbf{y} = \left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}\right)^{-1}\mathbf{x}$, small perturbations in $\mathbf{x}$ will result in large changes in $\mathbf{y}$. In the context of sparse GP, this means that small differences in function values will result in large changes in the probability density. 
\subsection{Jitter for Matrix Inversion}
One approach is to apply a spectral shift to 
$\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}$ to increase its smallest eigenvalue. This can be done by adding a `jitter' term $\epsilon$ such that the matrix inversion becomes $\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}+\epsilon\mathbf{I}_M\right)^{-1}$, raising the lowest eigenvalue $\lambda_1$ to $\lambda_1 + \epsilon$. \cite{burt2020convergence} shows that the evidence lower bound in ($\ref{svgp-optimal-elbo}$) is monotonically decreasing in $\epsilon$ where:
\begin{align}
    \label{svgp-optimal-elbo-jitter}
    \mathcal{F}(\nu^*) \geq \mathcal{F}_{\epsilon}(\nu^*) = & -\frac{N}{2} \log\left(2\pi \right) \\
    & -\frac{1}{2} \log \left(\det\left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M} +\epsilon\mathbf{I}_M\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)\right)\\
    & -\frac{1}{2} \left(\mathbf{Y}_N\right)^T \left( \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}+\epsilon\mathbf{I}_M\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)^{-1} \mathbf{Y}_N \\
    & - \frac{1}{2\sigma^2}\tr\left(\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_N} - \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_M}\left(\mathbf{K}_{\mathbf{X}_M, \mathbf{X}_M}+\epsilon\mathbf{I}_M\right)^{-1} \mathbf{K}_{\mathbf{X}_M, \mathbf{X}_N}\right)
\end{align}
showing that adding jitter introduces a gap between the small gap on the loss objective, though in practice this impact is typically negligable \cite{burt2020convergence}.
\subsection{Inducing Points Selection}
Careful selection of $\mathbf{X}_M$ can also ensure numerical stability of the matrix inversion. Additionally, finding the optimal inducing points $\left\{x_m, y_m\right\}_{m=1}^{M} \subset \{x_n, y_n\}_{n=1}^{N}$ that optimise the loss objective $\mathcal{L}(\nu)$ in ($\ref{elbo}$) can be computationally expensive. \cite{burt2020convergence} proposes a \textit{greedy variance selection} method which iteratively selects the next inducing point based on the highest marginal variance in the prior conditioned on the current set of inducing points:
\begin{align}
    \label{greedy-varaince-selection}
    \text{index}_{m+1} = \argmax \left(\diag \left(\mathbf{K}_{\mathbf{X}_N, \mathbf{X}_N} - \mathbf{K}_{\mathbf{X}_N, \mathbf{X}_{m}} \left(\mathbf{K}_{\mathbf{X}_{m}, \mathbf{X}_{m}} \right) \mathbf{K}_{\mathbf{X}_{m}, \mathbf{X}_N}\right)\right)
\end{align}
where $m < M$, $\mathbf{X}_{m}$ are the $m$ inducing points already chosen, and $\text{index}_{m+1}$ is the index of the next inducing point.
\\\jw{It doesn't seem like \cite{burt2020convergence} explicitly mentions how this will help with matrix inversion. Need to more carefully read this though.}

\section{Appendix}
\subsection{Appendix A}\label{svgp-kld-bayesian}
We wish to match the log-likelihood of $P_{GP}$ for training data $\left\{ x_n, y_n\right\}_{n=1}^{N}$ by maximising the lower bound with respect to $\nu$ for $Q_{GP}^{\nu}$ on inducing points $\{x_m, y_m\}_{m=1}^{M}$. We begin by reformulating the log-likelihood:
\begin{align}
    \mathcal{L} &= \log P\left(\mathbf{Y}_{N} =\left\{ y_n\right\}_{n=1}^N \big\vert \mathbf{X}_N = \left\{ x_n\right\}_{n=1}^N\right) 
    \\&= \log P\left(\mathbf{Y}_{N} =\left\{ y_n\right\}_{n=1}^N \big\vert \mathbf{Y}_M, \mathbf{X}_M = \left\{ x_m\right\}_{m=1}^M, \mathbf{X}_N = \left\{ x_n\right\}_{n=1}^N\right) 
    \\&= \log \int_{\mathbb{R}^M} P\left(\mathbf{Y}_{N} =\left\{ y_n\right\}_{n=1}^N, \mathbf{Y}_M \big\vert \mathbf{X}_M = \left\{ x_m\right\}_{m=1}^M, \mathbf{X}_N = \left\{ x_n\right\}_{n=1}^N\right) d\mathbf{Y}_M 
    \\&= \log \int_{\mathbb{R}^M} P\left(\left\{ y_n\right\}, \mathbf{Y}_M \big\vert \left\{ x_m\right\},  \left\{ x_n\right\}\right) d\mathbf{Y}_M 
    \label{reference-log-like-shorthand}
\end{align}
where we just simplify the notation in (\ref{reference-log-like-shorthand}). Introducing $Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)$, the $Q_{GP}^{\nu}$ prior on the inducing points, we can equivalently write:
\begin{align}
            \mathcal{L} &= \log \int_{\mathbb{R}^M} \frac{P\left(\left\{ y_n\right\}, \mathbf{Y}_M \big\vert \left\{ x_m\right\}, \left\{ x_n\right\}\right)Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)}{Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)} d\mathbf{Y}_M 
    \\&= \log \int_{\mathbb{R}^M} \frac{P\left(\left\{ y_n\right\} \big\vert \mathbf{Y}_M , \left\{ x_m\right\}, \left\{ x_n\right\}\right)P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\}\right)Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)}{Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)} d\mathbf{Y}_M 
    \\&\qquad\geq  \int_{\mathbb{R}^M} Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right) \log \left(\frac{P\left(\left\{ y_n\right\} \big\vert \mathbf{Y}_M , \left\{ x_m\right\}, \left\{ x_n\right\}\right)P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\}\right)}{Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)} \right)d\mathbf{Y}_M 
    \label{log-like-jensen}
    \\ & \qquad\qquad \eqqcolon \mathcal{F(\nu)}
\end{align}
where we lower bound the log-likelihood with Jensen's inequality in (\ref{log-like-jensen}). $\mathcal{F(\nu)}$ is the free energy or evidence lower bound (ELBO) and can be expressed:
\begin{multline}
    \mathcal{F(\nu)} = \int_{\mathbb{R}^M} Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right) \log P\left(\left\{ y_n\right\} \big\vert \mathbf{Y}_M , \left\{ x_m\right\}, \left\{ x_n\right\}\right)d\mathbf{Y}_M 
    \\ + \int_{\mathbb{R}^M} Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right) \log \left(\frac{P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\}\right)}{Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)} \right)d\mathbf{Y}_M
\end{multline}
Recovering the standard result:
\begin{align}
    \mathcal{F(\nu)} &=  \log P\left(\left\{ y_n\right\} \big\vert \left\{ x_n\right\}\right) - \KLD\left[Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right) \| P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)\right]
    \\ &= \sum_{n=1}^N \left[\log P \left(y_n \big\vert  x_n \right)\right] - \KLD\left[Q^{\nu}\left(\mathbf{Y}_M \vert \left\{ x_m\right\} \right) \| P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)\right]
\end{align}

We see from (\ref{log-like-jensen}) that $\mathcal{F(\nu)}$ can also be expressed:
\begin{align}
    \mathcal{F(\nu)} &= - \mathbb{E}_{\mathbf{Y}_M \sim Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)} \left[\log \left(\frac{Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right)}{P\left(\left\{ y_n\right\} \big\vert \mathbf{Y}_M , \left\{ x_m\right\}, \left\{ x_n\right\}\right)P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\}\right)} \right) \right]\\
    &= -\KLD \left[Q^{\nu}\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\} \right) \| P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\}, \left\{ y_n\right\}, \left\{ x_n\right\}\right) \right]
    \label{free-energy-kld}
\end{align}
where we see that $P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\}, \left\{ y_n\right\}, \left\{ x_n\right\}\right) \propto P\left(\left\{ y_n\right\} \big\vert \mathbf{Y}_M , \left\{ x_m\right\}, \left\{ x_n\right\}\right)P\left(\mathbf{Y}_M \big\vert \left\{ x_m\right\}\right)$, the posterior distribution.
Thus, the KL-divergence is the natural divergence that arises from a Bayesian approach. (\ref{free-energy-kld}) is the negative KL between the prior of the approximating distribution $Q^{\nu}$ and the \textit{true} Bayesian posterior of $P$. In other words, $Q^{\nu*}$ is the projection of the Bayesian posterior onto the space of approximating GPs $\left\{Q^{\nu}: \nu \in \Gamma\right\}$ with respect to the KL-divergence. This is indicated as the minimisation of the $\KLD$ in (\ref{svgp-minimiser}).

\begingroup
\let\clearpage\relax
\AtNextBibliography{\small}
\section*{References}
\printbibliography[heading=talikarng, title = {References}]
\endgroup
\end{document}